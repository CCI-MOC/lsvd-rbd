diff --git a/aio.cc b/aio.cc
index 3531be7..eb3c1c5 100644
--- a/aio.cc
+++ b/aio.cc
@@ -5,7 +5,15 @@ enum lsvd_op {
     LSVD_OP_WRITE = 2
 };
 
-typedef std::tuple<size_t,size_t,size_t> cache_miss;
+/* offset_base, len, buf_offset */
+//typedef std::tuple<size_t,size_t,size_t> cache_miss;
+struct cache_miss {
+    size_t offset;		// volume offset, in bytes
+    size_t len;			// in bytes
+    size_t buf_offset;		// in bytes
+    cache_miss(size_t _off, size_t _len, size_t _buf_offset) :
+	offset(_off), len(_len), buf_offset(_buf_offset) {}
+};
 
 class lsvd_aio {
 public:
diff --git a/lsvd.py b/lsvd.py
index 307f774..771bcaf 100644
--- a/lsvd.py
+++ b/lsvd.py
@@ -124,6 +124,55 @@ def wcache_oldest(blk):
 def wcache_checkpoint():
     lsvd_lib.wcache_write_ckpt()
 
+rsuper = None
+def rcache_init(blkno):
+    global rsuper
+    assert _fd != -1
+    lsvd_lib.rcache_init(c_uint(blkno), c_int(_fd))
+    rsuper = j_read_super()
+    lsvd_lib.rcache_getsuper(byref(rsuper))
+
+def rcache_shutdown():
+    lsvd_lib.rcache_shutdown()
+
+def rcache_read(offset, nbytes):
+    assert (nbytes % 512) == 0 and (offset % 512) == 0
+    buf = (c_char * nbytes)()
+    lsvd_lib.rcache_read(buf, c_ulong(offset), c_ulong(nbytes))
+    return buf[0:nbytes]
+
+def rcache_add(obj, offset, data):
+    nbytes = len(data)
+    assert (nbytes % 512) == 0
+    lsvd_lib.rcache_add(c_int(obj), c_int(offset), data, c_int(nbytes), data)
+    
+def rcache_getmap():
+    n = rsuper.units
+    k = (obj_offset * n)()
+    v = (c_int * n)()
+    m = lsvd_lib.rcache_getmap(byref(k), byref(v), n)
+    keys = [[_.obj, _.offset] for _ in k[0:m]]
+    vals = v[:]
+    return list(zip(keys, vals))
+    
+def rcache_flatmap():
+    n = rsuper.units
+    vals = (obj_offset * n)()
+    n = lsvd_lib.rcache_get_flat(byref(vals), c_int(n))
+    return [[_.obj,_.offset] for _ in vals[0:n]]
+
+def rcache_bitmap():
+    n = rsuper.units
+    vals = (c_ushort * n)()
+    n = lsvd_lib.rcache_get_masks(byref(vals), c_int(n))
+    return vals[0:n]
+
+def rcache_evict(n):
+    lsvd_lib.rcache_evict(c_int(n))
+
+def rcache_reset():
+    lsvd_lib.rcache_reset()
+
 #----------------
 # these manipulate the objmap directly, without going through the translation layer
 #
diff --git a/lsvd_rbd.cc b/lsvd_rbd.cc
index f90aad8..eeb76cb 100644
--- a/lsvd_rbd.cc
+++ b/lsvd_rbd.cc
@@ -38,14 +38,25 @@ namespace fs = std::experimental::filesystem;
 #include <errno.h>
 
 std::mutex printf_m;
-#if 0
-#define xprintf(...) do { \
+bool _debug_init_done;
+int  _debug_mask;
+enum {DBG_MAP = 1, DBG_HITS = 2, DBG_AIO = 4};
+void debug_init(void)
+{
+    if (getenv("DBG_AIO"))
+	_debug_mask |= DBG_AIO;
+    if (getenv("DBG_HITS"))
+	_debug_mask |= DBG_HITS;
+    if (getenv("DBG_MAP"))
+	_debug_mask |= DBG_MAP;
+}
+    
+#define xprintf(mask, ...) do { \
+    if (!_debug_init_done) debug_init(); \
+    if (mask & _debug_mask) { \
 	std::unique_lock lk(printf_m); \
 	fprintf(stderr, __VA_ARGS__); \
-    } while (0)
-#else
-#define xprintf(...)
-#endif
+    }} while (0)
 
 struct _log {
     int l;
@@ -121,6 +132,14 @@ public:
 					  size_t offset) = 0;
     virtual ssize_t read_numbered_object(int seq, char *buf, size_t len,
 					 size_t offset) = 0;
+    virtual int aio_read_object(const char *name, char *buf, size_t len, size_t offset,
+				void (*cb)(void*), void *ptr) = 0;
+    virtual int aio_read_num_obj(int seq, char *buf, size_t len, size_t offset,
+				 void (*cb)(void*), void *ptr) = 0;
+    virtual int aio_read_objectv(const char *name, iovec *iov, int iovcnt, size_t offset,
+				 void (*cb)(void*), void *ptr) = 0;
+    virtual int aio_read_num_objv(int seq, iovec *iov, int iovcnt, size_t offset,
+				  void (*cb)(void*), void *ptr) = 0;
     virtual std::string object_name(int seq) = 0;
     virtual ~backend(){}
 };
@@ -783,6 +802,25 @@ public:
     }
 };
 
+#include <sys/stat.h>
+#include <sys/ioctl.h>
+#include <linux/fs.h>
+
+size_t getsize64(int fd)
+{
+    struct stat sb;
+    size_t size;
+    
+    if (fstat(fd, &sb) < 0)
+	throw_fs_error("stat");
+    if (S_ISBLK(sb.st_mode)) {
+	if (ioctl(fd, BLKGETSIZE64, &size) < 0)
+	    throw_fs_error("ioctl");
+    }
+    else
+	size = sb.st_size;
+    return size;
+}
 
 /* the read cache is:
  * 1. indexed by obj/offset[*], not LBA
@@ -800,6 +838,7 @@ class read_cache {
     objmap             *omap;
     translate          *be;
     int                 fd;
+    size_t              dev_max;
     backend            *io;
     
     int               unit_sectors;
@@ -810,7 +849,6 @@ class read_cache {
     int               n_hits;
     int               n_misses;
     
-    thread_pool<cache_work*> workers;
     thread_pool<int> misc_threads; // eviction thread, for now
     bool             nothreads;	   // for debug
     
@@ -887,33 +925,118 @@ class read_cache {
 	}
     }
 
+    // TODO: iovec version
+    void do_add(extmap::obj_offset oo, int sectors, char *buf) {
+	// must be 4KB aligned
+	assert(!(oo.offset & 7));
+
+	while (sectors > 0) {
+	    std::unique_lock lk(m);
+
+	    extmap::obj_offset obj_blk = {oo.obj, oo.offset / unit_sectors};
+	    int cache_blk = -1;
+	    auto it = map.find(obj_blk);
+	    if (it != map.end())
+		cache_blk = it->second;
+	    else if (free_blks.size() > 0) {
+		cache_blk = free_blks.back();
+		free_blks.pop_back();
+	    }
+	    else {
+		//xprintf("ADD %d.%d: no space\n", oo.obj, oo.offset);
+		return;
+	    }
+	    while (busy[cache_blk])
+		cv.wait(lk);
+	    busy[cache_blk] = true;
+	    auto mask = bitmap[cache_blk];
+	    lk.unlock();
+
+	    assert(cache_blk >= 0);
+
+	    auto obj_page = oo.offset / 8;
+	    auto pages_in_blk = unit_sectors / 8;
+	    auto blk_page = obj_blk.offset * pages_in_blk;
+	    smartiov iov;
+
+	    for (int i = obj_page - blk_page; sectors > 0 && i < pages_in_blk; i++) {
+		mask |= (1 << i);
+		iov.push_back((iovec){buf, 4096});
+		buf += 4096;
+		sectors -= 8;
+		oo.offset += 8;
+	    }
+
+	    off_t blk_offset = ((cache_blk * pages_in_blk) + super->base) * 4096L;
+	    blk_offset += (obj_page - blk_page) * 4096L;
+	    assert(blk_offset + iov.bytes() <= dev_max);
+	    if (pwritev(fd, iov.data(), iov.size(), blk_offset) < 0)
+		throw_fs_error("rcache2");
+
+	    lk.lock();
+	    map[obj_blk] = cache_blk;
+	    bitmap[cache_blk] = mask;
+	    flat_map[cache_blk] = obj_blk;
+	    busy[cache_blk] = false;
+	    map_dirty = true;
+	    cv.notify_one();
+	    lk.unlock();
+	}
+    }
+
+    struct add_work {
+	extmap::obj_offset oo;
+	int sectors;
+	char *buf;
+	add_work(extmap::obj_offset _oo, int _sectors, char *_buf) {
+	    oo = _oo;
+	    sectors = _sectors;
+	    buf = _buf;
+	}
+    };
+    void adder(thread_pool<add_work*> *p) {
+	while (p->running) {
+	    add_work *w;
+	    if (!p->get(w))
+		break;
+	    auto [oo, sectors, buf] = *w;
+	    do_add(oo, sectors, buf);
+	    free(buf);
+	    delete w;
+	}
+    }
+    thread_pool<add_work*> adders;
+
+public:
     std::atomic<int> n_lines_read = 0;
-    
-    void do_readv(size_t offset, const iovec *iov, int iovcnt) {
+
+    void readv(size_t offset, const iovec *iov, int iovcnt,
+	       void (*callback)(void*), void *ptr) {
 	auto iovs = smartiov(iov, iovcnt);
 	size_t len = iovs.bytes();
 	lba_t lba = offset/512, sectors = len/512;
 	std::vector<std::tuple<lba_t,lba_t,extmap::obj_offset>> extents;
-	xprintf("rc: do_readv %d+%d\n", (int)lba, (int)sectors);
+	xprintf(DBG_MAP, "rc: do_readv %d+%d\n", (int)lba, (int)sectors);
 	std::shared_lock lk(omap->m);
 	std::unique_lock lk2(m);
 	for (auto it = omap->map.lookup(lba);
 	     it != omap->map.end() && it->base() < lba+sectors; it++) {
 	    auto [_b, _l, _p] = it->vals(lba, lba+sectors);
-	    xprintf("rc: map: %d+%d -> %d+%d\n", (int)lba, (int)sectors, (int)_b, (int)(_l-_b));
+	    xprintf(DBG_MAP, "rc: map: %d+%d -> %d+%d\n", (int)lba, (int)sectors, (int)_b, (int)(_l-_b));
 	    extents.push_back(it->vals(lba, lba+sectors));
 	}
 
-	if (n_hits + n_misses >= 10000) {
+	if (n_hits + n_misses >= 30000) {
 	    hit_rate = (hit_rate * 0.5) + (0.5 * n_hits) / (n_hits + n_misses);
-	    //xprintf("hit rate: %f hits %d misses %d lines_read %d\n", hit_rate, n_hits, n_misses, n_lines_read);
+	    int x = n_lines_read;
+	    xprintf(DBG_HITS, "hit rate: %f lines_read %d\n", hit_rate, x);
 	    n_hits = n_misses = 0;
 	}
 	/* hack for better random performance - if hit rate is less than 50%, gradually
 	 * stop using the cache.
 	 */
 	std::uniform_real_distribution<double> unif(0.0,1.0);
-	bool use_cache = unif(rng) < 2 * hit_rate;
+	bool use_cache = unif(rng) < 2 * hit_rate + 0.001;
 	lk2.unlock();
 	lk.unlock();
 
@@ -928,7 +1051,7 @@ class read_cache {
 		auto bytes = (base-lba)*512;
 		iovs.slice(buf_offset, buf_offset+bytes).zero();
 		len -= bytes;
-		xprintf("rc: readzero: %d+%d\n", (int)base, (int)lba);
+		xprintf(DBG_MAP, "rc: readzero: %d+%d\n", (int)base, (int)lba);
 	    }
 	    while (base < limit) {
 		extmap::obj_offset unit = {ptr.obj, ptr.offset / unit_sectors};
@@ -951,12 +1074,14 @@ class read_cache {
 		    if ((access_mask & bitmap[n]) == access_mask)
 			in_cache = true;
 		}
-		if (in_cache) {
-		    n_hits++;
+		if (in_cache) 
 		    (*in_use)[n]++;
+#if 0
+		    n_hits += (blk_top_offset - blk_offset);
 		}
 		else
 		    n_misses++;
+#endif
 		lk2.unlock();
 
 		if (in_cache) {
@@ -965,8 +1090,11 @@ class read_cache {
 			finish = start + (blk_top_offset - blk_offset);
 		    size_t bytes = 512 * (finish - start);
 
-		    xprintf("rc: %d+%d in_cache: %d\n", (int)base, (int)bytes/512, (int)start);
+		    n_hits += (finish - start) / 8;
+		    
+		    xprintf(DBG_MAP, "rc: %d+%d in_cache: %d\n", (int)base, (int)bytes/512, (int)start);
 		    auto tmp = iovs.slice(buf_offset, buf_offset+bytes);
+		    assert(512L*start + tmp.bytes() <= dev_max);
 		    if (preadv(fd, tmp.data(), tmp.size(), 512L*start) < 0)
 			throw_fs_error("rcache");
 		    
@@ -979,7 +1107,7 @@ class read_cache {
 		}
 		else if (!use_cache) {
 		    lba_t sectors = limit - base;
-		    xprintf("rc: %d+%d rd_obj: %d+%d\n", (int)base, (int)sectors,
+		    xprintf(DBG_MAP, "rc: %d+%d rd_obj: %d+%d\n", (int)base, (int)sectors,
 			    (int)ptr.obj, (int)ptr.offset);
 		    auto _iov = iovs.slice(buf_offset, buf_offset + 512*sectors);
 		    io->read_numbered_objectv(ptr.obj, _iov.data(), _iov.size(),
@@ -987,6 +1115,7 @@ class read_cache {
 		    base = limit;
 		    buf_offset += 512 * sectors;
 		    len -= 512 * sectors;
+		    // don't account for hits or misses here
 		}
 		else {
 		    n_lines_read++;
@@ -996,9 +1125,13 @@ class read_cache {
                     size_t start = 512 * blk_offset,
 			finish = 512 * blk_top_offset;
 		    assert((int)finish <= bytes);
-		    xprintf("rc: %d+%d fetch -> cache (%d.%d)\n", (int)base,
+		    xprintf(DBG_MAP, "rc: %d+%d fetch -> cache (%d.%d)\n", (int)base,
 			    (int)(limit-base), (int)unit.obj, (int)blk_base);
 
+		    int h = (blk_top_offset - blk_offset)/8;
+		    n_misses += (unit_sectors - h);
+		    n_hits += h;
+
 		    iovs.slice(buf_offset,
 			       buf_offset+(finish-start)).copy_in(cache_line+start);
 
@@ -1013,98 +1146,23 @@ class read_cache {
 	    lba = limit;
 	}
 	if (len > 0) {
-	    xprintf("rc: zero %d+%d\n", (int)lba, (int)len/512);
+	    xprintf(DBG_MAP, "rc: zero %d+%d\n", (int)lba, (int)len/512);
 	    iovs.slice(buf_offset, buf_offset+len).zero();
 	}
+	callback(ptr);
 
 	// now read is finished, and we can add shit to the cache
 	for (auto [oo, n, cache_line] : to_add) {
-	    add(oo, n, cache_line);
-	    free(cache_line);
+	    auto w = new add_work(oo, n, cache_line);
+	    adders.put(w);
 	}
     }
 
-    void reader(thread_pool<cache_work*> *p) {
-	while (p->running) {
-	    cache_work *w;
-	    if (!p->get(w))
-		break;
-	    auto [iov, iovcnt] = w->iovs.c_iov();
-	    do_readv(w->lba * 512, iov, iovcnt);
-	    DBG((long)w->ptr);
-	    w->callback(w->ptr);
-	    delete w;
-	}
-    }
-
-public:
-
-    // TODO: iovec version
-    void add(extmap::obj_offset oo, int sectors, char *buf) {
-	// must be 4KB aligned
-	assert(!(oo.offset & 7));
-
-	while (sectors > 0) {
-	    std::unique_lock lk(m);
-
-	    extmap::obj_offset obj_blk = {oo.obj, oo.offset / unit_sectors};
-	    int cache_blk = -1;
-	    auto it = map.find(obj_blk);
-	    if (it != map.end())
-		cache_blk = it->second;
-	    else if (free_blks.size() > 0) {
-		cache_blk = free_blks.back();
-		free_blks.pop_back();
-	    }
-	    else {
-		//xprintf("ADD %d.%d: no space\n", oo.obj, oo.offset);
-		return;
-	    }
-	    while (busy[cache_blk])
-		cv.wait(lk);
-	    busy[cache_blk] = true;
-	    auto mask = bitmap[cache_blk];
-	    lk.unlock();
-
-	    assert(cache_blk >= 0);
-
-	    auto obj_page = oo.offset / 8;
-	    auto pages_in_blk = unit_sectors / 8;
-	    auto blk_page = obj_blk.offset * pages_in_blk;
-	    std::vector<iovec> iov;
-
-	    for (int i = obj_page - blk_page; sectors > 0 && i < pages_in_blk; i++) {
-		mask |= (1 << i);
-		iov.push_back((iovec){buf, 4096});
-		buf += 4096;
-		sectors -= 8;
-		oo.offset += 8;
-	    }
-
-	    off_t blk_offset = ((cache_blk * pages_in_blk) + super->base) * 4096L;
-	    blk_offset += (obj_page - blk_page) * 4096L;
-	    if (pwritev(fd, iov.data(), iov.size(), blk_offset) < 0)
-		throw_fs_error("rcache2");
-
-	    lk.lock();
-	    map[obj_blk] = cache_blk;
-	    bitmap[cache_blk] = mask;
-	    flat_map[cache_blk] = obj_blk;
-	    busy[cache_blk] = false;
-	    map_dirty = true;
-	    cv.notify_one();
-	    lk.unlock();
-	}
-    }
-
-    void readv(size_t offset, const iovec *iov, int iovcnt, void (*callback)(void*), void *ptr) {
-	workers.put(new cache_work(offset/512, iov, iovcnt, callback, ptr));
-    }
-
     read_cache(uint32_t blkno, int _fd, bool nt, translate *_be, objmap *_om, backend *_io) :
-	omap(_om), be(_be), fd(_fd), io(_io), hit_rate(1.0), n_hits(0), n_misses(0), workers(&m),
-	misc_threads(&m), nothreads(nt)
+	omap(_om), be(_be), fd(_fd), io(_io), hit_rate(1.0), n_hits(0), n_misses(0),
+	misc_threads(&m), nothreads(nt), adders(&m)
     {
+	dev_max = getsize64(fd);
 	n_lines_read = 0;
 	char *buf = (char*)aligned_alloc(512, 4096);
 	if (pread(fd, buf, 4096, 4096L*blkno) < 4096)
@@ -1140,10 +1198,9 @@ public:
 	in_use = new std::vector<std::atomic<int>>(super->units);
 	map_dirty = false;
 
-	for (int i = 0; i < 16; i++)
-	    workers.pool.push(std::thread(&read_cache::reader, this, &workers));
-	if (!nothreads)
-	    misc_threads.pool.push(std::thread(&read_cache::evict_thread, this, &misc_threads));
+	for (int i = 0; i < 4; i++)
+	    adders.pool.push(std::thread(&read_cache::adder, this, &adders));
+	misc_threads.pool.push(std::thread(&read_cache::evict_thread, this, &misc_threads));
     }
 
     void write_map(void) {
@@ -1160,6 +1217,10 @@ public:
 
     /* debugging
      */
+    void add(extmap::obj_offset oo, int sectors, char *buf) {
+	do_add(oo, sectors, buf);
+    }
+
     void get_info(j_read_super **p_super, extmap::obj_offset **p_flat, uint16_t **p_bitmap,
 		  std::vector<int> **p_free_blks, std::map<extmap::obj_offset,int> **p_map) {
 	if (p_super != NULL)
@@ -1252,6 +1313,22 @@ public:
 	    throw_fs_error("read_obj");
 	return val;
     }
+    int aio_read_object(const char *name, char *buf, size_t len, size_t offset,
+    			void (*cb)(void*), void *ptr) {
+	return 0;
+    }
+    int aio_read_num_obj(int seq, char *buf, size_t len, size_t offset,
+    			 void (*cb)(void*), void *ptr) {
+	return 0;
+    }
+    int aio_read_objectv(const char *name, iovec *iov, int iovcnt, size_t offset,
+    			 void (*cb)(void*), void *ptr) {
+	return 0;
+    }
+    int aio_read_num_objv(int seq, iovec *iov, int iovcnt, size_t offset,
+				  void (*cb)(void*), void *ptr) {
+	return 0;
+    }
     ~file_backend() {
 	free((void*)prefix);
 	for (auto it = cached_fds.begin(); it != cached_fds.end(); it++)
@@ -1275,6 +1352,7 @@ static bool aligned(void *ptr, int a)
  */
 class write_cache {
     int            fd;
+    size_t         dev_max;
     uint32_t       super_blkno;
     j_write_super *super;	// 4KB
 
@@ -1372,7 +1450,7 @@ class write_cache {
 	    long _lba = (blockno+1)*8;
 	    for (auto w : work) {
 		int len = w->iovs.bytes() / 512;
-		xprintf("wr: %ld+%d -> %ld\n", w->lba, len, _lba);
+		xprintf(DBG_MAP, "wr: %ld+%d -> %ld\n", w->lba, len, _lba);
 		_lba += len;
 	    }
 	    
@@ -1381,7 +1459,7 @@ class write_cache {
 		assert(pad < (int)super->limit);
 		lengths[pad] = super->limit - pad;
 		assert(lengths[pad] > 0);
-		//xprintf("lens[%d] <- %d\n", pad, super->limit - pad);
+		//xprintf(DBG_MAP, "lens[%d] <- %d\n", pad, super->limit - pad);
 	    }
 
 	    j_hdr *j = mk_header(hdr, LSVD_J_DATA, my_uuid, 1+blocks);
@@ -1391,9 +1469,11 @@ class write_cache {
 
 	    lk.unlock();
 	    
-	    if (pad != 0)
+	    if (pad != 0) {
+		assert((pad+1)*4096UL <= dev_max);
 		if (pwrite(fd, pad_hdr, 4096, pad*4096L) < 0)
 		    throw_fs_error("wpad");
+	    }
 
 	    std::vector<j_extent> extents;
 
@@ -1476,6 +1556,7 @@ class write_cache {
 	char *buf = (char*)aligned_alloc(512, 4096);
 	j_hdr *h = (j_hdr*)buf;
 
+	assert((blk+1)*4096UL <= dev_max);
 	if (pread(fd, buf, 4096, blk*4096L) < 0)
 	    throw_fs_error("wcache");
 	if (h->magic != LSVD_MAGIC) {
@@ -1650,7 +1731,8 @@ class write_cache {
 	super_copy->len_start = super->len_start = blockno+map_pages;
 	super_copy->len_blocks = super->len_blocks = len_pages;
 	super_copy->len_entries = super->len_entries = _lengths.size();
-	
+
+	assert(4096UL*blockno + 4096UL*ckpt_pages <= dev_max);
 	if (pwrite(fd, e_buf, 4096*ckpt_pages, 4096L*blockno) < 0)
 	    throw_fs_error("wckpt_e");
 	if (pwrite(fd, (char*)super_copy, 4096, 4096L*super_blkno) < 0)
@@ -1664,89 +1746,10 @@ class write_cache {
     }
 
 public:
-
-    class aio_readv_work {
-    public:
-	size_t offset = 0;
-	smartiov iovs;
-	std::vector<cache_miss> *misses = NULL;
-	void (*cb)(void*) = NULL;
-	void *ptr = NULL;
-	aio_readv_work() {}
-	aio_readv_work(size_t _offset, const iovec *iov, int iovcnt,
-		       std::vector<cache_miss> *_misses,
-		       void (*_cb)(void*), void *_ptr) :
-	    offset(_offset), iovs(iov, iovcnt), misses(_misses),
-	    cb(_cb), ptr(_ptr) {}
-    };
-
-    class wc_read_aio : lsvd_aio {
-	std::vector<cache_miss> *misses = NULL;
-	void (*cb2)(void*) = NULL;
-	int state;
-	std::mutex *m;
-	write_cache *cache;
-	
-    public:
-	wc_read_aio(size_t _offset, const iovec *iov, int iovcnt, std::vector<cache_miss> *_misses,
-		    void (*_cb)(void*), void *_ptr, write_cache *c) : lsvd_aio(_offset/512, iov, iovcnt) {
-	    misses = _misses;
-	    cache = c;
-	    m = &c->m;
-	    cb2 = _cb;
-	    ptr1 = _ptr;
-	}
-	void run(void) {
-	    auto bytes = iovs.bytes();
-	    lba_t base = lba, limit = base + bytes/512, prev = base;
-	    std::unique_lock<std::mutex> lk(*m);
-
-	    size_t buf_offset = 0;
-	    std::vector<std::pair<smartiov, size_t>> to_read;
-
-	    for (auto it = cache->map.lookup(base); it != cache->map.end() && it->base() < limit; it++) {
-		auto [_base, _limit, plba] = it->vals(base, limit);
-		if (_base > prev) {
-		    size_t bytes = 512 * (_base - prev);
-		    misses->push_back(std::tuple((size_t)512*prev, bytes, buf_offset));
-		    buf_offset += bytes;
-		}
-
-		size_t bytes = 512 * (_limit - _base),
-		    nvme_offset = 512L * plba;
-		auto slice = iovs.slice(buf_offset, buf_offset+bytes);
-		to_read.push_back(std::make_pair(slice, nvme_offset));
-		xprintf("wr: rd: %ld+%ld <- %ld\n", _base, _limit-_base, plba);
-		buf_offset += bytes;
-		prev = _limit;
-	    }
-	    lk.unlock();
-	    for (auto [slice, nvme_offset] : to_read)
-		if (preadv(cache->fd, slice.data(), slice.size(), nvme_offset) < 0)
-		    throw_fs_error("wcache_read");
-
-	    if (prev < limit)
-		misses->push_back(std::tuple(512 * prev, 512 * (limit - prev), buf_offset));
-	    cb2(ptr1);
-	    delete this;
-	}
-    };
-    
-    thread_pool<wc_read_aio*> readv_workers;
-    
-    void aio_readv_thread(thread_pool<wc_read_aio*> *p) {
-	while (p->running) {
-	    wc_read_aio *w;
-	    if (!p->get(w)) 
-		break;
-	    w->run();
-	}
-    }
-
-    write_cache(uint32_t blkno, int _fd, translate *_be, int n_threads) :
-	workers(&m), readv_workers(&m) {
+    write_cache(uint32_t blkno, int _fd, translate *_be, int n_threads) : workers(&m) {
 	super_blkno = blkno;
 	fd = _fd;
+	dev_max = getsize64(fd);
 	be = _be;
 	char *buf = (char*)aligned_alloc(512, 4096);
 	if (pread(fd, buf, 4096, 4096L*blkno) < 4096)
@@ -1796,8 +1799,6 @@ public:
 	// https://stackoverflow.com/questions/22657770/using-c-11-multithreading-on-non-static-member-function
 	for (auto i = 0; i < n_threads; i++)
 	    workers.pool.push(std::thread(&write_cache::writer, this, &workers));
-	for (auto i = 0; i < 16; i++)
-	    readv_workers.pool.push(std::thread(&write_cache::aio_readv_thread, this, &readv_workers));
 
 	misc_threads = new thread_pool<int>(&m);
 	misc_threads->pool.push(std::thread(&write_cache::evict_thread, this, misc_threads));
@@ -1815,27 +1816,22 @@ public:
 	workers.put_locked(new cache_work(offset/512, iov, iovcnt, callback, ptr));
     }
 
-   /* returns tuples of:
-     * offset, len, buf_offset
-     */
-
-#if 0
-    // TODO: how the hell to handle fragments?
-    void readv(size_t offset, const iovec *iov, int iovcnt, std::vector<cache_miss> *misses) {
-	auto iovs = smartiov(iov, iovcnt);
+    void aio_readv(size_t offset, const iovec *iov, int iovcnt,
+		   std::vector<cache_miss> *misses, void (*cb)(void*), void *ptr) {
+	smartiov iovs(iov, iovcnt);
 	auto bytes = iovs.bytes();
-
 	lba_t base = offset/512, limit = base + bytes/512, prev = base;
 	std::unique_lock<std::mutex> lk(m);
 
 	size_t buf_offset = 0;
 	std::vector<std::pair<smartiov, size_t>> to_read;
 
-	for (auto it = map.lookup(base); it != map.end() && it->base() < limit; it++) {
+	for (auto it = map.lookup(base);
+	     it != map.end() && it->base() < limit; it++) {
 	    auto [_base, _limit, plba] = it->vals(base, limit);
 	    if (_base > prev) {
 		size_t bytes = 512 * (_base - prev);
-		misses->push_back(std::tuple((size_t)512*prev, bytes, buf_offset));
+		misses->push_back(cache_miss((size_t)512*prev, bytes, buf_offset));
 		buf_offset += bytes;
 	    }
 
@@ -1843,24 +1839,20 @@ public:
 		nvme_offset = 512L * plba;
 	    auto slice = iovs.slice(buf_offset, buf_offset+bytes);
 	    to_read.push_back(std::make_pair(slice, nvme_offset));
-	    xprintf("wr: rd: %ld+%ld <- %ld\n", _base, _limit-_base, plba);
+	    xprintf(DBG_MAP, "wr: rd: %ld+%ld <- %ld\n", _base, _limit-_base, plba);
 	    buf_offset += bytes;
 	    prev = _limit;
 	}
 	lk.unlock();
-	for (auto [slice, nvme_offset] : to_read)
+	for (auto [slice, nvme_offset] : to_read) {
+	    assert(nvme_offset + slice.bytes() <= dev_max);
 	    if (preadv(fd, slice.data(), slice.size(), nvme_offset) < 0)
 		throw_fs_error("wcache_read");
+	}
 
 	if (prev < limit)
-	    misses->push_back(std::tuple(512 * prev, 512 * (limit - prev), buf_offset));
-    }
-#endif
-    
-    void aio_readv(size_t offset, const iovec *iov, int iovcnt,
-		   std::vector<cache_miss> *misses, void (*cb)(void*), void *ptr) {
-	auto w = new wc_read_aio(offset, iov, iovcnt, misses, cb, ptr, this);
-	readv_workers.put(w);
+	    misses->push_back(cache_miss(512 * prev, 512 * (limit - prev), buf_offset));
+	cb(ptr);
     }
     
     // debugging
@@ -1945,6 +1937,58 @@ public:
 	free(buf);
 	return r;
     }
+    struct rna {
+    public:
+	void *ptr;
+	char *buf;
+	iovec *iov = NULL;
+	int    iovcnt = 0;
+	void (*cb)(void*);
+	rna(void *_ptr, char *_buf, void (*_cb)(void*)) : ptr(_ptr), buf(_buf), cb(_cb) {}
+    };
+    static void rna_cb(rados_completion_t c, void *ptr) {
+	rna *r = (rna*)ptr;
+	if (r->iov != NULL) {
+	    smartiov iovs(r->iov, r->iovcnt);
+	    iovs.copy_in(r->buf);
+	    free(r->buf);
+	}
+	r->cb(r->ptr);
+	rados_aio_release(c);
+	delete r;
+    }
+    int aio_read_object(const char *name, char *buf, size_t len, size_t offset,
+			void (*cb)(void*), void *ptr)
+    {
+	rados_completion_t c;
+	rna *r = new rna(ptr, NULL, cb);
+	rados_aio_create_completion((void*)r, rna_cb, NULL, &c);
+	return rados_aio_read(io_ctx, name, c, buf, len, offset);
+    }
+    int aio_read_num_obj(int seq, char *buf, size_t len, size_t offset,
+			 void (*cb)(void*), void *ptr) {
+	auto name = std::string(prefix) + "." + hex(seq);
+	return aio_read_object(name.c_str(), buf, len, offset, cb, ptr);
+    }
+    int aio_read_objectv(const char *name, iovec *iov, int iovcnt, size_t offset,
+			 void (*cb)(void*), void *ptr)
+    {
+	rados_completion_t c;
+	smartiov iovs(iov, iovcnt);
+	int len = iovs.bytes();
+	char *buf = (char*)malloc(len);
+	rna *r = new rna(ptr, buf, cb);
+	r->iov = iov;
+	r->iovcnt = iovcnt;
+	rados_aio_create_completion((void*)r, rna_cb, NULL, &c);
+	return rados_aio_read(io_ctx, name, c, buf, len, offset);
+    }
+    int aio_read_num_objv(int seq, iovec *iov, int iovcnt, size_t offset,
+			  void (*cb)(void*), void *ptr) {
+	auto name = std::string(prefix) + "." + hex(seq);
+	return aio_read_objectv(name.c_str(), iov, iovcnt, offset, cb, ptr);
+    }
+
     ~rados_backend() {
 	free((void*)prefix);
 	rados_ioctx_destroy(io_ctx);
@@ -2040,6 +2084,9 @@ extern "C" void wcache_read(char *buf, uint64_t offset, uint64_t len)
     wait1 w;
     wcache->aio_readv(offset, &iov, 1, &misses, wait1_cb, (void*)&w);
     wait1_wait(&w);
+
+    for (auto [_base, _len, buf_offset] : misses)
+	memset(buf2 + buf_offset, 0, _len);
     memcpy(buf, buf2, len);
     free(buf2);
 }
@@ -2188,8 +2235,8 @@ extern "C" void rcache_read(char *buf, uint64_t offset, uint64_t len)
     char *buf2 = (char*)aligned_alloc(512, len); // just assume it's not
     waitq q;
     iovec iov = {buf2, len};
-    std::unique_lock lk(q.m);
     rcache->readv(offset, &iov, 1, waitq_cb, (void*)&q);
+    std::unique_lock lk(q.m);
     while (!q.done)
 	q.cv.wait(lk);
     memcpy(buf, buf2, len);
@@ -2449,6 +2496,8 @@ void rbd_aio_readv_fsm(void *ptr)
     readv_state *s = (readv_state*)ptr;
     std::unique_lock lk(m);
 
+    xprintf(DBG_AIO, "fsm %p %d\n", ptr, s->phase);
+    
     switch (s->phase) {
     case 0:
 	s->fri = (fake_rbd_image*)s->image;
@@ -2470,6 +2519,7 @@ void rbd_aio_readv_fsm(void *ptr)
 	}
 	s->phase = 1;
 	lk.unlock();
+	xprintf(DBG_AIO, " ->w readv(1) %p\n", s);
 	s->fri->wcache->aio_readv(s->off, s->tmp_iov, s->tmp_iovcnt,
 				  &s->misses, rbd_aio_readv_fsm, ptr);
 	break;
@@ -2478,28 +2528,34 @@ void rbd_aio_readv_fsm(void *ptr)
 	if (s->misses.size() == 0) {
 	    s->phase = 3;
 	    lk.unlock();
-	    xprintf(" ->fsm\n");
+	    xprintf(DBG_AIO, " ->fsm(3) %p\n", s);
 	    rbd_aio_readv_fsm(ptr);
 	}
 	else {
 	    auto iovs2 = smartiov(s->tmp_iov, s->tmp_iovcnt);
 	    s->phase = 2;
 	    lk.unlock();
+	    std::vector<std::pair<smartiov,size_t>> readv_work;
 	    for (auto [_off, _len, buf_offset] : s->misses) {
-		auto slice = iovs2.slice(buf_offset, buf_offset+_len);
+		readv_work.push_back(std::pair(iovs2.slice(buf_offset, buf_offset+_len),
+					       _off));
 		s->n++;
+	    }
+	    for (auto [slice, _off] : readv_work) 
 		s->fri->rcache->readv(_off, slice.data(), slice.size(),
 				      rbd_aio_readv_fsm, ptr);
-	    }
 	}
 	break;
 	
     case 2:
-	if (--(s->n) == 0) {
+    {int n = --(s->n);
+	xprintf(DBG_AIO, " n->%d %p\n", n, s);
+	if (n == 0) {
 	    s->phase = 3;
 	    lk.unlock();
 	    rbd_aio_readv_fsm(ptr);
 	}
+    }
 	break;
        
     case 3: {
diff --git a/lsvd_types.py b/lsvd_types.py
index 3784c87..9f46c2b 100644
--- a/lsvd_types.py
+++ b/lsvd_types.py
@@ -107,55 +107,6 @@ class obj_offset(LittleEndianStructure):
                 ("offset", c_ulong, 28)]
 sizeof_obj_offset = sizeof(obj_offset)
 
-rsuper = None
-def rcache_init(blkno):
-    global rsuper
-    assert _fd != -1
-    lsvd_lib.rcache_init(c_uint(blkno), c_int(_fd))
-    rsuper = j_read_super()
-    lsvd_lib.rcache_getsuper(byref(rsuper))
-
-def rcache_shutdown():
-    lsvd_lib.rcache_shutdown()
-
-def rcache_read(offset, nbytes):
-    assert (nbytes % 512) == 0 and (offset % 512) == 0
-    buf = (c_char * nbytes)()
-    lsvd_lib.rcache_read(buf, c_ulong(offset), c_ulong(nbytes))
-    return buf[0:nbytes]
-
-def rcache_add(obj, offset, data):
-    nbytes = len(data)
-    assert (nbytes % 512) == 0
-    lsvd_lib.rcache_add(c_int(obj), c_int(offset), data, c_int(nbytes), data)
-    
-def rcache_getmap():
-    n = rsuper.units
-    k = (obj_offset * n)()
-    v = (c_int * n)()
-    m = lsvd_lib.rcache_getmap(byref(k), byref(v), n)
-    keys = [[_.obj, _.offset] for _ in k[0:m]]
-    vals = v[:]
-    return list(zip(keys, vals))
-    
-def rcache_flatmap():
-    n = rsuper.units
-    vals = (obj_offset * n)()
-    n = lsvd_lib.rcache_get_flat(byref(vals), c_int(n))
-    return [[_.obj,_.offset] for _ in vals[0:n]]
-
-def rcache_bitmap():
-    n = rsuper.units
-    vals = (c_ushort * n)()
-    n = lsvd_lib.rcache_get_masks(byref(vals), c_int(n))
-    return vals[0:n]
-
-def rcache_evict(n):
-    lsvd_lib.rcache_evict(c_int(n))
-
-def rcache_reset():
-    lsvd_lib.rcache_reset()
-
 class j_extent(LittleEndianStructure):
     _fields_ = [("lba", c_ulong, 40),
                 ("len", c_ulong, 24)]
diff --git a/mkcache.py b/mkcache.py
index e084962..f643c11 100644
--- a/mkcache.py
+++ b/mkcache.py
@@ -89,7 +89,7 @@ if __name__ == '__main__':
         r_units = int(0.75*pages) // 16
         r_pages = r_units * 16
         r_oh = div_round_up(r_units*(2+lsvd.sizeof_obj_offset), 4096)
-        w_pages = pages - r_pages - 3     # mkcache subtracts write metadata
+        w_pages = pages - r_pages - r_oh - 3     # mkcache subtracts write metadata
     
         mkcache(args.device, uuid, write_zeros=False, wblks=w_pages, rblks=r_pages)
     else:
diff --git a/test1.py b/test1.py
index 3b93c4f..9eda34b 100644
--- a/test1.py
+++ b/test1.py
@@ -7,11 +7,11 @@
 import unittest
 import lsvd
 import os
+import mkdisk
 
 img = "/tmp/bkt/obj"
-
-val = os.system("./mkdisk --size 10m %s" % img)
-assert val == 0
+sectors = 10*1024*2 # 10MB
+mkdisk.mkdisk(img, sectors)
 
 _size = lsvd.init(img, 1, False)
 assert _size == 10*1024*1024
diff --git a/test3.py b/test3.py
index 1787ec0..53db3fe 100644
--- a/test3.py
+++ b/test3.py
@@ -5,6 +5,7 @@ import lsvd
 import os
 import ctypes
 import time
+import mkdisk
 import mkcache
 import test2 as t2
 
@@ -15,8 +16,8 @@ fd2 = -1
 
 def startup():
     global fd2
-    val = os.system("./mkdisk --size 10m %s" % img)
-    assert val == 0
+    sectors = 10*1024*2 # 10MB
+    mkdisk.mkdisk(img, sectors)
     lsvd.init(img, 1, False)
     mkcache.mkcache(nvme)
     lsvd.cache_open(nvme)
