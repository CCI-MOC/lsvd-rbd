+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
++ date +%FT%T
+ cur_time=2024-01-14T02:45:13
+ default_cache_size=128849018880
+ cache_size=128849018880
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=120
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T02:45:13.rbd-multi.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=rbd-benchmark
+ imgsize=10G
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ rbd -p rssd2 rm rbd-benchmark.multi.1
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 rm rbd-benchmark.multi.2
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 rm rbd-benchmark.multi.3
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 rm rbd-benchmark.multi.4
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.1
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.2
+ wait
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.3
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.4
Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 11% complete...Thick provisioning: 10% complete...Thick provisioning: 10% complete...Thick provisioning: 10% complete...Thick provisioning: 12% complete...Thick provisioning: 11% complete...Thick provisioning: 11% complete...Thick provisioning: 11% complete...Thick provisioning: 13% complete...Thick provisioning: 12% complete...Thick provisioning: 12% complete...Thick provisioning: 14% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 15% complete...Thick provisioning: 13% complete...Thick provisioning: 13% complete...Thick provisioning: 16% complete...Thick provisioning: 14% complete...Thick provisioning: 14% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 15% complete...Thick provisioning: 15% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 16% complete...Thick provisioning: 16% complete...Thick provisioning: 16% complete...Thick provisioning: 19% complete...Thick provisioning: 17% complete...Thick provisioning: 17% complete...Thick provisioning: 17% complete...Thick provisioning: 20% complete...Thick provisioning: 18% complete...Thick provisioning: 18% complete...Thick provisioning: 18% complete...Thick provisioning: 21% complete...Thick provisioning: 19% complete...Thick provisioning: 19% complete...Thick provisioning: 19% complete...Thick provisioning: 22% complete...Thick provisioning: 20% complete...Thick provisioning: 20% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 23% complete...Thick provisioning: 21% complete...Thick provisioning: 21% complete...Thick provisioning: 24% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 25% complete...Thick provisioning: 23% complete...Thick provisioning: 23% complete...Thick provisioning: 23% complete...Thick provisioning: 26% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 27% complete...Thick provisioning: 25% complete...Thick provisioning: 25% complete...Thick provisioning: 25% complete...Thick provisioning: 28% complete...Thick provisioning: 26% complete...Thick provisioning: 26% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 29% complete...Thick provisioning: 27% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 30% complete...Thick provisioning: 28% complete...Thick provisioning: 28% complete...Thick provisioning: 31% complete...Thick provisioning: 29% complete...Thick provisioning: 29% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 32% complete...Thick provisioning: 30% complete...Thick provisioning: 30% complete...Thick provisioning: 33% complete...Thick provisioning: 31% complete...Thick provisioning: 31% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 34% complete...Thick provisioning: 32% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 33% complete...Thick provisioning: 35% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 36% complete...Thick provisioning: 34% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 35% complete...Thick provisioning: 37% complete...Thick provisioning: 35% complete...Thick provisioning: 38% complete...Thick provisioning: 36% complete...Thick provisioning: 36% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 37% complete...Thick provisioning: 39% complete...Thick provisioning: 37% complete...Thick provisioning: 40% complete...Thick provisioning: 38% complete...Thick provisioning: 38% complete...Thick provisioning: 38% complete...Thick provisioning: 41% complete...Thick provisioning: 39% complete...Thick provisioning: 39% complete...Thick provisioning: 42% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 40% complete...Thick provisioning: 43% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 44% complete...Thick provisioning: 41% complete...Thick provisioning: 41% complete...Thick provisioning: 45% complete...Thick provisioning: 42% complete...Thick provisioning: 42% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 46% complete...Thick provisioning: 43% complete...Thick provisioning: 43% complete...Thick provisioning: 47% complete...Thick provisioning: 44% complete...Thick provisioning: 44% complete...Thick provisioning: 44% complete...Thick provisioning: 48% complete...Thick provisioning: 45% complete...Thick provisioning: 45% complete...Thick provisioning: 45% complete...Thick provisioning: 49% complete...Thick provisioning: 46% complete...Thick provisioning: 46% complete...Thick provisioning: 50% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 51% complete...Thick provisioning: 47% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 48% complete...Thick provisioning: 52% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 49% complete...Thick provisioning: 53% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 50% complete...Thick provisioning: 54% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 55% complete...Thick provisioning: 51% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 56% complete...Thick provisioning: 52% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 57% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 53% complete...Thick provisioning: 58% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 59% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 60% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 61% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 62% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 63% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 64% complete...Thick provisioning: 60% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 65% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 66% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 67% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 68% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 69% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 70% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 71% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 72% complete...Thick provisioning: 68% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 73% complete...Thick provisioning: 69% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 74% complete...Thick provisioning: 70% complete...Thick provisioning: 69% complete...Thick provisioning: 75% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 76% complete...Thick provisioning: 72% complete...Thick provisioning: 71% complete...Thick provisioning: 77% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 78% complete...Thick provisioning: 74% complete...Thick provisioning: 73% complete...Thick provisioning: 79% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 74% complete...Thick provisioning: 80% complete...Thick provisioning: 76% complete...Thick provisioning: 75% complete...Thick provisioning: 75% complete...Thick provisioning: 81% complete...Thick provisioning: 77% complete...Thick provisioning: 76% complete...Thick provisioning: 76% complete...Thick provisioning: 82% complete...Thick provisioning: 78% complete...Thick provisioning: 77% complete...Thick provisioning: 83% complete...Thick provisioning: 77% complete...Thick provisioning: 79% complete...Thick provisioning: 78% complete...Thick provisioning: 80% complete...Thick provisioning: 84% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 81% complete...Thick provisioning: 79% complete...Thick provisioning: 85% complete...Thick provisioning: 80% complete...Thick provisioning: 80% complete...Thick provisioning: 82% complete...Thick provisioning: 86% complete...Thick provisioning: 81% complete...Thick provisioning: 81% complete...Thick provisioning: 83% complete...Thick provisioning: 87% complete...Thick provisioning: 82% complete...Thick provisioning: 84% complete...Thick provisioning: 88% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 85% complete...Thick provisioning: 89% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 90% complete...Thick provisioning: 86% complete...Thick provisioning: 85% complete...Thick provisioning: 84% complete...Thick provisioning: 91% complete...Thick provisioning: 87% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 92% complete...Thick provisioning: 88% complete...Thick provisioning: 87% complete...Thick provisioning: 86% complete...Thick provisioning: 93% complete...Thick provisioning: 88% complete...Thick provisioning: 87% complete...Thick provisioning: 89% complete...Thick provisioning: 94% complete...Thick provisioning: 89% complete...Thick provisioning: 88% complete...Thick provisioning: 90% complete...Thick provisioning: 95% complete...Thick provisioning: 90% complete...Thick provisioning: 89% complete...Thick provisioning: 91% complete...Thick provisioning: 96% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 92% complete...Thick provisioning: 91% complete...Thick provisioning: 97% complete...Thick provisioning: 92% complete...Thick provisioning: 93% complete...Thick provisioning: 98% complete...Thick provisioning: 94% complete...Thick provisioning: 93% complete...Thick provisioning: 92% complete...Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 99% complete...Thick provisioning: 93% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 94% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 95% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 96% complete...Thick provisioning: 99% complete...Thick provisioning: 98% complete...Thick provisioning: 97% complete...Thick provisioning: 100% complete...Thick provisioning: 99% complete...Thick provisioning: 98% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 100% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 100% complete...Thick provisioning: 100% complete...done.
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ fstrim /mnt/nvme
+ launch_gw_background
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ sleep 5
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-14 02:46:46.277574] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-14 02:46:46.277747] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid459886 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-14 02:46:46.379396] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-14 02:46:46.528337] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-14 02:46:46.528403] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-14 02:46:46.528468] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-14 02:46:46.528474] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-14 02:46:52.043307] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-14 02:46:52.834247] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 rbd-benchmark.multi.1
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.1
+ local bdev=bdev_rbd-benchmark.multi.1
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.1 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.1
[2024-01-14 02:46:53.212328] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.1 rbd disk to lun
bdev_rbd-benchmark.multi.1
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.1
+ add_rbd_img rssd2 rbd-benchmark.multi.2
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.2
+ local bdev=bdev_rbd-benchmark.multi.2
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.2 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.2
[2024-01-14 02:46:53.985990] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.2 rbd disk to lun
bdev_rbd-benchmark.multi.2
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.2
+ add_rbd_img rssd2 rbd-benchmark.multi.3
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.3
+ local bdev=bdev_rbd-benchmark.multi.3
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.3 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.3
[2024-01-14 02:46:54.772212] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.3 rbd disk to lun
bdev_rbd-benchmark.multi.3
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.3
+ add_rbd_img rssd2 rbd-benchmark.multi.4
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.4
+ local bdev=bdev_rbd-benchmark.multi.4
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.4 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.4
[2024-01-14 02:46:55.526130] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.4 rbd disk to lun
bdev_rbd-benchmark.multi.4
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.4
+ trap 'cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T02:45:13.rbd-multi.rssd2.txt multi-client/client-bench-multi.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T02:45:13.rbd-multi.rssd2.txt
+ local benchscript=multi-client/client-bench-multi.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T02:45:13.rbd-multi.rssd2.txt
===Starting client benchmark

NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
device: nvme1
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n2          SPDK00000000000001   SPDK_Controller1                         2          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n3          SPDK00000000000001   SPDK_Controller1                         3          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n4          SPDK00000000000001   SPDK_Controller1                         4          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
Using device /dev/nvme1n1
/dev/nvme1n2
/dev/nvme1n3
/dev/nvme1n4


===Fio: workload=randread, time=60, iodepth=256, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=51609: Sun Jan 14 02:48:03 2024
  read: IOPS=60.6k, BW=237MiB/s (248MB/s)(13.9GiB/60004msec)
    slat (usec): min=4, max=12361, avg=62.29, stdev=102.12
    clat (usec): min=1190, max=100708, avg=16835.02, stdev=7341.21
     lat (usec): min=1195, max=100718, avg=16897.78, stdev=7363.02
    clat percentiles (usec):
     |  1.00th=[ 4883],  5.00th=[ 6128], 10.00th=[ 7898], 20.00th=[10683],
     | 30.00th=[12518], 40.00th=[14091], 50.00th=[16057], 60.00th=[17957],
     | 70.00th=[20055], 80.00th=[22414], 90.00th=[26084], 95.00th=[30016],
     | 99.00th=[39060], 99.50th=[42730], 99.90th=[51643], 99.95th=[55313],
     | 99.99th=[63701]
   bw (  KiB/s): min=108128, max=448640, per=100.00%, avg=242697.34, stdev=20133.53, samples=476
   iops        : min=27032, max=112160, avg=60674.34, stdev=5033.38, samples=476
  lat (msec)   : 2=0.01%, 4=0.07%, 10=16.98%, 20=53.29%, 50=29.52%
  lat (msec)   : 100=0.13%, 250=0.01%
  cpu          : usr=8.22%, sys=19.21%, ctx=1934380, majf=0, minf=1474
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3635018,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=237MiB/s (248MB/s), 237MiB/s-237MiB/s (248MB/s-248MB/s), io=13.9GiB (14.9GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=60.6k, BW=237MiB/s (248MB/s)(13.9GiB/60004msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process

j1: (groupid=0, jobs=1): err= 0: pid=51663: Sun Jan 14 02:48:35 2024
  read: IOPS=48.8k, BW=190MiB/s (200MB/s)(5715MiB/30002msec)
    slat (usec): min=5, max=1970, avg=17.47, stdev=54.81
    clat (usec): min=603, max=45860, avg=2604.57, stdev=870.93
     lat (usec): min=649, max=45870, avg=2622.40, stdev=869.47
    clat percentiles (usec):
     |  1.00th=[ 1139],  5.00th=[ 1434], 10.00th=[ 1795], 20.00th=[ 2147],
     | 30.00th=[ 2311], 40.00th=[ 2442], 50.00th=[ 2540], 60.00th=[ 2638],
     | 70.00th=[ 2769], 80.00th=[ 2966], 90.00th=[ 3294], 95.00th=[ 3752],
     | 99.00th=[ 5800], 99.50th=[ 7701], 99.90th=[ 8586], 99.95th=[10028],
     | 99.99th=[16188]
   bw (  KiB/s): min=123552, max=208688, per=100.00%, avg=195103.05, stdev=15323.01, samples=59
   iops        : min=30888, max=52172, avg=48775.76, stdev=3830.72, samples=59
  lat (usec)   : 750=0.01%, 1000=0.28%
  lat (msec)   : 2=14.22%, 4=81.86%, 10=3.58%, 20=0.04%, 50=0.01%
  cpu          : usr=19.19%, sys=42.53%, ctx=120104, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1462994,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=190MiB/s (200MB/s), 190MiB/s-190MiB/s (200MB/s-200MB/s), io=5715MiB (5992MB), run=30002-30002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=48.8k, BW=190MiB/s (200MB/s)(5715MiB/30002msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes

j1: (groupid=0, jobs=2): err= 0: pid=51711: Sun Jan 14 02:49:08 2024
  read: IOPS=59.3k, BW=231MiB/s (243MB/s)(6945MiB/30006msec)
    slat (usec): min=4, max=3768, avg=30.48, stdev=88.92
    clat (usec): min=515, max=60078, avg=4286.96, stdev=3286.51
     lat (usec): min=542, max=60154, avg=4317.82, stdev=3286.56
    clat percentiles (usec):
     |  1.00th=[  857],  5.00th=[ 1037], 10.00th=[ 1188], 20.00th=[ 1450],
     | 30.00th=[ 1860], 40.00th=[ 2835], 50.00th=[ 3916], 60.00th=[ 4424],
     | 70.00th=[ 4883], 80.00th=[ 6063], 90.00th=[ 8586], 95.00th=[10945],
     | 99.00th=[15664], 99.50th=[17433], 99.90th=[21365], 99.95th=[23725],
     | 99.99th=[39060]
   bw (  KiB/s): min=182832, max=310472, per=100.00%, avg=237039.05, stdev=13386.71, samples=118
   iops        : min=45708, max=77618, avg=59259.80, stdev=3346.68, samples=118
  lat (usec)   : 750=0.19%, 1000=3.68%
  lat (msec)   : 2=28.34%, 4=18.98%, 10=42.12%, 20=6.52%, 50=0.16%
  lat (msec)   : 100=0.01%
  cpu          : usr=12.39%, sys=28.93%, ctx=441016, majf=0, minf=277
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1777909,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=231MiB/s (243MB/s), 231MiB/s-231MiB/s (243MB/s-243MB/s), io=6945MiB (7282MB), run=30006-30006msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=59.3k, BW=231MiB/s (243MB/s)(6945MiB/30006msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes

j1: (groupid=0, jobs=3): err= 0: pid=51759: Sun Jan 14 02:49:40 2024
  read: IOPS=64.3k, BW=251MiB/s (264MB/s)(7541MiB/30002msec)
    slat (usec): min=4, max=5848, avg=42.97, stdev=108.22
    clat (usec): min=512, max=52386, avg=5921.48, stdev=4658.44
     lat (usec): min=558, max=52394, avg=5964.88, stdev=4664.46
    clat percentiles (usec):
     |  1.00th=[  865],  5.00th=[ 1106], 10.00th=[ 1319], 20.00th=[ 1860],
     | 30.00th=[ 3261], 40.00th=[ 4228], 50.00th=[ 4948], 60.00th=[ 5604],
     | 70.00th=[ 6718], 80.00th=[ 8586], 90.00th=[11994], 95.00th=[15270],
     | 99.00th=[22938], 99.50th=[25560], 99.90th=[29492], 99.95th=[31065],
     | 99.99th=[36963]
   bw (  KiB/s): min=165075, max=379960, per=100.00%, avg=257555.31, stdev=18752.86, samples=177
   iops        : min=41268, max=94990, avg=64388.63, stdev=4688.21, samples=177
  lat (usec)   : 750=0.26%, 1000=2.56%
  lat (msec)   : 2=18.94%, 4=15.09%, 10=47.92%, 20=13.23%, 50=2.00%
  lat (msec)   : 100=0.01%
  cpu          : usr=10.38%, sys=23.53%, ctx=539947, majf=0, minf=417
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1930476,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=251MiB/s (264MB/s), 251MiB/s-251MiB/s (264MB/s-264MB/s), io=7541MiB (7907MB), run=30002-30002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=64.3k, BW=251MiB/s (264MB/s)(7541MiB/30002msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=51809: Sun Jan 14 02:50:13 2024
  read: IOPS=62.3k, BW=243MiB/s (255MB/s)(7305MiB/30007msec)
    slat (usec): min=4, max=5060, avg=60.43, stdev=149.73
    clat (usec): min=661, max=43602, avg=8151.15, stdev=3234.46
     lat (usec): min=694, max=43615, avg=8212.02, stdev=3244.35
    clat percentiles (usec):
     |  1.00th=[ 2704],  5.00th=[ 4228], 10.00th=[ 5080], 20.00th=[ 6128],
     | 30.00th=[ 6718], 40.00th=[ 7177], 50.00th=[ 7504], 60.00th=[ 7963],
     | 70.00th=[ 8586], 80.00th=[ 9765], 90.00th=[11600], 95.00th=[14353],
     | 99.00th=[20579], 99.50th=[23200], 99.90th=[28443], 99.95th=[31065],
     | 99.99th=[35914]
   bw (  KiB/s): min=151784, max=374208, per=100.00%, avg=250220.47, stdev=11797.09, samples=236
   iops        : min=37946, max=93552, avg=62555.19, stdev=2949.27, samples=236
  lat (usec)   : 750=0.01%, 1000=0.03%
  lat (msec)   : 2=0.37%, 4=3.57%, 10=78.04%, 20=16.82%, 50=1.17%
  cpu          : usr=7.51%, sys=18.07%, ctx=516932, majf=0, minf=780
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1870116,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=243MiB/s (255MB/s), 243MiB/s-243MiB/s (255MB/s-255MB/s), io=7305MiB (7660MB), run=30007-30007msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=62.3k, BW=243MiB/s (255MB/s)(7305MiB/30007msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process

j1: (groupid=0, jobs=1): err= 0: pid=51858: Sun Jan 14 02:50:45 2024
  read: IOPS=3623, BW=14.2MiB/s (14.8MB/s)(425MiB/30038msec)
    slat (usec): min=4, max=9749, avg=267.22, stdev=206.80
    clat (usec): min=641, max=66202, avg=35048.39, stdev=10081.12
     lat (usec): min=725, max=66226, avg=35316.53, stdev=10134.46
    clat percentiles (usec):
     |  1.00th=[ 2442],  5.00th=[ 9110], 10.00th=[17433], 20.00th=[31589],
     | 30.00th=[37487], 40.00th=[38011], 50.00th=[38536], 60.00th=[39060],
     | 70.00th=[40109], 80.00th=[40633], 90.00th=[42206], 95.00th=[43254],
     | 99.00th=[45351], 99.50th=[45876], 99.90th=[51119], 99.95th=[57410],
     | 99.99th=[64750]
   bw (  KiB/s): min=12856, max=17920, per=100.00%, avg=14496.13, stdev=776.78, samples=60
   iops        : min= 3214, max= 4480, avg=3624.03, stdev=194.20, samples=60
  lat (usec)   : 750=0.01%, 1000=0.04%
  lat (msec)   : 2=0.67%, 4=1.20%, 10=3.61%, 20=6.07%, 50=88.26%
  lat (msec)   : 100=0.15%
  cpu          : usr=3.67%, sys=10.67%, ctx=97832, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=108848,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=14.2MiB/s (14.8MB/s), 14.2MiB/s-14.2MiB/s (14.8MB/s-14.8MB/s), io=425MiB (446MB), run=30038-30038msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) read:  read: IOPS=3623, BW=14.2MiB/s (14.8MB/s)(425MiB/30038msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes

j1: (groupid=0, jobs=2): err= 0: pid=51904: Sun Jan 14 02:51:18 2024
  read: IOPS=7533, BW=29.4MiB/s (30.9MB/s)(883MiB/30020msec)
    slat (usec): min=4, max=24362, avg=257.95, stdev=281.79
    clat (usec): min=479, max=83863, avg=33716.15, stdev=12451.81
     lat (usec): min=587, max=85372, avg=33974.94, stdev=12537.39
    clat percentiles (usec):
     |  1.00th=[ 4555],  5.00th=[10290], 10.00th=[11207], 20.00th=[22676],
     | 30.00th=[34866], 40.00th=[38011], 50.00th=[38536], 60.00th=[39060],
     | 70.00th=[39584], 80.00th=[40633], 90.00th=[42730], 95.00th=[44827],
     | 99.00th=[63177], 99.50th=[76022], 99.90th=[80217], 99.95th=[81265],
     | 99.99th=[82314]
   bw (  KiB/s): min=19712, max=53016, per=99.95%, avg=30120.53, stdev=3854.00, samples=120
   iops        : min= 4928, max=13254, avg=7530.13, stdev=963.50, samples=120
  lat (usec)   : 500=0.01%, 750=0.02%, 1000=0.04%
  lat (msec)   : 2=0.25%, 4=0.49%, 10=3.82%, 20=14.44%, 50=78.97%
  lat (msec)   : 100=1.96%
  cpu          : usr=3.16%, sys=9.33%, ctx=204688, majf=0, minf=278
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=226158,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=29.4MiB/s (30.9MB/s), 29.4MiB/s-29.4MiB/s (30.9MB/s-30.9MB/s), io=883MiB (926MB), run=30020-30020msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) read:  read: IOPS=7533, BW=29.4MiB/s (30.9MB/s)(883MiB/30020msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes

j1: (groupid=0, jobs=3): err= 0: pid=51951: Sun Jan 14 02:51:50 2024
  read: IOPS=11.5k, BW=44.8MiB/s (46.9MB/s)(1345MiB/30040msec)
    slat (usec): min=4, max=35752, avg=255.57, stdev=323.10
    clat (usec): min=422, max=121051, avg=33228.87, stdev=13295.03
     lat (usec): min=468, max=121435, avg=33485.24, stdev=13384.50
    clat percentiles (msec):
     |  1.00th=[    3],  5.00th=[    9], 10.00th=[   11], 20.00th=[   23],
     | 30.00th=[   32], 40.00th=[   37], 50.00th=[   39], 60.00th=[   40],
     | 70.00th=[   40], 80.00th=[   41], 90.00th=[   43], 95.00th=[   45],
     | 99.00th=[   78], 99.50th=[   84], 99.90th=[   88], 99.95th=[  107],
     | 99.99th=[  121]
   bw (  KiB/s): min=23160, max=73880, per=100.00%, avg=45877.16, stdev=3135.53, samples=179
   iops        : min= 5790, max=18470, avg=11469.20, stdev=783.89, samples=179
  lat (usec)   : 500=0.01%, 750=0.04%, 1000=0.08%
  lat (msec)   : 2=0.47%, 4=0.88%, 10=5.80%, 20=11.53%, 50=78.91%
  lat (msec)   : 100=2.23%, 250=0.05%
  cpu          : usr=2.98%, sys=8.59%, ctx=316594, majf=0, minf=422
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=344258,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=44.8MiB/s (46.9MB/s), 44.8MiB/s-44.8MiB/s (46.9MB/s-46.9MB/s), io=1345MiB (1410MB), run=30040-30040msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) read:  read: IOPS=11.5k, BW=44.8MiB/s (46.9MB/s)(1345MiB/30040msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=51999: Sun Jan 14 02:52:23 2024
  read: IOPS=14.8k, BW=57.8MiB/s (60.6MB/s)(1735MiB/30044msec)
    slat (usec): min=4, max=60491, avg=263.49, stdev=384.05
    clat (usec): min=464, max=182750, avg=34336.58, stdev=15173.43
     lat (usec): min=624, max=183151, avg=34600.89, stdev=15273.81
    clat percentiles (msec):
     |  1.00th=[    3],  5.00th=[   10], 10.00th=[   12], 20.00th=[   23],
     | 30.00th=[   35], 40.00th=[   38], 50.00th=[   39], 60.00th=[   40],
     | 70.00th=[   40], 80.00th=[   41], 90.00th=[   43], 95.00th=[   46],
     | 99.00th=[   82], 99.50th=[  113], 99.90th=[  159], 99.95th=[  165],
     | 99.99th=[  174]
   bw (  KiB/s): min=32856, max=91768, per=99.90%, avg=59092.52, stdev=2908.29, samples=238
   iops        : min= 8214, max=22942, avg=14773.13, stdev=727.07, samples=238
  lat (usec)   : 500=0.01%, 750=0.03%, 1000=0.08%
  lat (msec)   : 2=0.47%, 4=0.89%, 10=4.55%, 20=12.57%, 50=77.75%
  lat (msec)   : 100=2.94%, 250=0.71%
  cpu          : usr=2.94%, sys=8.40%, ctx=418224, majf=0, minf=560
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=444283,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=57.8MiB/s (60.6MB/s), 57.8MiB/s-57.8MiB/s (60.6MB/s-60.6MB/s), io=1735MiB (1820MB), run=30044-30044msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=14.8k, BW=57.8MiB/s (60.6MB/s)(1735MiB/30044msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52049: Sun Jan 14 02:52:55 2024
  read: IOPS=61.1k, BW=239MiB/s (250MB/s)(7163MiB/30003msec)
    slat (usec): min=4, max=5975, avg=61.55, stdev=118.08
    clat (usec): min=510, max=46818, avg=8311.44, stdev=4365.62
     lat (usec): min=554, max=46842, avg=8373.47, stdev=4380.52
    clat percentiles (usec):
     |  1.00th=[ 1106],  5.00th=[ 1811], 10.00th=[ 3294], 20.00th=[ 5014],
     | 30.00th=[ 6063], 40.00th=[ 6849], 50.00th=[ 7767], 60.00th=[ 8717],
     | 70.00th=[ 9765], 80.00th=[11207], 90.00th=[13566], 95.00th=[16319],
     | 99.00th=[22676], 99.50th=[25297], 99.90th=[30802], 99.95th=[33162],
     | 99.99th=[38011]
   bw (  KiB/s): min=140168, max=389664, per=100.00%, avg=245046.24, stdev=16685.70, samples=236
   iops        : min=35042, max=97416, avg=61261.56, stdev=4171.42, samples=236
  lat (usec)   : 750=0.02%, 1000=0.49%
  lat (msec)   : 2=5.41%, 4=6.93%, 10=59.41%, 20=25.66%, 50=2.08%
  cpu          : usr=8.00%, sys=19.61%, ctx=819583, majf=0, minf=803
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1833790,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=239MiB/s (250MB/s), 239MiB/s-239MiB/s (250MB/s-250MB/s), io=7163MiB (7511MB), run=30003-30003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=61.1k, BW=239MiB/s (250MB/s)(7163MiB/30003msec)


===Fio: workload=randread, time=30, iodepth=128, bs=8ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52100: Sun Jan 14 02:53:27 2024
  read: IOPS=53.8k, BW=421MiB/s (441MB/s)(12.3GiB/30003msec)
    slat (usec): min=4, max=8127, avg=70.60, stdev=113.21
    clat (usec): min=538, max=74558, avg=9435.12, stdev=5081.65
     lat (usec): min=550, max=75129, avg=9506.17, stdev=5100.75
    clat percentiles (usec):
     |  1.00th=[ 1045],  5.00th=[ 1745], 10.00th=[ 3687], 20.00th=[ 5866],
     | 30.00th=[ 7177], 40.00th=[ 8029], 50.00th=[ 8717], 60.00th=[ 9503],
     | 70.00th=[10683], 80.00th=[12649], 90.00th=[15533], 95.00th=[18744],
     | 99.00th=[26608], 99.50th=[30278], 99.90th=[38011], 99.95th=[41681],
     | 99.99th=[52167]
   bw (  KiB/s): min=198171, max=758416, per=99.87%, avg=430146.75, stdev=37651.58, samples=236
   iops        : min=24770, max=94802, avg=53767.83, stdev=4706.44, samples=236
  lat (usec)   : 750=0.02%, 1000=0.73%
  lat (msec)   : 2=5.07%, 4=5.39%, 10=53.66%, 20=31.26%, 50=3.86%
  lat (msec)   : 100=0.01%
  cpu          : usr=7.14%, sys=16.74%, ctx=934765, majf=0, minf=1071
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1615340,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=421MiB/s (441MB/s), 421MiB/s-421MiB/s (441MB/s-441MB/s), io=12.3GiB (13.2GB), run=30003-30003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=8ki) randread:  read: IOPS=53.8k, BW=421MiB/s (441MB/s)(12.3GiB/30003msec)


===Fio: workload=randread, time=30, iodepth=128, bs=16ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52149: Sun Jan 14 02:54:00 2024
  read: IOPS=47.2k, BW=738MiB/s (774MB/s)(21.6GiB/30003msec)
    slat (usec): min=4, max=4811, avg=80.37, stdev=101.63
    clat (usec): min=708, max=54760, avg=10751.60, stdev=6261.79
     lat (usec): min=764, max=54851, avg=10832.51, stdev=6280.05
    clat percentiles (usec):
     |  1.00th=[ 1205],  5.00th=[ 1696], 10.00th=[ 3392], 20.00th=[ 5866],
     | 30.00th=[ 6718], 40.00th=[ 8455], 50.00th=[10028], 60.00th=[11338],
     | 70.00th=[12911], 80.00th=[15008], 90.00th=[19006], 95.00th=[22676],
     | 99.00th=[30540], 99.50th=[33817], 99.90th=[40109], 99.95th=[42206],
     | 99.99th=[46400]
   bw (  KiB/s): min=417536, max=1271808, per=99.98%, avg=755779.25, stdev=51187.53, samples=236
   iops        : min=26096, max=79488, avg=47236.20, stdev=3199.22, samples=236
  lat (usec)   : 750=0.01%, 1000=0.11%
  lat (msec)   : 2=6.53%, 4=3.74%, 10=39.53%, 20=41.79%, 50=8.30%
  lat (msec)   : 100=0.01%
  cpu          : usr=6.88%, sys=17.12%, ctx=1038999, majf=0, minf=2094
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1417566,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=738MiB/s (774MB/s), 738MiB/s-738MiB/s (774MB/s-774MB/s), io=21.6GiB (23.2GB), run=30003-30003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=16ki) randread:  read: IOPS=47.2k, BW=738MiB/s (774MB/s)(21.6GiB/30003msec)


===Fio: workload=randread, time=30, iodepth=128, bs=32ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52201: Sun Jan 14 02:54:32 2024
  read: IOPS=37.0k, BW=1157MiB/s (1213MB/s)(33.9GiB/30011msec)
    slat (usec): min=5, max=11753, avg=103.31, stdev=154.49
    clat (usec): min=706, max=112793, avg=13722.79, stdev=8825.63
     lat (usec): min=783, max=113012, avg=13826.66, stdev=8848.64
    clat percentiles (usec):
     |  1.00th=[ 1582],  5.00th=[ 2278], 10.00th=[ 3261], 20.00th=[ 7177],
     | 30.00th=[ 8586], 40.00th=[ 9896], 50.00th=[11338], 60.00th=[13698],
     | 70.00th=[16909], 80.00th=[20579], 90.00th=[25560], 95.00th=[30278],
     | 99.00th=[41157], 99.50th=[45876], 99.90th=[55837], 99.95th=[60556],
     | 99.99th=[72877]
   bw (  MiB/s): min=  625, max= 1744, per=100.00%, avg=1159.21, stdev=81.08, samples=236
   iops        : min=20002, max=55834, avg=37094.63, stdev=2594.59, samples=236
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=3.33%, 4=8.91%, 10=28.78%, 20=37.47%, 50=21.25%
  lat (msec)   : 100=0.26%, 250=0.01%
  cpu          : usr=6.03%, sys=16.78%, ctx=925402, majf=0, minf=4937
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1110888,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=1157MiB/s (1213MB/s), 1157MiB/s-1157MiB/s (1213MB/s-1213MB/s), io=33.9GiB (36.4GB), run=30011-30011msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=32ki) randread:  read: IOPS=37.0k, BW=1157MiB/s (1213MB/s)(33.9GiB/30011msec)


===Fio: workload=randread, time=30, iodepth=128, bs=64ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52250: Sun Jan 14 02:55:05 2024
  read: IOPS=19.1k, BW=1192MiB/s (1250MB/s)(34.9GiB/30018msec)
    slat (usec): min=6, max=23118, avg=204.12, stdev=240.78
    clat (usec): min=1672, max=106925, avg=26628.21, stdev=11624.97
     lat (usec): min=1690, max=106991, avg=26832.96, stdev=11698.54
    clat percentiles (usec):
     |  1.00th=[ 7308],  5.00th=[ 9372], 10.00th=[12125], 20.00th=[16319],
     | 30.00th=[18482], 40.00th=[21890], 50.00th=[25560], 60.00th=[29754],
     | 70.00th=[33817], 80.00th=[36439], 90.00th=[39584], 95.00th=[45351],
     | 99.00th=[61080], 99.50th=[65274], 99.90th=[74974], 99.95th=[80217],
     | 99.99th=[93848]
   bw (  MiB/s): min=  660, max= 1870, per=99.96%, avg=1191.63, stdev=101.39, samples=236
   iops        : min=10568, max=29932, avg=19066.10, stdev=1622.32, samples=236
  lat (msec)   : 2=0.01%, 4=0.05%, 10=6.20%, 20=28.18%, 50=62.60%
  lat (msec)   : 100=2.96%, 250=0.01%
  cpu          : usr=3.38%, sys=10.08%, ctx=581736, majf=0, minf=8233
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=572556,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=1192MiB/s (1250MB/s), 1192MiB/s-1192MiB/s (1250MB/s-1250MB/s), io=34.9GiB (37.5GB), run=30018-30018msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=64ki) randread:  read: IOPS=19.1k, BW=1192MiB/s (1250MB/s)(34.9GiB/30018msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52304: Sun Jan 14 02:55:37 2024
  read: IOPS=14.1k, BW=54.9MiB/s (57.6MB/s)(1650MiB/30044msec)
    slat (usec): min=4, max=36667, avg=277.40, stdev=327.14
    clat (usec): min=499, max=201820, avg=36097.18, stdev=12678.83
     lat (usec): min=587, max=202144, avg=36375.41, stdev=12758.82
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[   13], 10.00th=[   19], 20.00th=[   31],
     | 30.00th=[   37], 40.00th=[   38], 50.00th=[   39], 60.00th=[   40],
     | 70.00th=[   41], 80.00th=[   42], 90.00th=[   44], 95.00th=[   46],
     | 99.00th=[   82], 99.50th=[  101], 99.90th=[  121], 99.95th=[  129],
     | 99.99th=[  201]
   bw (  KiB/s): min=34968, max=73168, per=100.00%, avg=56325.25, stdev=1886.59, samples=237
   iops        : min= 8742, max=18292, avg=14081.34, stdev=471.65, samples=237
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.04%
  lat (msec)   : 2=0.33%, 4=0.64%, 10=2.13%, 20=8.14%, 50=85.23%
  lat (msec)   : 100=2.98%, 250=0.50%
  cpu          : usr=2.89%, sys=8.21%, ctx=398695, majf=0, minf=689
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=422462,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=54.9MiB/s (57.6MB/s), 54.9MiB/s-54.9MiB/s (57.6MB/s-57.6MB/s), io=1650MiB (1730MB), run=30044-30044msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=14.1k, BW=54.9MiB/s (57.6MB/s)(1650MiB/30044msec)


===Fio: workload=read, time=30, iodepth=128, bs=8ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52353: Sun Jan 14 02:56:10 2024
  read: IOPS=13.5k, BW=105MiB/s (110MB/s)(3161MiB/30044msec)
    slat (usec): min=4, max=46760, avg=290.16, stdev=392.47
    clat (usec): min=441, max=245510, avg=37694.34, stdev=15367.30
     lat (usec): min=554, max=246453, avg=37985.38, stdev=15459.74
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[   12], 10.00th=[   17], 20.00th=[   30],
     | 30.00th=[   35], 40.00th=[   40], 50.00th=[   41], 60.00th=[   42],
     | 70.00th=[   43], 80.00th=[   44], 90.00th=[   47], 95.00th=[   53],
     | 99.00th=[   90], 99.50th=[  116], 99.90th=[  174], 99.95th=[  192],
     | 99.99th=[  230]
   bw (  KiB/s): min=61936, max=150544, per=100.00%, avg=108072.72, stdev=4180.41, samples=237
   iops        : min= 7742, max=18818, avg=13509.09, stdev=522.55, samples=237
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.05%
  lat (msec)   : 2=0.36%, 4=0.69%, 10=2.52%, 20=8.60%, 50=81.62%
  lat (msec)   : 100=5.39%, 250=0.74%
  cpu          : usr=2.91%, sys=8.39%, ctx=382373, majf=0, minf=1321
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=404600,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=105MiB/s (110MB/s), 105MiB/s-105MiB/s (110MB/s-110MB/s), io=3161MiB (3314MB), run=30044-30044msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=8ki) read:  read: IOPS=13.5k, BW=105MiB/s (110MB/s)(3161MiB/30044msec)


===Fio: workload=read, time=30, iodepth=128, bs=16ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52403: Sun Jan 14 02:56:42 2024
  read: IOPS=15.9k, BW=249MiB/s (261MB/s)(7473MiB/30028msec)
    slat (usec): min=4, max=56508, avg=244.55, stdev=332.89
    clat (usec): min=558, max=154250, avg=31889.16, stdev=15344.11
     lat (usec): min=691, max=155835, avg=32134.51, stdev=15407.76
    clat percentiles (usec):
     |  1.00th=[  1811],  5.00th=[  5997], 10.00th=[ 10945], 20.00th=[ 16909],
     | 30.00th=[ 23987], 40.00th=[ 30802], 50.00th=[ 34866], 60.00th=[ 37487],
     | 70.00th=[ 40109], 80.00th=[ 43254], 90.00th=[ 46400], 95.00th=[ 50070],
     | 99.00th=[ 77071], 99.50th=[ 90702], 99.90th=[124257], 99.95th=[131597],
     | 99.99th=[149947]
   bw (  KiB/s): min=182368, max=361408, per=100.00%, avg=255630.41, stdev=10240.08, samples=236
   iops        : min=11398, max=22588, avg=15976.81, stdev=640.01, samples=236
  lat (usec)   : 750=0.01%, 1000=0.10%
  lat (msec)   : 2=1.08%, 4=1.91%, 10=5.90%, 20=15.36%, 50=70.43%
  lat (msec)   : 100=4.89%, 250=0.31%
  cpu          : usr=3.33%, sys=9.29%, ctx=452064, majf=0, minf=2535
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=478252,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=249MiB/s (261MB/s), 249MiB/s-249MiB/s (261MB/s-261MB/s), io=7473MiB (7836MB), run=30028-30028msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=16ki) read:  read: IOPS=15.9k, BW=249MiB/s (261MB/s)(7473MiB/30028msec)


===Fio: workload=read, time=30, iodepth=128, bs=32ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52453: Sun Jan 14 02:57:15 2024
  read: IOPS=15.0k, BW=469MiB/s (492MB/s)(13.8GiB/30027msec)
    slat (usec): min=5, max=41512, avg=261.04, stdev=314.11
    clat (usec): min=1036, max=147345, avg=33817.18, stdev=12847.52
     lat (usec): min=1072, max=147982, avg=34079.00, stdev=12908.61
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[   16], 10.00th=[   18], 20.00th=[   23],
     | 30.00th=[   28], 40.00th=[   31], 50.00th=[   34], 60.00th=[   37],
     | 70.00th=[   40], 80.00th=[   43], 90.00th=[   48], 95.00th=[   55],
     | 99.00th=[   74], 99.50th=[   83], 99.90th=[  101], 99.95th=[  107],
     | 99.99th=[  121]
   bw (  KiB/s): min=338141, max=684639, per=100.00%, avg=481158.25, stdev=21044.25, samples=239
   iops        : min=10566, max=21394, avg=15035.45, stdev=657.65, samples=239
  lat (msec)   : 2=0.03%, 4=0.24%, 10=1.30%, 20=13.15%, 50=77.43%
  lat (msec)   : 100=7.75%, 250=0.10%
  cpu          : usr=3.01%, sys=9.14%, ctx=432161, majf=0, minf=5058
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=450847,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=469MiB/s (492MB/s), 469MiB/s-469MiB/s (492MB/s-492MB/s), io=13.8GiB (14.8GB), run=30027-30027msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=32ki) read:  read: IOPS=15.0k, BW=469MiB/s (492MB/s)(13.8GiB/30027msec)


===Fio: workload=read, time=30, iodepth=128, bs=64ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52504: Sun Jan 14 02:57:47 2024
  read: IOPS=10.5k, BW=658MiB/s (690MB/s)(19.3GiB/30027msec)
    slat (usec): min=6, max=63072, avg=373.55, stdev=455.35
    clat (usec): min=1388, max=227800, avg=48243.14, stdev=21199.63
     lat (usec): min=1407, max=228082, avg=48617.52, stdev=21313.95
    clat percentiles (msec):
     |  1.00th=[    7],  5.00th=[   18], 10.00th=[   24], 20.00th=[   31],
     | 30.00th=[   37], 40.00th=[   43], 50.00th=[   47], 60.00th=[   52],
     | 70.00th=[   57], 80.00th=[   63], 90.00th=[   74], 95.00th=[   86],
     | 99.00th=[  113], 99.50th=[  128], 99.90th=[  155], 99.95th=[  171],
     | 99.99th=[  228]
   bw (  KiB/s): min=387200, max=1023872, per=100.00%, avg=674158.64, stdev=50078.85, samples=236
   iops        : min= 6050, max=15998, avg=10533.73, stdev=782.48, samples=236
  lat (msec)   : 2=0.01%, 4=0.26%, 10=1.75%, 20=4.63%, 50=50.33%
  lat (msec)   : 100=41.12%, 250=1.92%
  cpu          : usr=2.12%, sys=6.99%, ctx=321335, majf=0, minf=8238
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=316113,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=658MiB/s (690MB/s), 658MiB/s-658MiB/s (690MB/s-690MB/s), io=19.3GiB (20.7GB), run=30027-30027msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=64ki) read:  read: IOPS=10.5k, BW=658MiB/s (690MB/s)(19.3GiB/30027msec)


===Fio: workload=randread, time=30, iodepth=1, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52554: Sun Jan 14 02:58:20 2024
  read: IOPS=2907, BW=11.4MiB/s (11.9MB/s)(341MiB/30002msec)
    slat (usec): min=6, max=469, avg=32.51, stdev=11.55
    clat (usec): min=610, max=10497, avg=1332.54, stdev=227.26
     lat (usec): min=634, max=10519, avg=1366.12, stdev=227.69
    clat percentiles (usec):
     |  1.00th=[  930],  5.00th=[ 1029], 10.00th=[ 1090], 20.00th=[ 1156],
     | 30.00th=[ 1221], 40.00th=[ 1270], 50.00th=[ 1319], 60.00th=[ 1369],
     | 70.00th=[ 1434], 80.00th=[ 1500], 90.00th=[ 1582], 95.00th=[ 1663],
     | 99.00th=[ 1827], 99.50th=[ 1926], 99.90th=[ 2147], 99.95th=[ 2474],
     | 99.99th=[ 9241]
   bw (  KiB/s): min=11144, max=12264, per=100.00%, avg=11643.25, stdev=50.61, samples=236
   iops        : min= 2786, max= 3066, avg=2910.81, stdev=12.66, samples=236
  lat (usec)   : 750=0.03%, 1000=3.17%
  lat (msec)   : 2=96.52%, 4=0.25%, 10=0.03%, 20=0.01%
  cpu          : usr=1.71%, sys=3.68%, ctx=87290, majf=0, minf=46
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=87233,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=11.4MiB/s (11.9MB/s), 11.4MiB/s-11.4MiB/s (11.9MB/s-11.9MB/s), io=341MiB (357MB), run=30002-30002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=1; bs=4k) randread:  read: IOPS=2907, BW=11.4MiB/s (11.9MB/s)(341MiB/30002msec)


===Fio: workload=randread, time=30, iodepth=32, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52603: Sun Jan 14 02:58:52 2024
  read: IOPS=56.9k, BW=222MiB/s (233MB/s)(6668MiB/30003msec)
    slat (usec): min=4, max=550, avg= 9.89, stdev= 5.88
    clat (usec): min=403, max=23857, avg=2236.79, stdev=1354.68
     lat (usec): min=411, max=23871, avg=2247.00, stdev=1354.75
    clat percentiles (usec):
     |  1.00th=[  816],  5.00th=[  979], 10.00th=[ 1106], 20.00th=[ 1303],
     | 30.00th=[ 1483], 40.00th=[ 1663], 50.00th=[ 1860], 60.00th=[ 2089],
     | 70.00th=[ 2409], 80.00th=[ 2868], 90.00th=[ 3818], 95.00th=[ 4817],
     | 99.00th=[ 7308], 99.50th=[ 8586], 99.90th=[12387], 99.95th=[14091],
     | 99.99th=[17957]
   bw (  KiB/s): min=184616, max=254384, per=100.00%, avg=228285.59, stdev=3953.33, samples=236
   iops        : min=46154, max=63596, avg=57071.41, stdev=988.34, samples=236
  lat (usec)   : 500=0.01%, 750=0.42%, 1000=5.19%
  lat (msec)   : 2=50.51%, 4=35.06%, 10=8.54%, 20=0.27%, 50=0.01%
  cpu          : usr=8.10%, sys=18.55%, ctx=1116367, majf=0, minf=171
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=1706952,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=222MiB/s (233MB/s), 222MiB/s-222MiB/s (233MB/s-233MB/s), io=6668MiB (6992MB), run=30003-30003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=32; bs=4k) randread:  read: IOPS=56.9k, BW=222MiB/s (233MB/s)(6668MiB/30003msec)


===Fio: workload=randread, time=30, iodepth=64, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52655: Sun Jan 14 02:59:24 2024
  read: IOPS=58.0k, BW=226MiB/s (237MB/s)(6792MiB/30004msec)
    slat (usec): min=4, max=7251, avg=45.25, stdev=77.88
    clat (usec): min=396, max=47961, avg=4368.65, stdev=2952.89
     lat (usec): min=407, max=48117, avg=4414.34, stdev=2964.51
    clat percentiles (usec):
     |  1.00th=[  881],  5.00th=[ 1123], 10.00th=[ 1385], 20.00th=[ 2073],
     | 30.00th=[ 2638], 40.00th=[ 3097], 50.00th=[ 3621], 60.00th=[ 4228],
     | 70.00th=[ 5014], 80.00th=[ 6194], 90.00th=[ 8291], 95.00th=[10290],
     | 99.00th=[14484], 99.50th=[16319], 99.90th=[20841], 99.95th=[23462],
     | 99.99th=[31327]
   bw (  KiB/s): min=168048, max=324112, per=100.00%, avg=232255.32, stdev=10218.53, samples=236
   iops        : min=42012, max=81028, avg=58063.80, stdev=2554.62, samples=236
  lat (usec)   : 500=0.01%, 750=0.18%, 1000=2.46%
  lat (msec)   : 2=16.25%, 4=37.36%, 10=38.27%, 20=5.34%, 50=0.14%
  cpu          : usr=8.42%, sys=19.87%, ctx=1077895, majf=0, minf=300
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=1738803,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=226MiB/s (237MB/s), 226MiB/s-226MiB/s (237MB/s-237MB/s), io=6792MiB (7122MB), run=30004-30004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=64; bs=4k) randread:  read: IOPS=58.0k, BW=226MiB/s (237MB/s)(6792MiB/30004msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52707: Sun Jan 14 02:59:57 2024
  read: IOPS=58.6k, BW=229MiB/s (240MB/s)(6862MiB/30005msec)
    slat (usec): min=4, max=16009, avg=64.57, stdev=141.83
    clat (usec): min=464, max=92111, avg=8676.32, stdev=6014.08
     lat (usec): min=503, max=92120, avg=8741.33, stdev=6024.19
    clat percentiles (usec):
     |  1.00th=[  988],  5.00th=[ 1582], 10.00th=[ 2737], 20.00th=[ 5080],
     | 30.00th=[ 5866], 40.00th=[ 6521], 50.00th=[ 7177], 60.00th=[ 7963],
     | 70.00th=[ 9372], 80.00th=[11994], 90.00th=[15270], 95.00th=[19530],
     | 99.00th=[31589], 99.50th=[37487], 99.90th=[53216], 99.95th=[60556],
     | 99.99th=[74974]
   bw (  KiB/s): min=125944, max=342203, per=99.95%, avg=234084.69, stdev=13297.94, samples=236
   iops        : min=31486, max=85550, avg=58520.49, stdev=3324.47, samples=236
  lat (usec)   : 500=0.01%, 750=0.08%, 1000=0.98%
  lat (msec)   : 2=6.19%, 4=5.75%, 10=59.50%, 20=22.83%, 50=4.54%
  lat (msec)   : 100=0.13%
  cpu          : usr=7.22%, sys=17.38%, ctx=813667, majf=0, minf=559
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1756795,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=229MiB/s (240MB/s), 229MiB/s-229MiB/s (240MB/s-240MB/s), io=6862MiB (7196MB), run=30005-30005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4k) randread:  read: IOPS=58.6k, BW=229MiB/s (240MB/s)(6862MiB/30005msec)


===Fio: workload=read, time=30, iodepth=1, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52757: Sun Jan 14 03:00:29 2024
  read: IOPS=3173, BW=12.4MiB/s (13.0MB/s)(372MiB/30001msec)
    slat (usec): min=5, max=475, avg=29.78, stdev=12.40
    clat (usec): min=366, max=10100, avg=1222.16, stdev=244.66
     lat (usec): min=396, max=10142, avg=1252.89, stdev=246.21
    clat percentiles (usec):
     |  1.00th=[  734],  5.00th=[  889], 10.00th=[  955], 20.00th=[ 1037],
     | 30.00th=[ 1090], 40.00th=[ 1156], 50.00th=[ 1205], 60.00th=[ 1270],
     | 70.00th=[ 1336], 80.00th=[ 1418], 90.00th=[ 1516], 95.00th=[ 1582],
     | 99.00th=[ 1729], 99.50th=[ 1811], 99.90th=[ 2008], 99.95th=[ 2089],
     | 99.99th=[ 9634]
   bw (  KiB/s): min=10304, max=15928, per=100.00%, avg=12709.83, stdev=358.86, samples=236
   iops        : min= 2576, max= 3982, avg=3177.46, stdev=89.72, samples=236
  lat (usec)   : 500=0.01%, 750=1.22%, 1000=13.78%
  lat (msec)   : 2=84.89%, 4=0.09%, 10=0.02%, 20=0.01%
  cpu          : usr=1.51%, sys=3.87%, ctx=95451, majf=0, minf=53
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=95210,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=12.4MiB/s (13.0MB/s), 12.4MiB/s-12.4MiB/s (13.0MB/s-13.0MB/s), io=372MiB (390MB), run=30001-30001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=1; bs=4k) read:  read: IOPS=3173, BW=12.4MiB/s (13.0MB/s)(372MiB/30001msec)


===Fio: workload=read, time=30, iodepth=32, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52807: Sun Jan 14 03:01:02 2024
  read: IOPS=14.5k, BW=56.7MiB/s (59.5MB/s)(1702MiB/30012msec)
    slat (usec): min=4, max=505, avg=15.41, stdev= 7.35
    clat (usec): min=434, max=24617, avg=8794.09, stdev=3086.00
     lat (usec): min=448, max=24637, avg=8810.06, stdev=3088.05
    clat percentiles (usec):
     |  1.00th=[ 1663],  5.00th=[ 2573], 10.00th=[ 2900], 20.00th=[ 7242],
     | 30.00th=[ 9241], 40.00th=[ 9503], 50.00th=[ 9634], 60.00th=[ 9896],
     | 70.00th=[10028], 80.00th=[10290], 90.00th=[10814], 95.00th=[11338],
     | 99.00th=[19530], 99.50th=[20317], 99.90th=[21365], 99.95th=[21890],
     | 99.99th=[23462]
   bw (  KiB/s): min=37648, max=119840, per=100.00%, avg=58197.42, stdev=3598.51, samples=236
   iops        : min= 9412, max=29960, avg=14549.36, stdev=899.63, samples=236
  lat (usec)   : 500=0.01%, 750=0.06%, 1000=0.17%
  lat (msec)   : 2=1.53%, 4=12.85%, 10=53.34%, 20=31.33%, 50=0.72%
  cpu          : usr=3.31%, sys=8.24%, ctx=400164, majf=0, minf=174
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=435817,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=56.7MiB/s (59.5MB/s), 56.7MiB/s-56.7MiB/s (59.5MB/s-59.5MB/s), io=1702MiB (1785MB), run=30012-30012msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=32; bs=4k) read:  read: IOPS=14.5k, BW=56.7MiB/s (59.5MB/s)(1702MiB/30012msec)


===Fio: workload=read, time=30, iodepth=64, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52856: Sun Jan 14 03:01:34 2024
  read: IOPS=14.4k, BW=56.3MiB/s (59.1MB/s)(1691MiB/30020msec)
    slat (usec): min=4, max=17218, avg=126.82, stdev=226.82
    clat (usec): min=436, max=76376, avg=17621.67, stdev=6732.69
     lat (usec): min=449, max=76390, avg=17749.16, stdev=6780.19
    clat percentiles (usec):
     |  1.00th=[ 2474],  5.00th=[ 5145], 10.00th=[ 5932], 20.00th=[13960],
     | 30.00th=[17957], 40.00th=[18744], 50.00th=[19268], 60.00th=[19530],
     | 70.00th=[20055], 80.00th=[20579], 90.00th=[21627], 95.00th=[23200],
     | 99.00th=[40633], 99.50th=[45351], 99.90th=[54264], 99.95th=[56361],
     | 99.99th=[74974]
   bw (  KiB/s): min=35104, max=85888, per=99.95%, avg=57646.21, stdev=2537.39, samples=238
   iops        : min= 8776, max=21472, avg=14411.32, stdev=634.34, samples=238
  lat (usec)   : 500=0.01%, 750=0.05%, 1000=0.10%
  lat (msec)   : 2=0.55%, 4=1.63%, 10=13.61%, 20=54.12%, 50=29.67%
  lat (msec)   : 100=0.27%
  cpu          : usr=3.14%, sys=8.02%, ctx=399973, majf=0, minf=305
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=432830,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=56.3MiB/s (59.1MB/s), 56.3MiB/s-56.3MiB/s (59.1MB/s-59.1MB/s), io=1691MiB (1773MB), run=30020-30020msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=64; bs=4k) read:  read: IOPS=14.4k, BW=56.3MiB/s (59.1MB/s)(1691MiB/30020msec)


===Fio: workload=read, time=30, iodepth=128, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=52905: Sun Jan 14 03:02:07 2024
  read: IOPS=15.0k, BW=58.4MiB/s (61.3MB/s)(1755MiB/30044msec)
    slat (usec): min=4, max=33781, avg=261.07, stdev=312.52
    clat (usec): min=429, max=166499, avg=33943.90, stdev=14302.33
     lat (usec): min=536, max=167534, avg=34205.79, stdev=14399.12
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[   10], 10.00th=[   12], 20.00th=[   23],
     | 30.00th=[   33], 40.00th=[   37], 50.00th=[   39], 60.00th=[   39],
     | 70.00th=[   40], 80.00th=[   41], 90.00th=[   43], 95.00th=[   46],
     | 99.00th=[   82], 99.50th=[   91], 99.90th=[  140], 99.95th=[  150],
     | 99.99th=[  155]
   bw (  KiB/s): min=32120, max=93688, per=100.00%, avg=59902.27, stdev=2913.33, samples=238
   iops        : min= 8030, max=23422, avg=14975.57, stdev=728.33, samples=238
  lat (usec)   : 500=0.01%, 750=0.04%, 1000=0.08%
  lat (msec)   : 2=0.43%, 4=0.84%, 10=4.65%, 20=12.29%, 50=78.20%
  lat (msec)   : 100=3.20%, 250=0.27%
  cpu          : usr=3.04%, sys=8.37%, ctx=423393, majf=0, minf=558
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=449361,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=58.4MiB/s (61.3MB/s), 58.4MiB/s-58.4MiB/s (61.3MB/s-61.3MB/s), io=1755MiB (1841MB), run=30044-30044msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4k) read:  read: IOPS=15.0k, BW=58.4MiB/s (61.3MB/s)(1755MiB/30044msec)
umount: /mnt/fsbench: not mounted.
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T02:45:13.rbd-multi.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T02:45:13.rbd-multi.rssd2.txt
Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=60.6k, BW=237MiB/s (248MB/s)(13.9GiB/60004msec)
Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=48.8k, BW=190MiB/s (200MB/s)(5715MiB/30002msec)
Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=59.3k, BW=231MiB/s (243MB/s)(6945MiB/30006msec)
Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=64.3k, BW=251MiB/s (264MB/s)(7541MiB/30002msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=62.3k, BW=243MiB/s (255MB/s)(7305MiB/30007msec)
Fio (disks=1, iodepth=128; bs=4ki) read:  read: IOPS=3623, BW=14.2MiB/s (14.8MB/s)(425MiB/30038msec)
Fio (disks=2, iodepth=128; bs=4ki) read:  read: IOPS=7533, BW=29.4MiB/s (30.9MB/s)(883MiB/30020msec)
Fio (disks=3, iodepth=128; bs=4ki) read:  read: IOPS=11.5k, BW=44.8MiB/s (46.9MB/s)(1345MiB/30040msec)
Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=14.8k, BW=57.8MiB/s (60.6MB/s)(1735MiB/30044msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=61.1k, BW=239MiB/s (250MB/s)(7163MiB/30003msec)
Fio (disks=4, iodepth=128; bs=8ki) randread:  read: IOPS=53.8k, BW=421MiB/s (441MB/s)(12.3GiB/30003msec)
Fio (disks=4, iodepth=128; bs=16ki) randread:  read: IOPS=47.2k, BW=738MiB/s (774MB/s)(21.6GiB/30003msec)
Fio (disks=4, iodepth=128; bs=32ki) randread:  read: IOPS=37.0k, BW=1157MiB/s (1213MB/s)(33.9GiB/30011msec)
Fio (disks=4, iodepth=128; bs=64ki) randread:  read: IOPS=19.1k, BW=1192MiB/s (1250MB/s)(34.9GiB/30018msec)
Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=14.1k, BW=54.9MiB/s (57.6MB/s)(1650MiB/30044msec)
Fio (disks=4, iodepth=128; bs=8ki) read:  read: IOPS=13.5k, BW=105MiB/s (110MB/s)(3161MiB/30044msec)
Fio (disks=4, iodepth=128; bs=16ki) read:  read: IOPS=15.9k, BW=249MiB/s (261MB/s)(7473MiB/30028msec)
Fio (disks=4, iodepth=128; bs=32ki) read:  read: IOPS=15.0k, BW=469MiB/s (492MB/s)(13.8GiB/30027msec)
Fio (disks=4, iodepth=128; bs=64ki) read:  read: IOPS=10.5k, BW=658MiB/s (690MB/s)(19.3GiB/30027msec)
Fio (disks=4, iodepth=1; bs=4k) randread:  read: IOPS=2907, BW=11.4MiB/s (11.9MB/s)(341MiB/30002msec)
Fio (disks=4, iodepth=32; bs=4k) randread:  read: IOPS=56.9k, BW=222MiB/s (233MB/s)(6668MiB/30003msec)
Fio (disks=4, iodepth=64; bs=4k) randread:  read: IOPS=58.0k, BW=226MiB/s (237MB/s)(6792MiB/30004msec)
Fio (disks=4, iodepth=128; bs=4k) randread:  read: IOPS=58.6k, BW=229MiB/s (240MB/s)(6862MiB/30005msec)
Fio (disks=4, iodepth=1; bs=4k) read:  read: IOPS=3173, BW=12.4MiB/s (13.0MB/s)(372MiB/30001msec)
Fio (disks=4, iodepth=32; bs=4k) read:  read: IOPS=14.5k, BW=56.7MiB/s (59.5MB/s)(1702MiB/30012msec)
Fio (disks=4, iodepth=64; bs=4k) read:  read: IOPS=14.4k, BW=56.3MiB/s (59.1MB/s)(1691MiB/30020msec)
Fio (disks=4, iodepth=128; bs=4k) read:  read: IOPS=15.0k, BW=58.4MiB/s (61.3MB/s)(1755MiB/30044msec)
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
++ date +%FT%T
+ cur_time=2024-01-14T03:02:10
+ default_cache_size=128849018880
+ cache_size=128849018880
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=120
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:02:10.rbd-multi.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=rbd-benchmark
+ imgsize=10G
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ rbd -p triple-hdd rm rbd-benchmark.multi.1
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd rm rbd-benchmark.multi.2
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd rm rbd-benchmark.multi.3
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd rm rbd-benchmark.multi.4
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.1
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.2
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.3
+ wait
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.4
Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 5% complete...Thick provisioning: 4% complete...Thick provisioning: 6% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 4% complete...Thick provisioning: 7% complete...Thick provisioning: 6% complete...Thick provisioning: 5% complete...Thick provisioning: 8% complete...Thick provisioning: 5% complete...Thick provisioning: 7% complete...Thick provisioning: 6% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 6% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 13% complete...Thick provisioning: 12% complete...Thick provisioning: 10% complete...Thick provisioning: 14% complete...Thick provisioning: 13% complete...Thick provisioning: 10% complete...Thick provisioning: 15% complete...Thick provisioning: 14% complete...Thick provisioning: 11% complete...Thick provisioning: 16% complete...Thick provisioning: 11% complete...Thick provisioning: 15% complete...Thick provisioning: 17% complete...Thick provisioning: 12% complete...Thick provisioning: 12% complete...Thick provisioning: 16% complete...Thick provisioning: 13% complete...Thick provisioning: 18% complete...Thick provisioning: 13% complete...Thick provisioning: 17% complete...Thick provisioning: 19% complete...Thick provisioning: 14% complete...Thick provisioning: 20% complete...Thick provisioning: 18% complete...Thick provisioning: 14% complete...Thick provisioning: 19% complete...Thick provisioning: 21% complete...Thick provisioning: 15% complete...Thick provisioning: 15% complete...Thick provisioning: 20% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 21% complete...Thick provisioning: 16% complete...Thick provisioning: 24% complete...Thick provisioning: 16% complete...Thick provisioning: 22% complete...Thick provisioning: 17% complete...Thick provisioning: 25% complete...Thick provisioning: 23% complete...Thick provisioning: 26% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 27% complete...Thick provisioning: 18% complete...Thick provisioning: 24% complete...Thick provisioning: 28% complete...Thick provisioning: 19% complete...Thick provisioning: 29% complete...Thick provisioning: 25% complete...Thick provisioning: 19% complete...Thick provisioning: 26% complete...Thick provisioning: 20% complete...Thick provisioning: 30% complete...Thick provisioning: 20% complete...Thick provisioning: 31% complete...Thick provisioning: 27% complete...Thick provisioning: 21% complete...Thick provisioning: 28% complete...Thick provisioning: 32% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 33% complete...Thick provisioning: 29% complete...Thick provisioning: 22% complete...Thick provisioning: 34% complete...Thick provisioning: 23% complete...Thick provisioning: 30% complete...Thick provisioning: 35% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 31% complete...Thick provisioning: 36% complete...Thick provisioning: 32% complete...Thick provisioning: 37% complete...Thick provisioning: 25% complete...Thick provisioning: 24% complete...Thick provisioning: 38% complete...Thick provisioning: 33% complete...Thick provisioning: 39% complete...Thick provisioning: 26% complete...Thick provisioning: 25% complete...Thick provisioning: 34% complete...Thick provisioning: 40% complete...Thick provisioning: 35% complete...Thick provisioning: 41% complete...Thick provisioning: 27% complete...Thick provisioning: 26% complete...Thick provisioning: 36% complete...Thick provisioning: 42% complete...Thick provisioning: 28% complete...Thick provisioning: 27% complete...Thick provisioning: 43% complete...Thick provisioning: 37% complete...Thick provisioning: 29% complete...Thick provisioning: 44% complete...Thick provisioning: 28% complete...Thick provisioning: 38% complete...Thick provisioning: 45% complete...Thick provisioning: 30% complete...Thick provisioning: 39% complete...Thick provisioning: 46% complete...Thick provisioning: 29% complete...Thick provisioning: 47% complete...Thick provisioning: 31% complete...Thick provisioning: 40% complete...Thick provisioning: 30% complete...Thick provisioning: 48% complete...Thick provisioning: 41% complete...Thick provisioning: 49% complete...Thick provisioning: 32% complete...Thick provisioning: 50% complete...Thick provisioning: 42% complete...Thick provisioning: 31% complete...Thick provisioning: 33% complete...Thick provisioning: 51% complete...Thick provisioning: 43% complete...Thick provisioning: 32% complete...Thick provisioning: 52% complete...Thick provisioning: 44% complete...Thick provisioning: 34% complete...Thick provisioning: 33% complete...Thick provisioning: 53% complete...Thick provisioning: 45% complete...Thick provisioning: 35% complete...Thick provisioning: 54% complete...Thick provisioning: 46% complete...Thick provisioning: 55% complete...Thick provisioning: 34% complete...Thick provisioning: 56% complete...Thick provisioning: 36% complete...Thick provisioning: 47% complete...Thick provisioning: 35% complete...Thick provisioning: 57% complete...Thick provisioning: 48% complete...Thick provisioning: 37% complete...Thick provisioning: 58% complete...Thick provisioning: 36% complete...Thick provisioning: 59% complete...Thick provisioning: 38% complete...Thick provisioning: 49% complete...Thick provisioning: 37% complete...Thick provisioning: 60% complete...Thick provisioning: 50% complete...Thick provisioning: 39% complete...Thick provisioning: 61% complete...Thick provisioning: 51% complete...Thick provisioning: 38% complete...Thick provisioning: 62% complete...Thick provisioning: 52% complete...Thick provisioning: 40% complete...Thick provisioning: 63% complete...Thick provisioning: 39% complete...Thick provisioning: 41% complete...Thick provisioning: 53% complete...Thick provisioning: 40% complete...Thick provisioning: 64% complete...Thick provisioning: 54% complete...Thick provisioning: 42% complete...Thick provisioning: 65% complete...Thick provisioning: 55% complete...Thick provisioning: 41% complete...Thick provisioning: 43% complete...Thick provisioning: 56% complete...Thick provisioning: 66% complete...Thick provisioning: 42% complete...Thick provisioning: 57% complete...Thick provisioning: 44% complete...Thick provisioning: 58% complete...Thick provisioning: 43% complete...Thick provisioning: 67% complete...Thick provisioning: 45% complete...Thick provisioning: 59% complete...Thick provisioning: 68% complete...Thick provisioning: 44% complete...Thick provisioning: 69% complete...Thick provisioning: 46% complete...Thick provisioning: 60% complete...Thick provisioning: 70% complete...Thick provisioning: 45% complete...Thick provisioning: 71% complete...Thick provisioning: 47% complete...Thick provisioning: 61% complete...Thick provisioning: 46% complete...Thick provisioning: 62% complete...Thick provisioning: 72% complete...Thick provisioning: 48% complete...Thick provisioning: 63% complete...Thick provisioning: 73% complete...Thick provisioning: 47% complete...Thick provisioning: 49% complete...Thick provisioning: 74% complete...Thick provisioning: 64% complete...Thick provisioning: 48% complete...Thick provisioning: 75% complete...Thick provisioning: 50% complete...Thick provisioning: 76% complete...Thick provisioning: 65% complete...Thick provisioning: 49% complete...Thick provisioning: 77% complete...Thick provisioning: 51% complete...Thick provisioning: 66% complete...Thick provisioning: 78% complete...Thick provisioning: 50% complete...Thick provisioning: 67% complete...Thick provisioning: 52% complete...Thick provisioning: 79% complete...Thick provisioning: 51% complete...Thick provisioning: 80% complete...Thick provisioning: 68% complete...Thick provisioning: 53% complete...Thick provisioning: 81% complete...Thick provisioning: 69% complete...Thick provisioning: 52% complete...Thick provisioning: 82% complete...Thick provisioning: 54% complete...Thick provisioning: 70% complete...Thick provisioning: 83% complete...Thick provisioning: 53% complete...Thick provisioning: 55% complete...Thick provisioning: 84% complete...Thick provisioning: 71% complete...Thick provisioning: 54% complete...Thick provisioning: 85% complete...Thick provisioning: 56% complete...Thick provisioning: 72% complete...Thick provisioning: 86% complete...Thick provisioning: 55% complete...Thick provisioning: 57% complete...Thick provisioning: 73% complete...Thick provisioning: 87% complete...Thick provisioning: 74% complete...Thick provisioning: 58% complete...Thick provisioning: 56% complete...Thick provisioning: 88% complete...Thick provisioning: 75% complete...Thick provisioning: 89% complete...Thick provisioning: 59% complete...Thick provisioning: 57% complete...Thick provisioning: 76% complete...Thick provisioning: 90% complete...Thick provisioning: 60% complete...Thick provisioning: 58% complete...Thick provisioning: 91% complete...Thick provisioning: 77% complete...Thick provisioning: 61% complete...Thick provisioning: 92% complete...Thick provisioning: 59% complete...Thick provisioning: 78% complete...Thick provisioning: 62% complete...Thick provisioning: 79% complete...Thick provisioning: 60% complete...Thick provisioning: 93% complete...Thick provisioning: 63% complete...Thick provisioning: 80% complete...Thick provisioning: 61% complete...Thick provisioning: 94% complete...Thick provisioning: 81% complete...Thick provisioning: 64% complete...Thick provisioning: 95% complete...Thick provisioning: 62% complete...Thick provisioning: 82% complete...Thick provisioning: 96% complete...Thick provisioning: 65% complete...Thick provisioning: 83% complete...Thick provisioning: 97% complete...Thick provisioning: 63% complete...Thick provisioning: 66% complete...Thick provisioning: 84% complete...Thick provisioning: 98% complete...Thick provisioning: 64% complete...Thick provisioning: 85% complete...Thick provisioning: 99% complete...Thick provisioning: 67% complete...Thick provisioning: 100% complete...Thick provisioning: 65% complete...Thick provisioning: 86% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 68% complete...Thick provisioning: 87% complete...Thick provisioning: 66% complete...Thick provisioning: 69% complete...Thick provisioning: 88% complete...Thick provisioning: 67% complete...Thick provisioning: 70% complete...Thick provisioning: 89% complete...Thick provisioning: 68% complete...Thick provisioning: 90% complete...Thick provisioning: 71% complete...Thick provisioning: 91% complete...Thick provisioning: 69% complete...Thick provisioning: 72% complete...Thick provisioning: 92% complete...Thick provisioning: 70% complete...Thick provisioning: 73% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 74% complete...Thick provisioning: 71% complete...Thick provisioning: 95% complete...Thick provisioning: 75% complete...Thick provisioning: 72% complete...Thick provisioning: 96% complete...Thick provisioning: 76% complete...Thick provisioning: 73% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 74% complete...Thick provisioning: 77% complete...Thick provisioning: 99% complete...Thick provisioning: 75% complete...Thick provisioning: 78% complete...Thick provisioning: 100% complete...Thick provisioning: 76% complete...Thick provisioning: 79% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 77% complete...Thick provisioning: 80% complete...Thick provisioning: 78% complete...Thick provisioning: 81% complete...Thick provisioning: 79% complete...Thick provisioning: 82% complete...Thick provisioning: 80% complete...Thick provisioning: 83% complete...Thick provisioning: 81% complete...Thick provisioning: 84% complete...Thick provisioning: 82% complete...Thick provisioning: 85% complete...Thick provisioning: 83% complete...Thick provisioning: 86% complete...Thick provisioning: 84% complete...Thick provisioning: 87% complete...Thick provisioning: 85% complete...Thick provisioning: 88% complete...Thick provisioning: 86% complete...Thick provisioning: 89% complete...Thick provisioning: 87% complete...Thick provisioning: 90% complete...Thick provisioning: 88% complete...Thick provisioning: 91% complete...Thick provisioning: 89% complete...Thick provisioning: 92% complete...Thick provisioning: 90% complete...Thick provisioning: 93% complete...Thick provisioning: 91% complete...Thick provisioning: 94% complete...Thick provisioning: 92% complete...Thick provisioning: 95% complete...Thick provisioning: 93% complete...Thick provisioning: 96% complete...Thick provisioning: 94% complete...Thick provisioning: 97% complete...Thick provisioning: 95% complete...Thick provisioning: 98% complete...Thick provisioning: 96% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 97% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done.
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ fstrim /mnt/nvme
+ launch_gw_background
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ sleep 5
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-14 03:05:24.479391] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-14 03:05:24.479498] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid460845 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-14 03:05:24.550872] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-14 03:05:24.689423] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-14 03:05:24.689530] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-14 03:05:24.689637] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-14 03:05:24.689642] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-14 03:05:30.118176] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-14 03:05:31.021223] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd rbd-benchmark.multi.1
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.1
+ local bdev=bdev_rbd-benchmark.multi.1
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.1 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.1
[2024-01-14 03:05:31.512512] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.1 rbd disk to lun
bdev_rbd-benchmark.multi.1
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.1
+ add_rbd_img triple-hdd rbd-benchmark.multi.2
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.2
+ local bdev=bdev_rbd-benchmark.multi.2
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.2 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.2
[2024-01-14 03:05:32.475535] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.2 rbd disk to lun
bdev_rbd-benchmark.multi.2
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.2
+ add_rbd_img triple-hdd rbd-benchmark.multi.3
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.3
+ local bdev=bdev_rbd-benchmark.multi.3
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.3 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.3
[2024-01-14 03:05:33.174489] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.3 rbd disk to lun
bdev_rbd-benchmark.multi.3
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.3
+ add_rbd_img triple-hdd rbd-benchmark.multi.4
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.4
+ local bdev=bdev_rbd-benchmark.multi.4
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.4 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.4
[2024-01-14 03:05:34.136589] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.4 rbd disk to lun
bdev_rbd-benchmark.multi.4
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.4
+ trap 'cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:02:10.rbd-multi.triple-hdd.txt multi-client/client-bench-multi.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:02:10.rbd-multi.triple-hdd.txt
+ local benchscript=multi-client/client-bench-multi.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:02:10.rbd-multi.triple-hdd.txt
===Starting client benchmark

NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
device: nvme1
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n2          SPDK00000000000001   SPDK_Controller1                         2          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n3          SPDK00000000000001   SPDK_Controller1                         3          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n4          SPDK00000000000001   SPDK_Controller1                         4          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
Using device /dev/nvme1n1
/dev/nvme1n2
/dev/nvme1n3
/dev/nvme1n4


===Fio: workload=randread, time=60, iodepth=256, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53168: Sun Jan 14 03:06:42 2024
  read: IOPS=5631, BW=22.0MiB/s (23.1MB/s)(1326MiB/60283msec)
    slat (usec): min=4, max=197936, avg=700.13, stdev=968.50
    clat (msec): min=3, max=1766, avg=180.91, stdev=100.07
     lat (msec): min=3, max=1769, avg=181.62, stdev=100.16
    clat percentiles (msec):
     |  1.00th=[   45],  5.00th=[   84], 10.00th=[  112], 20.00th=[  132],
     | 30.00th=[  144], 40.00th=[  157], 50.00th=[  167], 60.00th=[  178],
     | 70.00th=[  190], 80.00th=[  207], 90.00th=[  243], 95.00th=[  300],
     | 99.00th=[  667], 99.50th=[  768], 99.90th=[ 1200], 99.95th=[ 1318],
     | 99.99th=[ 1485]
   bw (  KiB/s): min= 7864, max=37336, per=100.00%, avg=22568.60, stdev=1449.85, samples=480
   iops        : min= 1966, max= 9334, avg=5641.93, stdev=362.46, samples=480
  lat (msec)   : 4=0.01%, 10=0.01%, 20=0.01%, 50=1.94%, 100=5.59%
  lat (msec)   : 250=83.51%, 500=7.01%, 750=1.36%, 1000=0.37%, 2000=0.20%
  cpu          : usr=1.62%, sys=4.44%, ctx=285019, majf=0, minf=1468
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=339457,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=22.0MiB/s (23.1MB/s), 22.0MiB/s-22.0MiB/s (23.1MB/s-23.1MB/s), io=1326MiB (1390MB), run=60283-60283msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=5631, BW=22.0MiB/s (23.1MB/s)(1326MiB/60283msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process

j1: (groupid=0, jobs=1): err= 0: pid=53221: Sun Jan 14 03:07:14 2024
  read: IOPS=6535, BW=25.5MiB/s (26.8MB/s)(771MiB/30214msec)
    slat (usec): min=5, max=6088, avg=140.56, stdev=200.07
    clat (usec): min=372, max=502243, avg=19436.30, stdev=44888.64
     lat (usec): min=407, max=502263, avg=19577.63, stdev=44893.29
    clat percentiles (usec):
     |  1.00th=[   529],  5.00th=[   619], 10.00th=[   750], 20.00th=[  2180],
     | 30.00th=[  4424], 40.00th=[  6325], 50.00th=[  8094], 60.00th=[  9765],
     | 70.00th=[ 12780], 80.00th=[ 18482], 90.00th=[ 35390], 95.00th=[ 71828],
     | 99.00th=[265290], 99.50th=[333448], 99.90th=[413139], 99.95th=[434111],
     | 99.99th=[476054]
   bw (  KiB/s): min=19480, max=74528, per=100.00%, avg=26312.00, stdev=9439.40, samples=60
   iops        : min= 4870, max=18630, avg=6577.97, stdev=2359.68, samples=60
  lat (usec)   : 500=0.40%, 750=9.64%, 1000=5.49%
  lat (msec)   : 2=4.04%, 4=8.43%, 10=33.61%, 20=19.92%, 50=11.39%
  lat (msec)   : 100=3.35%, 250=2.56%, 500=1.16%, 750=0.01%
  cpu          : usr=5.89%, sys=13.72%, ctx=147870, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=197467,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=25.5MiB/s (26.8MB/s), 25.5MiB/s-25.5MiB/s (26.8MB/s-26.8MB/s), io=771MiB (809MB), run=30214-30214msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=6535, BW=25.5MiB/s (26.8MB/s)(771MiB/30214msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes

j1: (groupid=0, jobs=2): err= 0: pid=53268: Sun Jan 14 03:07:47 2024
  read: IOPS=6452, BW=25.2MiB/s (26.4MB/s)(768MiB/30471msec)
    slat (usec): min=4, max=119691, avg=294.49, stdev=466.51
    clat (usec): min=435, max=1187.2k, avg=39354.38, stdev=72866.65
     lat (usec): min=482, max=1187.4k, avg=39649.61, stdev=72877.80
    clat percentiles (usec):
     |  1.00th=[    619],  5.00th=[   3359], 10.00th=[   8848],
     | 20.00th=[  16319], 30.00th=[  21890], 40.00th=[  25297],
     | 50.00th=[  27919], 60.00th=[  30802], 70.00th=[  34341],
     | 80.00th=[  39060], 90.00th=[  53216], 95.00th=[  85459],
     | 99.00th=[ 438305], 99.50th=[ 591397], 99.90th=[ 994051],
     | 99.95th=[1082131], 99.99th=[1132463]
   bw (  KiB/s): min=15816, max=39896, per=100.00%, avg=26179.87, stdev=3124.95, samples=120
   iops        : min= 3954, max= 9974, avg=6544.97, stdev=781.24, samples=120
  lat (usec)   : 500=0.02%, 750=2.32%, 1000=1.09%
  lat (msec)   : 2=0.59%, 4=1.46%, 10=6.04%, 20=14.47%, 50=62.72%
  lat (msec)   : 100=7.12%, 250=2.45%, 500=0.95%, 750=0.58%, 1000=0.10%
  lat (msec)   : 2000=0.09%
  cpu          : usr=2.84%, sys=6.94%, ctx=165619, majf=0, minf=278
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=196603,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=25.2MiB/s (26.4MB/s), 25.2MiB/s-25.2MiB/s (26.4MB/s-26.4MB/s), io=768MiB (805MB), run=30471-30471msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=6452, BW=25.2MiB/s (26.4MB/s)(768MiB/30471msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes

j1: (groupid=0, jobs=3): err= 0: pid=53316: Sun Jan 14 03:08:20 2024
  read: IOPS=8988, BW=35.1MiB/s (36.8MB/s)(1076MiB/30642msec)
    slat (usec): min=4, max=15925, avg=311.33, stdev=430.72
    clat (usec): min=375, max=1742.8k, avg=42389.75, stdev=92162.61
     lat (usec): min=424, max=1742.8k, avg=42701.93, stdev=92192.24
    clat percentiles (usec):
     |  1.00th=[    537],  5.00th=[    627], 10.00th=[    725],
     | 20.00th=[   3556], 30.00th=[   7898], 40.00th=[  13566],
     | 50.00th=[  24249], 60.00th=[  31589], 70.00th=[  38011],
     | 80.00th=[  46924], 90.00th=[  69731], 95.00th=[ 137364],
     | 99.00th=[ 513803], 99.50th=[ 650118], 99.90th=[1027605],
     | 99.95th=[1082131], 99.99th=[1518339]
   bw (  KiB/s): min=21160, max=60496, per=100.00%, avg=36671.47, stdev=3627.10, samples=180
   iops        : min= 5290, max=15124, avg=9167.87, stdev=906.77, samples=180
  lat (usec)   : 500=0.24%, 750=10.76%, 1000=4.37%
  lat (msec)   : 2=1.59%, 4=4.02%, 10=14.17%, 20=10.95%, 50=36.30%
  lat (msec)   : 100=10.86%, 250=3.92%, 500=1.77%, 750=0.71%, 1000=0.21%
  lat (msec)   : 2000=0.14%
  cpu          : usr=2.85%, sys=7.04%, ctx=226809, majf=0, minf=420
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=275417,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=35.1MiB/s (36.8MB/s), 35.1MiB/s-35.1MiB/s (36.8MB/s-36.8MB/s), io=1076MiB (1128MB), run=30642-30642msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=8988, BW=35.1MiB/s (36.8MB/s)(1076MiB/30642msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53367: Sun Jan 14 03:08:53 2024
  read: IOPS=7335, BW=28.7MiB/s (30.0MB/s)(872MiB/30433msec)
    slat (usec): min=4, max=20387, avg=528.88, stdev=656.89
    clat (usec): min=484, max=1018.1k, avg=69185.70, stdev=66706.61
     lat (usec): min=497, max=1018.2k, avg=69715.56, stdev=66768.73
    clat percentiles (msec):
     |  1.00th=[   15],  5.00th=[   25], 10.00th=[   29], 20.00th=[   33],
     | 30.00th=[   39], 40.00th=[   44], 50.00th=[   51], 60.00th=[   59],
     | 70.00th=[   68], 80.00th=[   84], 90.00th=[  117], 95.00th=[  190],
     | 99.00th=[  393], 99.50th=[  460], 99.90th=[  567], 99.95th=[  617],
     | 99.99th=[  885]
   bw (  KiB/s): min=19864, max=41312, per=100.00%, avg=29696.80, stdev=1113.65, samples=240
   iops        : min= 4966, max=10328, avg=7424.20, stdev=278.41, samples=240
  lat (usec)   : 500=0.01%, 750=0.09%, 1000=0.04%
  lat (msec)   : 2=0.03%, 4=0.08%, 10=0.39%, 20=1.19%, 50=47.04%
  lat (msec)   : 100=37.13%, 250=10.73%, 500=3.02%, 750=0.24%, 1000=0.02%
  lat (msec)   : 2000=0.01%
  cpu          : usr=2.05%, sys=4.96%, ctx=190197, majf=0, minf=555
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=223234,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=28.7MiB/s (30.0MB/s), 28.7MiB/s-28.7MiB/s (30.0MB/s-30.0MB/s), io=872MiB (914MB), run=30433-30433msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=7335, BW=28.7MiB/s (30.0MB/s)(872MiB/30433msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process

j1: (groupid=0, jobs=1): err= 0: pid=53416: Sun Jan 14 03:09:25 2024
  read: IOPS=3598, BW=14.1MiB/s (14.7MB/s)(422MiB/30038msec)
    slat (usec): min=4, max=145491, avg=270.92, stdev=506.72
    clat (usec): min=941, max=183629, avg=35292.18, stdev=9094.17
     lat (usec): min=1097, max=184147, avg=35563.84, stdev=9136.22
    clat percentiles (msec):
     |  1.00th=[   10],  5.00th=[   17], 10.00th=[   23], 20.00th=[   36],
     | 30.00th=[   37], 40.00th=[   38], 50.00th=[   38], 60.00th=[   39],
     | 70.00th=[   39], 80.00th=[   39], 90.00th=[   40], 95.00th=[   41],
     | 99.00th=[   52], 99.50th=[   60], 99.90th=[  184], 99.95th=[  184],
     | 99.99th=[  184]
   bw (  KiB/s): min=10768, max=15776, per=100.00%, avg=14396.00, stdev=691.00, samples=60
   iops        : min= 2692, max= 3944, avg=3599.00, stdev=172.75, samples=60
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.09%, 4=0.15%, 10=1.00%, 20=6.57%, 50=91.18%
  lat (msec)   : 100=0.90%, 250=0.12%
  cpu          : usr=2.87%, sys=10.20%, ctx=93863, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=108097,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=14.1MiB/s (14.7MB/s), 14.1MiB/s-14.1MiB/s (14.7MB/s-14.7MB/s), io=422MiB (443MB), run=30038-30038msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) read:  read: IOPS=3598, BW=14.1MiB/s (14.7MB/s)(422MiB/30038msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes

j1: (groupid=0, jobs=2): err= 0: pid=53462: Sun Jan 14 03:09:58 2024
  read: IOPS=7205, BW=28.1MiB/s (29.5MB/s)(845MiB/30038msec)
    slat (usec): min=3, max=251847, avg=270.31, stdev=1018.65
    clat (usec): min=845, max=291053, avg=35238.07, stdev=16116.99
     lat (usec): min=930, max=291361, avg=35509.29, stdev=16207.04
    clat percentiles (msec):
     |  1.00th=[    7],  5.00th=[    8], 10.00th=[   16], 20.00th=[   33],
     | 30.00th=[   36], 40.00th=[   37], 50.00th=[   38], 60.00th=[   38],
     | 70.00th=[   39], 80.00th=[   39], 90.00th=[   40], 95.00th=[   44],
     | 99.00th=[  105], 99.50th=[  122], 99.90th=[  220], 99.95th=[  292],
     | 99.99th=[  292]
   bw (  KiB/s): min=10800, max=43544, per=100.00%, avg=28824.93, stdev=2900.14, samples=120
   iops        : min= 2700, max=10886, avg=7206.23, stdev=725.04, samples=120
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.03%, 4=0.06%, 10=7.70%, 20=4.17%, 50=84.54%
  lat (msec)   : 100=2.42%, 250=1.02%, 500=0.06%
  cpu          : usr=3.39%, sys=8.98%, ctx=197261, majf=0, minf=279
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=216441,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=28.1MiB/s (29.5MB/s), 28.1MiB/s-28.1MiB/s (29.5MB/s-29.5MB/s), io=845MiB (887MB), run=30038-30038msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) read:  read: IOPS=7205, BW=28.1MiB/s (29.5MB/s)(845MiB/30038msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes

j1: (groupid=0, jobs=3): err= 0: pid=53509: Sun Jan 14 03:10:30 2024
  read: IOPS=10.8k, BW=42.1MiB/s (44.1MB/s)(1263MiB/30039msec)
    slat (usec): min=4, max=249802, avg=271.43, stdev=801.79
    clat (usec): min=454, max=288424, avg=35374.21, stdev=16566.60
     lat (usec): min=539, max=288587, avg=35646.53, stdev=16666.66
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[    8], 10.00th=[   17], 20.00th=[   32],
     | 30.00th=[   36], 40.00th=[   37], 50.00th=[   37], 60.00th=[   38],
     | 70.00th=[   39], 80.00th=[   39], 90.00th=[   41], 95.00th=[   48],
     | 99.00th=[   93], 99.50th=[  134], 99.90th=[  199], 99.95th=[  247],
     | 99.99th=[  288]
   bw (  KiB/s): min=19384, max=61952, per=100.00%, avg=43103.76, stdev=2807.18, samples=178
   iops        : min= 4846, max=15488, avg=10775.94, stdev=701.79, samples=178
  lat (usec)   : 500=0.01%, 750=0.06%, 1000=0.09%
  lat (msec)   : 2=0.45%, 4=0.65%, 10=5.83%, 20=4.92%, 50=83.35%
  lat (msec)   : 100=3.90%, 250=0.69%, 500=0.04%
  cpu          : usr=3.04%, sys=8.61%, ctx=303499, majf=0, minf=421
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=323399,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=42.1MiB/s (44.1MB/s), 42.1MiB/s-42.1MiB/s (44.1MB/s-44.1MB/s), io=1263MiB (1325MB), run=30039-30039msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) read:  read: IOPS=10.8k, BW=42.1MiB/s (44.1MB/s)(1263MiB/30039msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53561: Sun Jan 14 03:11:03 2024
  read: IOPS=13.7k, BW=53.6MiB/s (56.2MB/s)(1609MiB/30038msec)
    slat (usec): min=3, max=321483, avg=285.32, stdev=1381.76
    clat (usec): min=405, max=362626, avg=37025.37, stdev=21975.85
     lat (usec): min=470, max=363035, avg=37311.52, stdev=22090.58
    clat percentiles (msec):
     |  1.00th=[    3],  5.00th=[   10], 10.00th=[   22], 20.00th=[   33],
     | 30.00th=[   36], 40.00th=[   37], 50.00th=[   37], 60.00th=[   38],
     | 70.00th=[   39], 80.00th=[   39], 90.00th=[   41], 95.00th=[   53],
     | 99.00th=[  142], 99.50th=[  178], 99.90th=[  305], 99.95th=[  313],
     | 99.99th=[  363]
   bw (  KiB/s): min=21768, max=82960, per=100.00%, avg=55011.16, stdev=2917.35, samples=238
   iops        : min= 5442, max=20740, avg=13752.79, stdev=729.34, samples=238
  lat (usec)   : 500=0.01%, 750=0.09%, 1000=0.13%
  lat (msec)   : 2=0.67%, 4=0.97%, 10=3.27%, 20=4.14%, 50=85.23%
  lat (msec)   : 100=3.95%, 250=1.31%, 500=0.24%
  cpu          : usr=2.80%, sys=7.80%, ctx=385701, majf=0, minf=564
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=411923,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=53.6MiB/s (56.2MB/s), 53.6MiB/s-53.6MiB/s (56.2MB/s-56.2MB/s), io=1609MiB (1687MB), run=30038-30038msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=13.7k, BW=53.6MiB/s (56.2MB/s)(1609MiB/30038msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53611: Sun Jan 14 03:11:36 2024
  read: IOPS=7997, BW=31.2MiB/s (32.8MB/s)(958MiB/30676msec)
    slat (usec): min=4, max=98044, avg=474.21, stdev=732.68
    clat (usec): min=427, max=1718.6k, avg=63472.45, stdev=107634.74
     lat (usec): min=461, max=1718.6k, avg=63947.54, stdev=107697.31
    clat percentiles (usec):
     |  1.00th=[    619],  5.00th=[    930], 10.00th=[   5080],
     | 20.00th=[  12125], 30.00th=[  21365], 40.00th=[  31327],
     | 50.00th=[  40633], 60.00th=[  50594], 70.00th=[  62653],
     | 80.00th=[  77071], 90.00th=[ 101188], 95.00th=[ 181404],
     | 99.00th=[ 692061], 99.50th=[ 784335], 99.90th=[ 935330],
     | 99.95th=[1002439], 99.99th=[1501561]
   bw (  KiB/s): min=14992, max=58576, per=100.00%, avg=32641.60, stdev=3434.39, samples=240
   iops        : min= 3748, max=14644, avg=8160.40, stdev=858.60, samples=240
  lat (usec)   : 500=0.02%, 750=3.14%, 1000=2.19%
  lat (msec)   : 2=0.85%, 4=2.33%, 10=9.16%, 20=10.71%, 50=30.96%
  lat (msec)   : 100=30.37%, 250=6.38%, 500=2.16%, 750=1.03%, 1000=0.64%
  lat (msec)   : 2000=0.05%
  cpu          : usr=1.99%, sys=5.09%, ctx=204766, majf=0, minf=558
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=245320,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=31.2MiB/s (32.8MB/s), 31.2MiB/s-31.2MiB/s (32.8MB/s-32.8MB/s), io=958MiB (1005MB), run=30676-30676msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=7997, BW=31.2MiB/s (32.8MB/s)(958MiB/30676msec)


===Fio: workload=randread, time=30, iodepth=128, bs=8ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53660: Sun Jan 14 03:12:09 2024
  read: IOPS=5674, BW=44.3MiB/s (46.5MB/s)(1363MiB/30742msec)
    slat (usec): min=4, max=478766, avg=658.55, stdev=1506.92
    clat (usec): min=532, max=2049.3k, avg=89408.53, stdev=189086.75
     lat (usec): min=604, max=2050.1k, avg=90068.08, stdev=189179.72
    clat percentiles (usec):
     |  1.00th=[   1663],  5.00th=[   4359], 10.00th=[   6521],
     | 20.00th=[  10421], 30.00th=[  21627], 40.00th=[  42730],
     | 50.00th=[  52167], 60.00th=[  62653], 70.00th=[  78119],
     | 80.00th=[ 101188], 90.00th=[ 143655], 95.00th=[ 214959],
     | 99.00th=[1283458], 99.50th=[1468007], 99.90th=[1635779],
     | 99.95th=[1702888], 99.99th=[1853883]
   bw (  KiB/s): min=15120, max=79584, per=100.00%, avg=46381.95, stdev=4953.80, samples=240
   iops        : min= 1890, max= 9948, avg=5797.72, stdev=619.22, samples=240
  lat (usec)   : 750=0.04%, 1000=0.20%
  lat (msec)   : 2=0.84%, 4=3.21%, 10=14.70%, 20=10.17%, 50=18.32%
  lat (msec)   : 100=31.98%, 250=16.39%, 500=1.68%, 750=0.32%, 1000=0.47%
  lat (msec)   : 2000=1.68%, >=2000=0.01%
  cpu          : usr=1.59%, sys=4.28%, ctx=148717, majf=0, minf=1073
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=174448,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=44.3MiB/s (46.5MB/s), 44.3MiB/s-44.3MiB/s (46.5MB/s-46.5MB/s), io=1363MiB (1429MB), run=30742-30742msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=8ki) randread:  read: IOPS=5674, BW=44.3MiB/s (46.5MB/s)(1363MiB/30742msec)


===Fio: workload=randread, time=30, iodepth=128, bs=16ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53710: Sun Jan 14 03:12:42 2024
  read: IOPS=5400, BW=84.4MiB/s (88.5MB/s)(2568MiB/30437msec)
    slat (usec): min=4, max=70656, avg=722.22, stdev=864.60
    clat (usec): min=894, max=1207.3k, avg=93927.05, stdev=118863.67
     lat (usec): min=905, max=1207.4k, avg=94650.33, stdev=118931.21
    clat percentiles (msec):
     |  1.00th=[   27],  5.00th=[   34], 10.00th=[   37], 20.00th=[   44],
     | 30.00th=[   52], 40.00th=[   61], 50.00th=[   71], 60.00th=[   82],
     | 70.00th=[   94], 80.00th=[  109], 90.00th=[  130], 95.00th=[  159],
     | 99.00th=[  835], 99.50th=[  911], 99.90th=[ 1053], 99.95th=[ 1099],
     | 99.99th=[ 1150]
   bw (  KiB/s): min=47008, max=137203, per=100.00%, avg=87460.80, stdev=5565.96, samples=240
   iops        : min= 2937, max= 8574, avg=5465.52, stdev=347.89, samples=240
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.03%, 10=0.12%, 20=0.23%, 50=27.99%
  lat (msec)   : 100=46.27%, 250=22.42%, 500=0.70%, 750=0.67%, 1000=1.40%
  lat (msec)   : 2000=0.17%
  cpu          : usr=1.51%, sys=4.31%, ctx=145372, majf=0, minf=2098
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.8%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=164379,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=84.4MiB/s (88.5MB/s), 84.4MiB/s-84.4MiB/s (88.5MB/s-88.5MB/s), io=2568MiB (2693MB), run=30437-30437msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=16ki) randread:  read: IOPS=5400, BW=84.4MiB/s (88.5MB/s)(2568MiB/30437msec)


===Fio: workload=randread, time=30, iodepth=128, bs=32ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53759: Sun Jan 14 03:13:15 2024
  read: IOPS=5383, BW=168MiB/s (176MB/s)(5177MiB/30773msec)
    slat (usec): min=5, max=213639, avg=665.28, stdev=1338.63
    clat (usec): min=843, max=2246.2k, avg=94345.74, stdev=233208.34
     lat (usec): min=867, max=2246.3k, avg=95012.07, stdev=233323.85
    clat percentiles (usec):
     |  1.00th=[   1975],  5.00th=[   4359], 10.00th=[   6194],
     | 20.00th=[   9372], 30.00th=[  13304], 40.00th=[  19792],
     | 50.00th=[  35914], 60.00th=[  55837], 70.00th=[  70779],
     | 80.00th=[ 104334], 90.00th=[ 175113], 95.00th=[ 250610],
     | 99.00th=[1635779], 99.50th=[1786774], 99.90th=[1954546],
     | 99.95th=[2038432], 99.99th=[2164261]
   bw (  KiB/s): min=55168, max=290816, per=100.00%, avg=176163.22, stdev=15488.68, samples=240
   iops        : min= 1724, max= 9088, avg=5505.07, stdev=484.02, samples=240
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=1.00%, 4=3.07%, 10=18.02%, 20=18.00%, 50=16.21%
  lat (msec)   : 100=22.96%, 250=15.71%, 500=2.21%, 750=0.50%, 1000=0.32%
  lat (msec)   : 2000=1.91%, >=2000=0.07%
  cpu          : usr=1.63%, sys=4.41%, ctx=150639, majf=0, minf=4145
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.8%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=165667,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=168MiB/s (176MB/s), 168MiB/s-168MiB/s (176MB/s-176MB/s), io=5177MiB (5429MB), run=30773-30773msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=32ki) randread:  read: IOPS=5383, BW=168MiB/s (176MB/s)(5177MiB/30773msec)


===Fio: workload=randread, time=30, iodepth=128, bs=64ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53808: Sun Jan 14 03:13:48 2024
  read: IOPS=4563, BW=285MiB/s (299MB/s)(8813MiB/30900msec)
    slat (usec): min=8, max=126109, avg=828.43, stdev=1125.88
    clat (usec): min=1511, max=1877.4k, avg=111268.78, stdev=157525.17
     lat (usec): min=1541, max=1878.1k, avg=112098.33, stdev=157596.46
    clat percentiles (msec):
     |  1.00th=[   25],  5.00th=[   38], 10.00th=[   44], 20.00th=[   53],
     | 30.00th=[   63], 40.00th=[   72], 50.00th=[   82], 60.00th=[   91],
     | 70.00th=[  101], 80.00th=[  116], 90.00th=[  148], 95.00th=[  203],
     | 99.00th=[ 1062], 99.50th=[ 1167], 99.90th=[ 1318], 99.95th=[ 1368],
     | 99.99th=[ 1770]
   bw (  KiB/s): min=170240, max=462592, per=100.00%, avg=299739.73, stdev=15687.40, samples=240
   iops        : min= 2660, max= 7228, avg=4683.47, stdev=245.11, samples=240
  lat (msec)   : 2=0.01%, 4=0.01%, 10=0.08%, 20=0.36%, 50=16.82%
  lat (msec)   : 100=52.21%, 250=26.86%, 500=1.06%, 750=0.28%, 1000=0.84%
  lat (msec)   : 2000=1.50%
  cpu          : usr=1.36%, sys=4.29%, ctx=136876, majf=0, minf=8239
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.8%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=141012,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=285MiB/s (299MB/s), 285MiB/s-285MiB/s (299MB/s-299MB/s), io=8813MiB (9241MB), run=30900-30900msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=64ki) randread:  read: IOPS=4563, BW=285MiB/s (299MB/s)(8813MiB/30900msec)


===Fio: workload=read, time=30, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53859: Sun Jan 14 03:14:21 2024
  read: IOPS=13.2k, BW=51.5MiB/s (54.0MB/s)(1547MiB/30014msec)
    slat (usec): min=4, max=250989, avg=297.77, stdev=809.75
    clat (msec): min=6, max=289, avg=38.48, stdev=12.76
     lat (msec): min=6, max=289, avg=38.78, stdev=12.83
    clat percentiles (msec):
     |  1.00th=[   23],  5.00th=[   29], 10.00th=[   31], 20.00th=[   35],
     | 30.00th=[   37], 40.00th=[   37], 50.00th=[   38], 60.00th=[   38],
     | 70.00th=[   39], 80.00th=[   40], 90.00th=[   43], 95.00th=[   52],
     | 99.00th=[   93], 99.50th=[  117], 99.90th=[  188], 99.95th=[  241],
     | 99.99th=[  288]
   bw (  KiB/s): min=22648, max=68544, per=99.96%, avg=52759.07, stdev=2022.24, samples=236
   iops        : min= 5662, max=17136, avg=13189.51, stdev=505.55, samples=236
  lat (msec)   : 10=0.06%, 20=0.62%, 50=93.80%, 100=4.63%, 250=0.85%
  lat (msec)   : 500=0.03%
  cpu          : usr=2.84%, sys=7.73%, ctx=381430, majf=0, minf=558
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=396018,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=51.5MiB/s (54.0MB/s), 51.5MiB/s-51.5MiB/s (54.0MB/s-54.0MB/s), io=1547MiB (1622MB), run=30014-30014msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=13.2k, BW=51.5MiB/s (54.0MB/s)(1547MiB/30014msec)


===Fio: workload=read, time=30, iodepth=128, bs=8ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53908: Sun Jan 14 03:14:53 2024
  read: IOPS=13.2k, BW=103MiB/s (108MB/s)(3104MiB/30030msec)
    slat (usec): min=4, max=158744, avg=295.92, stdev=965.33
    clat (usec): min=525, max=250469, avg=38377.18, stdev=18694.60
     lat (usec): min=568, max=254422, avg=38673.95, stdev=18798.76
    clat percentiles (msec):
     |  1.00th=[    7],  5.00th=[   15], 10.00th=[   22], 20.00th=[   32],
     | 30.00th=[   36], 40.00th=[   38], 50.00th=[   39], 60.00th=[   39],
     | 70.00th=[   40], 80.00th=[   41], 90.00th=[   44], 95.00th=[   62],
     | 99.00th=[  123], 99.50th=[  165], 99.90th=[  201], 99.95th=[  220],
     | 99.99th=[  243]
   bw (  KiB/s): min=50527, max=135424, per=100.00%, avg=105905.59, stdev=4183.45, samples=238
   iops        : min= 6315, max=16928, avg=13238.00, stdev=522.95, samples=238
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.10%, 4=0.24%, 10=1.69%, 20=6.75%, 50=84.19%
  lat (msec)   : 100=5.05%, 250=1.96%, 500=0.01%
  cpu          : usr=2.84%, sys=7.70%, ctx=385062, majf=0, minf=1075
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=397333,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=103MiB/s (108MB/s), 103MiB/s-103MiB/s (108MB/s-108MB/s), io=3104MiB (3255MB), run=30030-30030msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=8ki) read:  read: IOPS=13.2k, BW=103MiB/s (108MB/s)(3104MiB/30030msec)


===Fio: workload=read, time=30, iodepth=128, bs=16ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=53957: Sun Jan 14 03:15:26 2024
  read: IOPS=12.2k, BW=190MiB/s (199MB/s)(5709MiB/30064msec)
    slat (usec): min=4, max=149477, avg=322.96, stdev=1019.16
    clat (usec): min=574, max=315369, avg=41739.14, stdev=20334.53
     lat (usec): min=741, max=315697, avg=42062.94, stdev=20441.76
    clat percentiles (msec):
     |  1.00th=[    9],  5.00th=[   17], 10.00th=[   24], 20.00th=[   33],
     | 30.00th=[   37], 40.00th=[   40], 50.00th=[   41], 60.00th=[   42],
     | 70.00th=[   43], 80.00th=[   45], 90.00th=[   55], 95.00th=[   77],
     | 99.00th=[  122], 99.50th=[  155], 99.90th=[  224], 99.95th=[  255],
     | 99.99th=[  309]
   bw (  KiB/s): min=85280, max=276736, per=100.00%, avg=194771.09, stdev=10031.46, samples=237
   iops        : min= 5330, max=17296, avg=12173.19, stdev=626.96, samples=237
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.06%, 4=0.17%, 10=1.40%, 20=5.76%, 50=79.56%
  lat (msec)   : 100=10.64%, 250=2.34%, 500=0.06%
  cpu          : usr=2.56%, sys=7.30%, ctx=363300, majf=0, minf=2100
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=365349,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=190MiB/s (199MB/s), 190MiB/s-190MiB/s (199MB/s-199MB/s), io=5709MiB (5986MB), run=30064-30064msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=16ki) read:  read: IOPS=12.2k, BW=190MiB/s (199MB/s)(5709MiB/30064msec)


===Fio: workload=read, time=30, iodepth=128, bs=32ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54013: Sun Jan 14 03:15:58 2024
  read: IOPS=12.6k, BW=393MiB/s (412MB/s)(11.5GiB/30037msec)
    slat (usec): min=5, max=136394, avg=311.52, stdev=935.09
    clat (usec): min=853, max=361433, avg=40385.80, stdev=26855.66
     lat (usec): min=959, max=361970, avg=40698.14, stdev=26977.80
    clat percentiles (msec):
     |  1.00th=[    6],  5.00th=[   12], 10.00th=[   16], 20.00th=[   23],
     | 30.00th=[   28], 40.00th=[   33], 50.00th=[   37], 60.00th=[   41],
     | 70.00th=[   44], 80.00th=[   50], 90.00th=[   66], 95.00th=[   89],
     | 99.00th=[  150], 99.50th=[  178], 99.90th=[  266], 99.95th=[  292],
     | 99.99th=[  317]
   bw (  KiB/s): min=203776, max=577408, per=100.00%, avg=403913.76, stdev=25389.19, samples=236
   iops        : min= 6368, max=18044, avg=12622.31, stdev=793.41, samples=236
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.14%, 4=0.43%, 10=3.43%, 20=12.50%, 50=63.90%
  lat (msec)   : 100=16.09%, 250=3.37%, 500=0.14%
  cpu          : usr=2.56%, sys=7.77%, ctx=364486, majf=0, minf=4145
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=377736,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=393MiB/s (412MB/s), 393MiB/s-393MiB/s (412MB/s-412MB/s), io=11.5GiB (12.4GB), run=30037-30037msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=32ki) read:  read: IOPS=12.6k, BW=393MiB/s (412MB/s)(11.5GiB/30037msec)


===Fio: workload=read, time=30, iodepth=128, bs=64ki, disks=4 ===

j1: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54065: Sun Jan 14 03:16:31 2024
  read: IOPS=8243, BW=515MiB/s (540MB/s)(15.1GiB/30043msec)
    slat (usec): min=7, max=115459, avg=475.36, stdev=1198.12
    clat (msec): min=3, max=492, avg=61.59, stdev=37.52
     lat (msec): min=3, max=492, avg=62.06, stdev=37.67
    clat percentiles (msec):
     |  1.00th=[   18],  5.00th=[   24], 10.00th=[   29], 20.00th=[   36],
     | 30.00th=[   42], 40.00th=[   48], 50.00th=[   54], 60.00th=[   59],
     | 70.00th=[   66], 80.00th=[   78], 90.00th=[  103], 95.00th=[  138],
     | 99.00th=[  211], 99.50th=[  241], 99.90th=[  305], 99.95th=[  342],
     | 99.99th=[  393]
   bw (  KiB/s): min=233088, max=791936, per=99.82%, avg=526649.06, stdev=28618.25, samples=237
   iops        : min= 3642, max=12374, avg=8228.89, stdev=447.16, samples=237
  lat (msec)   : 4=0.01%, 10=0.04%, 20=1.99%, 50=41.96%, 100=45.49%
  lat (msec)   : 250=10.14%, 500=0.38%
  cpu          : usr=2.02%, sys=6.24%, ctx=242500, majf=0, minf=8241
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=247667,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=515MiB/s (540MB/s), 515MiB/s-515MiB/s (540MB/s-540MB/s), io=15.1GiB (16.2GB), run=30043-30043msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=64ki) read:  read: IOPS=8243, BW=515MiB/s (540MB/s)(15.1GiB/30043msec)


===Fio: workload=randread, time=30, iodepth=1, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54115: Sun Jan 14 03:17:03 2024
  read: IOPS=510, BW=2041KiB/s (2090kB/s)(59.8MiB/30011msec)
    slat (usec): min=14, max=456, avg=38.88, stdev= 7.59
    clat (usec): min=719, max=173818, avg=7788.59, stdev=7762.70
     lat (usec): min=760, max=173881, avg=7828.66, stdev=7762.79
    clat percentiles (usec):
     |  1.00th=[   996],  5.00th=[  1172], 10.00th=[  1254], 20.00th=[  3032],
     | 30.00th=[  4359], 40.00th=[  5604], 50.00th=[  6783], 60.00th=[  7898],
     | 70.00th=[  9110], 80.00th=[ 10159], 90.00th=[ 13042], 95.00th=[ 20841],
     | 99.00th=[ 33817], 99.50th=[ 39584], 99.90th=[110625], 99.95th=[125305],
     | 99.99th=[149947]
   bw (  KiB/s): min= 1176, max= 2634, per=99.96%, avg=2041.00, stdev=73.83, samples=236
   iops        : min=  294, max=  658, avg=510.20, stdev=18.44, samples=236
  lat (usec)   : 750=0.01%, 1000=1.03%
  lat (msec)   : 2=15.80%, 4=10.52%, 10=51.47%, 20=15.69%, 50=5.24%
  lat (msec)   : 100=0.12%, 250=0.12%
  cpu          : usr=0.30%, sys=0.75%, ctx=15375, majf=0, minf=51
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=15311,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=2041KiB/s (2090kB/s), 2041KiB/s-2041KiB/s (2090kB/s-2090kB/s), io=59.8MiB (62.7MB), run=30011-30011msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=1; bs=4k) randread:  read: IOPS=510, BW=2041KiB/s (2090kB/s)(59.8MiB/30011msec)


===Fio: workload=randread, time=30, iodepth=32, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54167: Sun Jan 14 03:17:37 2024
  read: IOPS=6874, BW=26.9MiB/s (28.2MB/s)(827MiB/30789msec)
    slat (usec): min=4, max=466, avg=16.90, stdev= 9.57
    clat (usec): min=395, max=1658.9k, avg=18556.00, stdev=59948.25
     lat (usec): min=408, max=1658.9k, avg=18573.45, stdev=59948.45
    clat percentiles (usec):
     |  1.00th=[    594],  5.00th=[    717], 10.00th=[    848],
     | 20.00th=[   2606], 30.00th=[   4359], 40.00th=[   6063],
     | 50.00th=[   7701], 60.00th=[   9372], 70.00th=[  11338],
     | 80.00th=[  16057], 90.00th=[  26608], 95.00th=[  47449],
     | 99.00th=[ 354419], 99.50th=[ 450888], 99.90th=[ 734004],
     | 99.95th=[ 859833], 99.99th=[1451230]
   bw (  KiB/s): min=12333, max=135624, per=100.00%, avg=28216.93, stdev=3736.06, samples=240
   iops        : min= 3083, max=33906, avg=7053.85, stdev=934.04, samples=240
  lat (usec)   : 500=0.07%, 750=6.52%, 1000=5.99%
  lat (msec)   : 2=4.27%, 4=11.18%, 10=36.32%, 20=21.06%, 50=9.87%
  lat (msec)   : 100=2.14%, 250=1.17%, 500=1.12%, 750=0.20%, 1000=0.07%
  lat (msec)   : 2000=0.03%
  cpu          : usr=2.20%, sys=4.34%, ctx=168387, majf=0, minf=176
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=99.9%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=211650,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=26.9MiB/s (28.2MB/s), 26.9MiB/s-26.9MiB/s (28.2MB/s-28.2MB/s), io=827MiB (867MB), run=30789-30789msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=32; bs=4k) randread:  read: IOPS=6874, BW=26.9MiB/s (28.2MB/s)(827MiB/30789msec)


===Fio: workload=randread, time=30, iodepth=64, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54216: Sun Jan 14 03:18:09 2024
  read: IOPS=8098, BW=31.6MiB/s (33.2MB/s)(959MiB/30304msec)
    slat (usec): min=4, max=99921, avg=320.60, stdev=655.25
    clat (usec): min=387, max=793198, avg=31260.59, stdev=62860.86
     lat (usec): min=398, max=793207, avg=31581.92, stdev=62913.27
    clat percentiles (usec):
     |  1.00th=[   553],  5.00th=[   668], 10.00th=[   848], 20.00th=[  5145],
     | 30.00th=[  8356], 40.00th=[ 11469], 50.00th=[ 16319], 60.00th=[ 21365],
     | 70.00th=[ 26870], 80.00th=[ 34341], 90.00th=[ 51643], 95.00th=[103285],
     | 99.00th=[350225], 99.50th=[438305], 99.90th=[650118], 99.95th=[700449],
     | 99.99th=[759170]
   bw (  KiB/s): min=17411, max=56088, per=100.00%, avg=32695.22, stdev=2524.80, samples=240
   iops        : min= 4352, max=14022, avg=8173.68, stdev=631.21, samples=240
  lat (usec)   : 500=0.14%, 750=7.58%, 1000=3.88%
  lat (msec)   : 2=1.30%, 4=4.12%, 10=18.81%, 20=21.70%, 50=32.02%
  lat (msec)   : 100=5.30%, 250=2.96%, 500=1.83%, 750=0.34%, 1000=0.01%
  cpu          : usr=2.07%, sys=5.19%, ctx=195815, majf=0, minf=441
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=245430,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=31.6MiB/s (33.2MB/s), 31.6MiB/s-31.6MiB/s (33.2MB/s-33.2MB/s), io=959MiB (1005MB), run=30304-30304msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=64; bs=4k) randread:  read: IOPS=8098, BW=31.6MiB/s (33.2MB/s)(959MiB/30304msec)


===Fio: workload=randread, time=30, iodepth=128, bs=4k, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54265: Sun Jan 14 03:18:42 2024
  read: IOPS=7985, BW=31.2MiB/s (32.7MB/s)(949MiB/30434msec)
    slat (usec): min=4, max=85706, avg=483.93, stdev=780.75
    clat (usec): min=416, max=1029.1k, avg=63594.75, stdev=75258.56
     lat (usec): min=449, max=1030.2k, avg=64079.56, stdev=75357.13
    clat percentiles (usec):
     |  1.00th=[   545],  5.00th=[   644], 10.00th=[   881], 20.00th=[ 12256],
     | 30.00th=[ 36439], 40.00th=[ 47449], 50.00th=[ 55837], 60.00th=[ 63177],
     | 70.00th=[ 71828], 80.00th=[ 82314], 90.00th=[100140], 95.00th=[137364],
     | 99.00th=[484443], 99.50th=[541066], 99.90th=[624952], 99.95th=[683672],
     | 99.99th=[851444]
   bw (  KiB/s): min=15736, max=65088, per=100.00%, avg=32337.47, stdev=4331.56, samples=240
   iops        : min= 3934, max=16272, avg=8084.37, stdev=1082.89, samples=240
  lat (usec)   : 500=0.18%, 750=7.93%, 1000=2.59%
  lat (msec)   : 2=0.78%, 4=1.63%, 10=5.63%, 20=4.08%, 50=19.90%
  lat (msec)   : 100=47.08%, 250=7.47%, 500=1.88%, 750=0.84%, 1000=0.02%
  lat (msec)   : 2000=0.01%
  cpu          : usr=1.98%, sys=5.10%, ctx=199558, majf=0, minf=559
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=243039,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=31.2MiB/s (32.7MB/s), 31.2MiB/s-31.2MiB/s (32.7MB/s-32.7MB/s), io=949MiB (995MB), run=30434-30434msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4k) randread:  read: IOPS=7985, BW=31.2MiB/s (32.7MB/s)(949MiB/30434msec)


===Fio: workload=read, time=30, iodepth=1, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54315: Sun Jan 14 03:19:15 2024
  read: IOPS=3247, BW=12.7MiB/s (13.3MB/s)(381MiB/30001msec)
    slat (usec): min=5, max=464, avg=32.51, stdev=12.22
    clat (usec): min=540, max=704496, avg=1190.23, stdev=3308.85
     lat (usec): min=563, max=704532, avg=1223.80, stdev=3308.97
    clat percentiles (usec):
     |  1.00th=[  832],  5.00th=[  930], 10.00th=[  988], 20.00th=[ 1057],
     | 30.00th=[ 1106], 40.00th=[ 1139], 50.00th=[ 1172], 60.00th=[ 1188],
     | 70.00th=[ 1221], 80.00th=[ 1254], 90.00th=[ 1303], 95.00th=[ 1352],
     | 99.00th=[ 1516], 99.50th=[ 1614], 99.90th=[ 7898], 99.95th=[15008],
     | 99.99th=[20579]
   bw (  KiB/s): min= 9368, max=14880, per=100.00%, avg=13107.10, stdev=215.78, samples=234
   iops        : min= 2342, max= 3720, avg=3275.96, stdev=53.94, samples=234
  lat (usec)   : 750=0.22%, 1000=11.64%
  lat (msec)   : 2=87.99%, 4=0.03%, 10=0.03%, 20=0.08%, 50=0.01%
  lat (msec)   : 250=0.01%, 750=0.01%
  cpu          : usr=1.68%, sys=4.13%, ctx=98067, majf=0, minf=57
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=97414,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=12.7MiB/s (13.3MB/s), 12.7MiB/s-12.7MiB/s (13.3MB/s-13.3MB/s), io=381MiB (399MB), run=30001-30001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=1; bs=4k) read:  read: IOPS=3247, BW=12.7MiB/s (13.3MB/s)(381MiB/30001msec)


===Fio: workload=read, time=30, iodepth=32, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54365: Sun Jan 14 03:19:47 2024
  read: IOPS=13.5k, BW=52.6MiB/s (55.1MB/s)(1578MiB/30011msec)
    slat (usec): min=4, max=501, avg=14.87, stdev= 7.58
    clat (usec): min=504, max=304357, avg=9491.11, stdev=8612.90
     lat (usec): min=516, max=304364, avg=9506.48, stdev=8613.49
    clat percentiles (usec):
     |  1.00th=[  1598],  5.00th=[  2180], 10.00th=[  4817], 20.00th=[  8586],
     | 30.00th=[  9110], 40.00th=[  9241], 50.00th=[  9372], 60.00th=[  9503],
     | 70.00th=[  9634], 80.00th=[  9765], 90.00th=[ 10421], 95.00th=[ 12387],
     | 99.00th=[ 26084], 99.50th=[ 32900], 99.90th=[120062], 99.95th=[248513],
     | 99.99th=[299893]
   bw (  KiB/s): min=24040, max=116240, per=100.00%, avg=53886.10, stdev=3717.45, samples=236
   iops        : min= 6010, max=29060, avg=13471.53, stdev=929.36, samples=236
  lat (usec)   : 750=0.01%, 1000=0.03%
  lat (msec)   : 2=3.74%, 4=5.85%, 10=75.32%, 20=12.60%, 50=2.29%
  lat (msec)   : 100=0.04%, 250=0.08%, 500=0.05%
  cpu          : usr=2.91%, sys=7.54%, ctx=371542, majf=0, minf=177
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=403899,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=52.6MiB/s (55.1MB/s), 52.6MiB/s-52.6MiB/s (55.1MB/s-55.1MB/s), io=1578MiB (1654MB), run=30011-30011msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=32; bs=4k) read:  read: IOPS=13.5k, BW=52.6MiB/s (55.1MB/s)(1578MiB/30011msec)


===Fio: workload=read, time=30, iodepth=64, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54414: Sun Jan 14 03:20:20 2024
  read: IOPS=13.2k, BW=51.6MiB/s (54.1MB/s)(1548MiB/30021msec)
    slat (usec): min=4, max=251396, avg=93.76, stdev=553.64
    clat (usec): min=431, max=305200, avg=19289.59, stdev=12034.22
     lat (usec): min=452, max=305492, avg=19383.93, stdev=12075.75
    clat percentiles (usec):
     |  1.00th=[  1860],  5.00th=[  7046], 10.00th=[ 13960], 20.00th=[ 16057],
     | 30.00th=[ 17695], 40.00th=[ 18220], 50.00th=[ 18482], 60.00th=[ 18744],
     | 70.00th=[ 19006], 80.00th=[ 19268], 90.00th=[ 21103], 95.00th=[ 34341],
     | 99.00th=[ 55837], 99.50th=[ 67634], 99.90th=[193987], 99.95th=[221250],
     | 99.99th=[299893]
   bw (  KiB/s): min=17832, max=82992, per=100.00%, avg=52818.03, stdev=3028.87, samples=236
   iops        : min= 4458, max=20748, avg=13204.51, stdev=757.22, samples=236
  lat (usec)   : 500=0.01%, 750=0.12%, 1000=0.18%
  lat (msec)   : 2=0.84%, 4=2.35%, 10=2.43%, 20=80.65%, 50=11.52%
  lat (msec)   : 100=1.61%, 250=0.25%, 500=0.04%
  cpu          : usr=2.98%, sys=7.77%, ctx=376855, majf=0, minf=358
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=396352,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=51.6MiB/s (54.1MB/s), 51.6MiB/s-51.6MiB/s (54.1MB/s-54.1MB/s), io=1548MiB (1623MB), run=30021-30021msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=64; bs=4k) read:  read: IOPS=13.2k, BW=51.6MiB/s (54.1MB/s)(1548MiB/30021msec)


===Fio: workload=read, time=30, iodepth=128, bs=4k, disks=4 ===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=54465: Sun Jan 14 03:20:52 2024
  read: IOPS=13.7k, BW=53.4MiB/s (56.0MB/s)(1626MiB/30455msec)
    slat (usec): min=4, max=478013, avg=283.75, stdev=1578.64
    clat (usec): min=449, max=1636.4k, avg=37075.49, stdev=38421.98
     lat (usec): min=564, max=1636.5k, avg=37360.04, stdev=38617.40
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[   13], 10.00th=[   26], 20.00th=[   30],
     | 30.00th=[   33], 40.00th=[   36], 50.00th=[   37], 60.00th=[   37],
     | 70.00th=[   38], 80.00th=[   39], 90.00th=[   41], 95.00th=[   48],
     | 99.00th=[  114], 99.50th=[  150], 99.90th=[  558], 99.95th=[ 1045],
     | 99.99th=[ 1552]
   bw (  KiB/s): min= 5040, max=83456, per=100.00%, avg=55432.67, stdev=3367.82, samples=240
   iops        : min= 1260, max=20864, avg=13858.17, stdev=841.96, samples=240
  lat (usec)   : 500=0.01%, 750=0.07%, 1000=0.10%
  lat (msec)   : 2=0.39%, 4=0.57%, 10=3.03%, 20=3.20%, 50=88.05%
  lat (msec)   : 100=3.14%, 250=1.18%, 500=0.15%, 750=0.05%, 1000=0.02%
  lat (msec)   : 2000=0.06%
  cpu          : usr=2.81%, sys=7.46%, ctx=394363, majf=0, minf=568
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=416254,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=53.4MiB/s (56.0MB/s), 53.4MiB/s-53.4MiB/s (56.0MB/s-56.0MB/s), io=1626MiB (1705MB), run=30455-30455msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4k) read:  read: IOPS=13.7k, BW=53.4MiB/s (56.0MB/s)(1626MiB/30455msec)
umount: /mnt/fsbench: not mounted.
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:02:10.rbd-multi.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:02:10.rbd-multi.triple-hdd.txt
Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=5631, BW=22.0MiB/s (23.1MB/s)(1326MiB/60283msec)
Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=6535, BW=25.5MiB/s (26.8MB/s)(771MiB/30214msec)
Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=6452, BW=25.2MiB/s (26.4MB/s)(768MiB/30471msec)
Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=8988, BW=35.1MiB/s (36.8MB/s)(1076MiB/30642msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=7335, BW=28.7MiB/s (30.0MB/s)(872MiB/30433msec)
Fio (disks=1, iodepth=128; bs=4ki) read:  read: IOPS=3598, BW=14.1MiB/s (14.7MB/s)(422MiB/30038msec)
Fio (disks=2, iodepth=128; bs=4ki) read:  read: IOPS=7205, BW=28.1MiB/s (29.5MB/s)(845MiB/30038msec)
Fio (disks=3, iodepth=128; bs=4ki) read:  read: IOPS=10.8k, BW=42.1MiB/s (44.1MB/s)(1263MiB/30039msec)
Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=13.7k, BW=53.6MiB/s (56.2MB/s)(1609MiB/30038msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=7997, BW=31.2MiB/s (32.8MB/s)(958MiB/30676msec)
Fio (disks=4, iodepth=128; bs=8ki) randread:  read: IOPS=5674, BW=44.3MiB/s (46.5MB/s)(1363MiB/30742msec)
Fio (disks=4, iodepth=128; bs=16ki) randread:  read: IOPS=5400, BW=84.4MiB/s (88.5MB/s)(2568MiB/30437msec)
Fio (disks=4, iodepth=128; bs=32ki) randread:  read: IOPS=5383, BW=168MiB/s (176MB/s)(5177MiB/30773msec)
Fio (disks=4, iodepth=128; bs=64ki) randread:  read: IOPS=4563, BW=285MiB/s (299MB/s)(8813MiB/30900msec)
Fio (disks=4, iodepth=128; bs=4ki) read:  read: IOPS=13.2k, BW=51.5MiB/s (54.0MB/s)(1547MiB/30014msec)
Fio (disks=4, iodepth=128; bs=8ki) read:  read: IOPS=13.2k, BW=103MiB/s (108MB/s)(3104MiB/30030msec)
Fio (disks=4, iodepth=128; bs=16ki) read:  read: IOPS=12.2k, BW=190MiB/s (199MB/s)(5709MiB/30064msec)
Fio (disks=4, iodepth=128; bs=32ki) read:  read: IOPS=12.6k, BW=393MiB/s (412MB/s)(11.5GiB/30037msec)
Fio (disks=4, iodepth=128; bs=64ki) read:  read: IOPS=8243, BW=515MiB/s (540MB/s)(15.1GiB/30043msec)
Fio (disks=4, iodepth=1; bs=4k) randread:  read: IOPS=510, BW=2041KiB/s (2090kB/s)(59.8MiB/30011msec)
Fio (disks=4, iodepth=32; bs=4k) randread:  read: IOPS=6874, BW=26.9MiB/s (28.2MB/s)(827MiB/30789msec)
Fio (disks=4, iodepth=64; bs=4k) randread:  read: IOPS=8098, BW=31.6MiB/s (33.2MB/s)(959MiB/30304msec)
Fio (disks=4, iodepth=128; bs=4k) randread:  read: IOPS=7985, BW=31.2MiB/s (32.7MB/s)(949MiB/30434msec)
Fio (disks=4, iodepth=1; bs=4k) read:  read: IOPS=3247, BW=12.7MiB/s (13.3MB/s)(381MiB/30001msec)
Fio (disks=4, iodepth=32; bs=4k) read:  read: IOPS=13.5k, BW=52.6MiB/s (55.1MB/s)(1578MiB/30011msec)
Fio (disks=4, iodepth=64; bs=4k) read:  read: IOPS=13.2k, BW=51.6MiB/s (54.1MB/s)(1548MiB/30021msec)
Fio (disks=4, iodepth=128; bs=4k) read:  read: IOPS=13.7k, BW=53.4MiB/s (56.0MB/s)(1626MiB/30455msec)
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=malloc
++ date +%FT%T
+ cur_time=2024-01-14T03:20:56
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-malloc/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:20:56.lsvd-malloc.240.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC nvme.cc
CC mkcache.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-14T03:20:56
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thick rssd2 lsvd-benchmark 80g
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark
Removing all objects from pool rssd2 with prefix lsvd-benchmark
++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 29934/52376 objects
+ ./thick-image --size=80g rssd2/lsvd-benchmark
Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 45% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 80% complete...Thick provisioning: 81% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 87% complete...Thick provisioning: 88% complete...Thick provisioning: 89% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 92% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p rssd2 stat lsvd-benchmark
rssd2/lsvd-benchmark mtime 2024-01-14T03:25:14.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-malloc/lsvd-write/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-malloc/lsvd-write/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-malloc/lsvd-write//lsvd-write/
mkdir: cannot create directory ‘/mnt/nvme-malloc/lsvd-write//lsvd-write/’: No space left on device
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=malloc
++ date +%FT%T
+ cur_time=2024-01-14T03:25:14
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-malloc/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:25:14.lsvd-malloc.240.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-14T03:25:14
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thick triple-hdd lsvd-benchmark 80g
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark
Removing all objects from pool triple-hdd with prefix lsvd-benchmark
+++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 9211/53625 objects
+ ./thick-image --size=80g triple-hdd/lsvd-benchmark
Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 45% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 80% complete...Thick provisioning: 81% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 87% complete...Thick provisioning: 88% complete...Thick provisioning: 89% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 92% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p triple-hdd stat lsvd-benchmark
triple-hdd/lsvd-benchmark mtime 2024-01-14T03:34:03.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-malloc/lsvd-write/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-malloc/lsvd-write/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-malloc/lsvd-write//lsvd-write/
mkdir: cannot create directory ‘/mnt/nvme-malloc/lsvd-write//lsvd-write/’: No space left on device
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-14T03:34:08
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:34:08.lsvd-nvme.240.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-14T03:34:09
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T03:34:09.lsvd-nvme.240.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
