+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=malloc
++ date +%FT%T
+ cur_time=2024-01-13T16:33:19
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-malloc/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:33:19.lsvd-malloc.240.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T16:33:19
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin rssd2 lsvd-benchmark 80g
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark
Removing all objects from pool rssd2 with prefix lsvd-benchmark
+++++++++++++++++++++++++
Removed 2/25847 objects
+ ./imgtool --create --rados --size=80g rssd2/lsvd-benchmark
+ rados -p rssd2 stat lsvd-benchmark
rssd2/lsvd-benchmark mtime 2024-01-13T16:33:51.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-malloc/lsvd-write/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-malloc/lsvd-write/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=257698037760
+ LSVD_CACHE_SIZE=257698037760
+ rm -rf /mnt/nvme-malloc/lsvd-write//lsvd-write/e078579e-6e2b-4d9b-9140-d3d3ba54e64e.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 16:33:51.421573] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 16:33:51.421697] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid27705 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 16:33:51.494024] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 16:33:51.596895] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 16:33:51.596952] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 16:33:51.597004] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-13 16:33:51.597007] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 16:33:57.192374] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 16:33:57.886957] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 245760 MiB in 16 shards, 15360 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-malloc/lsvd-write//lsvd-write//b12d93a2-960c-4955-aaac-51c2f669cc6b.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 16:34:00.215029] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:33:19.lsvd-malloc.240.rssd2.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:33:19.lsvd-malloc.240.rssd2.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/randomwrite.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:33:19.lsvd-malloc.240.rssd2.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
+ printf '===Starting client benchmark\n\n'
===Starting client benchmark

+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.343062 s, 306 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 4449630d-68e8-4724-91a7-d138005805a2
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.013: File-server Version 3.0 personality successfully loaded
0.013: Populating and pre-allocating filesets
0.222: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.222: Removing bigfileset tree (if exists)
0.225: Pre-allocating directories in bigfileset tree
0.617: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 55/55 hits, 0 MiB read, 0.000 read amp, 55 total
[0m37.871: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
37.871: Population and pre-allocation of filesets completed
37.871: Starting 1 filereader instances
38.879: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 120/120 hits, 0 MiB read, 0.000 read amp, 120 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 123/123 hits, 0 MiB read, 0.000 read amp, 123 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 124/124 hits, 0 MiB read, 0.000 read amp, 124 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 125/125 hits, 0 MiB read, 0.000 read amp, 125 total
[0m218.895: Run took 180 seconds...
218.901: Per-Operation Breakdown
statfile1            1176054ops     6533ops/s   0.0mb/s    0.012ms/op [0.004ms - 11.294ms]
deletefile1          1176055ops     6533ops/s   0.0mb/s    0.207ms/op [0.029ms - 71.168ms]
closefile3           1176056ops     6533ops/s   0.0mb/s    0.004ms/op [0.001ms - 7.363ms]
readfile1            1176057ops     6533ops/s 860.1mb/s    0.097ms/op [0.005ms - 40.323ms]
openfile2            1176057ops     6533ops/s   0.0mb/s    0.056ms/op [0.006ms - 269.317ms]
closefile2           1176057ops     6533ops/s   0.0mb/s    0.004ms/op [0.001ms - 11.384ms]
appendfilerand1      1176057ops     6533ops/s  51.0mb/s    0.436ms/op [0.001ms - 203.041ms]
openfile1            1176063ops     6533ops/s   0.0mb/s    0.060ms/op [0.007ms - 269.772ms]
closefile1           1176063ops     6533ops/s   0.0mb/s    0.006ms/op [0.001ms - 7.776ms]
wrtfile1             1176065ops     6533ops/s 817.4mb/s    5.993ms/op [0.013ms - 206.470ms]
createfile1          1176104ops     6533ops/s   0.0mb/s    0.119ms/op [0.023ms - 269.335ms]
218.901: IO Summary: 12936688 ops 71864.196 ops/s 6533/13066 rd/wr 1728.5mb/s 0.636ms/op
218.901: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:218.901: IO Summary: 12936688 ops 71864.196 ops/s 6533/13066 rd/wr 1728.5mb/s 0.636ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: OLTP Version 3.0  personality successfully loaded
0.003: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.006: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.163: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.163: Removing datafiles tree (if exists)
0.166: Pre-allocating directories in datafiles tree
0.166: Pre-allocating files in datafiles tree
37.098: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
37.098: Population and pre-allocation of filesets completed
37.098: Starting 200 shadow instances
37.228: Starting 10 dbwr instances
37.233: Starting 1 lgwr instances
38.236: Running...
218.288: Run took 180 seconds...
218.313: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     4545000ops    25243ops/s   0.0mb/s    6.305ms/op [0.018ms - 408.918ms]
shadow-post-lg       4545200ops    25244ops/s   0.0mb/s    0.076ms/op [0.002ms - 54.138ms]
shadowhog            4545200ops    25244ops/s   0.0mb/s    0.334ms/op [0.091ms - 84.291ms]
shadowread           4570800ops    25386ops/s  49.3mb/s    1.102ms/op [0.001ms - 214.946ms]
dbwr-aiowait         45448ops      252ops/s   0.0mb/s    2.178ms/op [0.004ms - 63.252ms]
dbwr-block           45448ops      252ops/s   0.0mb/s   19.329ms/op [0.003ms - 66.120ms]
dbwr-hog             45457ops      252ops/s   0.0mb/s    0.010ms/op [0.004ms - 10.050ms]
dbwrite-a            4547080ops    25255ops/s  49.3mb/s    0.003ms/op [0.000ms - 50.342ms]
lg-block             1420ops        8ops/s   0.0mb/s  126.694ms/op [36.188ms - 832.344ms]
lg-aiowait           1421ops        8ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.043ms]
lg-write             1422ops        8ops/s   2.0mb/s    0.009ms/op [0.001ms - 0.104ms]
218.313: IO Summary: 9166171 ops 50909.168 ops/s 25386/25263 rd/wr 100.6mb/s 0.562ms/op
218.313: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:218.313: IO Summary: 9166171 ops 50909.168 ops/s 25386/25263 rd/wr 100.6mb/s 0.562ms/op


===Filebench: workload=/tmp/filebench/randomwrite.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.002: Latency histogram enabled
0.002: Random Write Version 3.0 personality successfully loaded
0.002: Populating and pre-allocating filesets
0.002: Removing largefile1 tree (if exists)
0.005: Pre-allocating directories in largefile1 tree
0.005: Pre-allocating files in largefile1 tree
4.344: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
4.344: Population and pre-allocation of filesets completed
4.345: Starting 1 rand-write instances
5.347: Running...
185.364: Run took 180 seconds...
185.364: Per-Operation Breakdown
rand-write1          26607611ops   147806ops/s 1154.7mb/s    0.005ms/op [0.001ms - 176.893ms]	[ 0 0 0 0 0 0 0 0 0 0 650 33073 9697628 16743788 77240 45695 7363 943 31 1077 35 18 12 21 28 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ]
185.364: IO Summary: 26607611 ops 147806.320 ops/s 0/147806 rd/wr 1154.7mb/s 0.005ms/op
185.364: Shutting down processes

RESULT: Filebench /tmp/filebench/randomwrite.f:185.364: IO Summary: 26607611 ops 147806.320 ops/s 0/147806 rd/wr 1154.7mb/s 0.005ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.005: Varmail Version 3.0 personality successfully loaded
0.005: Populating and pre-allocating filesets
1.060: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
1.060: Removing bigfileset tree (if exists)
1.063: Pre-allocating directories in bigfileset tree
1.063: Pre-allocating files in bigfileset tree
77.749: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
77.749: Population and pre-allocation of filesets completed
77.750: Starting 1 filereader instances
78.756: Running...
258.772: Run took 180 seconds...
258.772: Per-Operation Breakdown
closefile4           376182ops     2090ops/s   0.0mb/s    0.003ms/op [0.001ms - 1.781ms]
readfile4            376182ops     2090ops/s  53.9mb/s    0.570ms/op [0.005ms - 77.719ms]
openfile4            376185ops     2090ops/s   0.0mb/s    0.022ms/op [0.005ms - 10.474ms]
closefile3           376185ops     2090ops/s   0.0mb/s    0.006ms/op [0.001ms - 3.123ms]
fsyncfile3           376185ops     2090ops/s   0.0mb/s    2.365ms/op [0.160ms - 209.297ms]
appendfilerand3      376188ops     2090ops/s  16.3mb/s    0.106ms/op [0.007ms - 28.904ms]
readfile3            376190ops     2090ops/s  54.0mb/s    0.613ms/op [0.004ms - 77.897ms]
openfile3            376191ops     2090ops/s   0.0mb/s    0.022ms/op [0.005ms - 9.548ms]
closefile2           376191ops     2090ops/s   0.0mb/s    0.006ms/op [0.001ms - 4.481ms]
fsyncfile2           376191ops     2090ops/s   0.0mb/s    2.461ms/op [0.651ms - 209.851ms]
appendfilerand2      376195ops     2090ops/s  16.3mb/s    0.243ms/op [0.015ms - 77.959ms]
createfile2          376195ops     2090ops/s   0.0mb/s    0.514ms/op [0.030ms - 209.795ms]
deletefile1          376195ops     2090ops/s   0.0mb/s    0.583ms/op [0.044ms - 209.504ms]
258.772: IO Summary: 4890455 ops 27166.966 ops/s 4180/4180 rd/wr 140.6mb/s 0.578ms/op
258.772: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:258.772: IO Summary: 4890455 ops 27166.966 ops/s 4180/4180 rd/wr 140.6mb/s 0.578ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:33:19.lsvd-malloc.240.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:33:19.lsvd-malloc.240.rssd2.txt
Filebench /tmp/filebench/fileserver.f:218.901: IO Summary: 12936688 ops 71864.196 ops/s 6533/13066 rd/wr 1728.5mb/s 0.636ms/op
Filebench /tmp/filebench/oltp.f:218.313: IO Summary: 9166171 ops 50909.168 ops/s 25386/25263 rd/wr 100.6mb/s 0.562ms/op
Filebench /tmp/filebench/randomwrite.f:185.364: IO Summary: 26607611 ops 147806.320 ops/s 0/147806 rd/wr 1154.7mb/s 0.005ms/op
Filebench /tmp/filebench/varmail.f:258.772: IO Summary: 4890455 ops 27166.966 ops/s 4180/4180 rd/wr 140.6mb/s 0.578ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7f7913fef640) exiting
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=malloc
++ date +%FT%T
+ cur_time=2024-01-13T16:49:30
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-malloc/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:49:30.lsvd-malloc.240.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC translate.cc
CC img_reader.cc
CC io.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T16:49:30
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin triple-hdd lsvd-benchmark 80g
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark
Removing all objects from pool triple-hdd with prefix lsvd-benchmark
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 26883/71297 objects
+ ./imgtool --create --rados --size=80g triple-hdd/lsvd-benchmark
+ rados -p triple-hdd stat lsvd-benchmark
triple-hdd/lsvd-benchmark mtime 2024-01-13T17:00:59.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-malloc/lsvd-write/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-malloc/lsvd-write/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=257698037760
+ LSVD_CACHE_SIZE=257698037760
+ rm -rf /mnt/nvme-malloc/lsvd-write//lsvd-write/b12d93a2-960c-4955-aaac-51c2f669cc6b.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 17:00:59.892571] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 17:00:59.892688] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid52461 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 17:00:59.963600] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 17:01:00.066703] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 17:01:00.066757] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 17:01:00.066823] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-13 17:01:00.066828] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 17:01:05.624243] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 17:01:06.487136] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 245760 MiB in 16 shards, 15360 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-malloc/lsvd-write//lsvd-write//d8558be2-5ae5-4625-8b59-280ea327e9ce.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 17:01:40.633711] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:49:30.lsvd-malloc.240.triple-hdd.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:49:30.lsvd-malloc.240.triple-hdd.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:49:30.lsvd-malloc.240.triple-hdd.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
+ dev_name=/dev/nvme1n1
Using device /dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.473188 s, 222 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: ac94220b-9ba3-465d-b303-8dbff2cbed0e
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: File-server Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.207: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.207: Removing bigfileset tree (if exists)
0.209: Pre-allocating directories in bigfileset tree
0.603: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 53/53 hits, 0 MiB read, 0.000 read amp, 53 total
[0m44.889: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
44.889: Population and pre-allocation of filesets completed
44.889: Starting 1 filereader instances
45.897: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 71/71 hits, 0 MiB read, 0.000 read amp, 71 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 121/121 hits, 0 MiB read, 0.000 read amp, 121 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 123/123 hits, 0 MiB read, 0.000 read amp, 123 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 125/125 hits, 0 MiB read, 0.000 read amp, 125 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 126/126 hits, 0 MiB read, 0.000 read amp, 126 total
[0m225.915: Run took 180 seconds...
225.920: Per-Operation Breakdown
statfile1            715204ops     3973ops/s   0.0mb/s    0.012ms/op [0.003ms - 9.187ms]
deletefile1          715204ops     3973ops/s   0.0mb/s    0.285ms/op [0.032ms - 544.045ms]
closefile3           715206ops     3973ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.070ms]
readfile1            715206ops     3973ops/s 519.4mb/s    0.114ms/op [0.005ms - 519.012ms]
openfile2            715206ops     3973ops/s   0.0mb/s    0.046ms/op [0.006ms - 34.267ms]
closefile2           715206ops     3973ops/s   0.0mb/s    0.004ms/op [0.001ms - 5.887ms]
appendfilerand1      715206ops     3973ops/s  31.0mb/s    0.805ms/op [0.001ms - 537.387ms]
openfile1            715212ops     3973ops/s   0.0mb/s    0.051ms/op [0.007ms - 33.226ms]
closefile1           715212ops     3973ops/s   0.0mb/s    0.006ms/op [0.001ms - 33.908ms]
wrtfile1             715212ops     3973ops/s 497.0mb/s   10.536ms/op [0.013ms - 281.577ms]
createfile1          715254ops     3973ops/s   0.0mb/s    0.153ms/op [0.025ms - 509.436ms]
225.920: IO Summary: 7867328 ops 43703.243 ops/s 3973/7946 rd/wr 1047.4mb/s 1.092ms/op
225.920: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:225.920: IO Summary: 7867328 ops 43703.243 ops/s 3973/7946 rd/wr 1047.4mb/s 1.092ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: OLTP Version 3.0  personality successfully loaded
0.003: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.006: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.116: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.116: Removing datafiles tree (if exists)
0.119: Pre-allocating directories in datafiles tree
0.119: Pre-allocating files in datafiles tree
57.126: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
57.126: Population and pre-allocation of filesets completed
57.126: Starting 200 shadow instances
57.244: Starting 10 dbwr instances
57.249: Starting 1 lgwr instances
58.253: Running...
238.303: Run took 180 seconds...
238.320: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     3952562ops    21953ops/s   0.0mb/s    7.511ms/op [0.019ms - 415.533ms]
shadow-post-lg       3952563ops    21953ops/s   0.0mb/s    0.077ms/op [0.002ms - 60.027ms]
shadowhog            3952586ops    21953ops/s   0.0mb/s    0.334ms/op [0.091ms - 94.087ms]
shadowread           3978261ops    22096ops/s  42.9mb/s    1.068ms/op [0.001ms - 265.326ms]
dbwr-aiowait         39515ops      219ops/s   0.0mb/s    2.047ms/op [0.004ms - 52.195ms]
dbwr-block           39518ops      219ops/s   0.0mb/s   21.299ms/op [0.003ms - 395.532ms]
dbwr-hog             39525ops      220ops/s   0.0mb/s    0.009ms/op [0.004ms - 6.231ms]
dbwrite-a            3953780ops    21960ops/s  42.9mb/s    0.003ms/op [0.000ms - 58.960ms]
lg-block             1235ops        7ops/s   0.0mb/s  145.699ms/op [33.998ms - 960.116ms]
lg-aiowait           1236ops        7ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.039ms]
lg-write             1237ops        7ops/s   1.7mb/s    0.009ms/op [0.001ms - 0.081ms]
238.320: IO Summary: 7974029 ops 44288.291 ops/s 22096/21966 rd/wr  87.5mb/s 0.545ms/op
238.320: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:238.320: IO Summary: 7974029 ops 44288.291 ops/s 22096/21966 rd/wr  87.5mb/s 0.545ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.002: Varmail Version 3.0 personality successfully loaded
0.002: Populating and pre-allocating filesets
0.700: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
0.700: Removing bigfileset tree (if exists)
0.703: Pre-allocating directories in bigfileset tree
0.703: Pre-allocating files in bigfileset tree
73.980: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
73.980: Population and pre-allocation of filesets completed
73.981: Starting 1 filereader instances
74.985: Running...
255.000: Run took 180 seconds...
255.001: Per-Operation Breakdown
closefile4           309283ops     1718ops/s   0.0mb/s    0.003ms/op [0.001ms - 0.417ms]
readfile4            309283ops     1718ops/s  45.5mb/s    0.768ms/op [0.005ms - 340.242ms]
openfile4            309283ops     1718ops/s   0.0mb/s    0.021ms/op [0.005ms - 3.458ms]
closefile3           309283ops     1718ops/s   0.0mb/s    0.006ms/op [0.001ms - 6.191ms]
fsyncfile3           309284ops     1718ops/s   0.0mb/s    2.852ms/op [0.594ms - 601.335ms]
appendfilerand3      309288ops     1718ops/s  13.4mb/s    0.075ms/op [0.007ms - 95.809ms]
readfile3            309288ops     1718ops/s  45.5mb/s    0.819ms/op [0.005ms - 309.871ms]
openfile3            309288ops     1718ops/s   0.0mb/s    0.021ms/op [0.005ms - 2.863ms]
closefile2           309288ops     1718ops/s   0.0mb/s    0.006ms/op [0.001ms - 9.444ms]
fsyncfile2           309291ops     1718ops/s   0.0mb/s    3.050ms/op [0.814ms - 602.615ms]
appendfilerand2      309299ops     1718ops/s  13.4mb/s    0.267ms/op [0.001ms - 599.604ms]
createfile2          309299ops     1718ops/s   0.0mb/s    0.650ms/op [0.031ms - 402.759ms]
deletefile1          309299ops     1718ops/s   0.0mb/s    0.639ms/op [0.041ms - 598.356ms]
255.001: IO Summary: 4020756 ops 22335.647 ops/s 3436/3436 rd/wr 117.8mb/s 0.706ms/op
255.001: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:255.001: IO Summary: 4020756 ops 22335.647 ops/s 3436/3436 rd/wr 117.8mb/s 0.706ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:49:30.lsvd-malloc.240.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T16:49:30.lsvd-malloc.240.triple-hdd.txt
Filebench /tmp/filebench/fileserver.f:225.920: IO Summary: 7867328 ops 43703.243 ops/s 3973/7946 rd/wr 1047.4mb/s 1.092ms/op
Filebench /tmp/filebench/oltp.f:238.320: IO Summary: 7974029 ops 44288.291 ops/s 22096/21966 rd/wr  87.5mb/s 0.545ms/op
Filebench /tmp/filebench/varmail.f:255.001: IO Summary: 4020756 ops 22335.647 ops/s 3436/3436 rd/wr 117.8mb/s 0.706ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7f277a7ec640) exiting
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-13T17:14:26
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:14:26.lsvd-nvme.240.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T17:14:26
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin rssd2 lsvd-benchmark 80g
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark
Removing all objects from pool rssd2 with prefix lsvd-benchmark
++++++++++++++++++++++++++++++++++++
Removed 10193/36038 objects
+ ./imgtool --create --rados --size=80g rssd2/lsvd-benchmark
+ rados -p rssd2 stat lsvd-benchmark
rssd2/lsvd-benchmark mtime 2024-01-13T17:15:33.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/lsvd-write/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/lsvd-write/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=257698037760
+ LSVD_CACHE_SIZE=257698037760
+ rm -rf '/mnt/nvme-remote/lsvd-write//lsvd-write/*'
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 17:15:57.250814] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 17:15:57.251001] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid75182 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 17:15:57.353976] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 17:15:57.497743] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 17:15:57.497811] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 17:15:57.497876] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-13 17:15:57.497878] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 17:16:03.023389] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 17:16:03.750080] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 245760 MiB in 16 shards, 15360 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote/lsvd-write//lsvd-write//7edbc87c-6e04-403d-b025-ce5c82960534.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 17:16:31.795992] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:14:26.lsvd-nvme.240.rssd2.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:14:26.lsvd-nvme.240.rssd2.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:14:26.lsvd-nvme.240.rssd2.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
+ dev_name=/dev/nvme1n1
Using device /dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.354208 s, 296 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 6c9e3066-ecbe-4cb8-94d7-b0e38b72a342
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.004: File-server Version 3.0 personality successfully loaded
0.005: Populating and pre-allocating filesets
0.337: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.337: Removing bigfileset tree (if exists)
0.340: Pre-allocating directories in bigfileset tree
0.745: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 53/53 hits, 0 MiB read, 0.000 read amp, 53 total
[0m35.892: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
35.892: Population and pre-allocation of filesets completed
35.892: Starting 1 filereader instances
36.900: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118/118 hits, 0 MiB read, 0.000 read amp, 118 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 119/119 hits, 0 MiB read, 0.000 read amp, 119 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 120/120 hits, 0 MiB read, 0.000 read amp, 120 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 124/124 hits, 0 MiB read, 0.000 read amp, 124 total
[0m216.917: Run took 180 seconds...
216.922: Per-Operation Breakdown
statfile1            1014666ops     5637ops/s   0.0mb/s    0.012ms/op [0.003ms - 7.878ms]
deletefile1          1014666ops     5637ops/s   0.0mb/s    0.181ms/op [0.030ms - 85.680ms]
closefile3           1014667ops     5637ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.070ms]
readfile1            1014669ops     5637ops/s 741.2mb/s    0.098ms/op [0.005ms - 57.881ms]
openfile2            1014669ops     5637ops/s   0.0mb/s    0.057ms/op [0.005ms - 68.148ms]
closefile2           1014669ops     5637ops/s   0.0mb/s    0.004ms/op [0.001ms - 6.348ms]
appendfilerand1      1014669ops     5637ops/s  44.1mb/s    0.531ms/op [0.001ms - 203.016ms]
openfile1            1014669ops     5637ops/s   0.0mb/s    0.062ms/op [0.007ms - 34.108ms]
closefile1           1014669ops     5637ops/s   0.0mb/s    0.006ms/op [0.001ms - 8.161ms]
wrtfile1             1014675ops     5637ops/s 705.1mb/s    7.116ms/op [0.013ms - 208.003ms]
createfile1          1014716ops     5637ops/s   0.0mb/s    0.127ms/op [0.025ms - 68.451ms]
216.922: IO Summary: 11161404 ops 62002.352 ops/s 5637/11273 rd/wr 1490.4mb/s 0.745ms/op
216.922: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:216.922: IO Summary: 11161404 ops 62002.352 ops/s 5637/11273 rd/wr 1490.4mb/s 0.745ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: OLTP Version 3.0  personality successfully loaded
0.003: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.006: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.134: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.134: Removing datafiles tree (if exists)
0.137: Pre-allocating directories in datafiles tree
0.138: Pre-allocating files in datafiles tree
38.272: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
38.272: Population and pre-allocation of filesets completed
38.272: Starting 200 shadow instances
38.400: Starting 10 dbwr instances
38.407: Starting 1 lgwr instances
39.416: Running...
219.483: Run took 180 seconds...
219.503: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     4509000ops    25041ops/s   0.0mb/s    6.556ms/op [0.019ms - 393.177ms]
shadow-post-lg       4509200ops    25042ops/s   0.0mb/s    0.069ms/op [0.002ms - 60.792ms]
shadowhog            4509200ops    25042ops/s   0.0mb/s    0.314ms/op [0.091ms - 103.519ms]
shadowread           4534800ops    25184ops/s  48.9mb/s    0.950ms/op [0.001ms - 118.982ms]
dbwr-aiowait         45082ops      250ops/s   0.0mb/s    2.117ms/op [0.005ms - 56.948ms]
dbwr-block           45085ops      250ops/s   0.0mb/s   17.807ms/op [0.003ms - 97.761ms]
dbwr-hog             45092ops      250ops/s   0.0mb/s    0.011ms/op [0.004ms - 17.053ms]
dbwrite-a            4510480ops    25049ops/s  48.9mb/s    0.003ms/op [0.000ms - 50.107ms]
lg-block             1409ops        8ops/s   0.0mb/s  127.717ms/op [35.282ms - 909.972ms]
lg-aiowait           1410ops        8ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.028ms]
lg-write             1411ops        8ops/s   2.0mb/s    0.009ms/op [0.001ms - 0.106ms]
219.503: IO Summary: 9093183 ops 50499.498 ops/s 25184/25057 rd/wr  99.8mb/s 0.486ms/op
219.503: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:219.503: IO Summary: 9093183 ops 50499.498 ops/s 25184/25057 rd/wr  99.8mb/s 0.486ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.004: Varmail Version 3.0 personality successfully loaded
0.004: Populating and pre-allocating filesets
1.056: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
1.056: Removing bigfileset tree (if exists)
1.059: Pre-allocating directories in bigfileset tree
1.059: Pre-allocating files in bigfileset tree
76.363: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
76.363: Population and pre-allocation of filesets completed
76.363: Starting 1 filereader instances
77.368: Running...
257.383: Run took 180 seconds...
257.384: Per-Operation Breakdown
closefile4           363605ops     2020ops/s   0.0mb/s    0.003ms/op [0.001ms - 8.783ms]
readfile4            363605ops     2020ops/s  52.4mb/s    0.571ms/op [0.004ms - 208.022ms]
openfile4            363605ops     2020ops/s   0.0mb/s    0.022ms/op [0.005ms - 12.658ms]
closefile3           363605ops     2020ops/s   0.0mb/s    0.006ms/op [0.001ms - 15.109ms]
fsyncfile3           363610ops     2020ops/s   0.0mb/s    2.480ms/op [0.105ms - 210.746ms]
appendfilerand3      363613ops     2020ops/s  15.8mb/s    0.101ms/op [0.007ms - 209.256ms]
readfile3            363613ops     2020ops/s  52.4mb/s    0.611ms/op [0.004ms - 209.831ms]
openfile3            363613ops     2020ops/s   0.0mb/s    0.022ms/op [0.005ms - 14.854ms]
closefile2           363613ops     2020ops/s   0.0mb/s    0.006ms/op [0.001ms - 14.309ms]
fsyncfile2           363616ops     2020ops/s   0.0mb/s    2.576ms/op [0.756ms - 210.794ms]
appendfilerand2      363621ops     2020ops/s  15.7mb/s    0.237ms/op [0.001ms - 209.158ms]
createfile2          363621ops     2020ops/s   0.0mb/s    0.499ms/op [0.030ms - 209.404ms]
deletefile1          363621ops     2020ops/s   0.0mb/s    0.588ms/op [0.043ms - 210.924ms]
257.384: IO Summary: 4726961 ops 26258.844 ops/s 4040/4040 rd/wr 136.3mb/s 0.594ms/op
257.384: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:257.384: IO Summary: 4726961 ops 26258.844 ops/s 4040/4040 rd/wr 136.3mb/s 0.594ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:14:26.lsvd-nvme.240.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:14:26.lsvd-nvme.240.rssd2.txt
Filebench /tmp/filebench/fileserver.f:216.922: IO Summary: 11161404 ops 62002.352 ops/s 5637/11273 rd/wr 1490.4mb/s 0.745ms/op
Filebench /tmp/filebench/oltp.f:219.503: IO Summary: 9093183 ops 50499.498 ops/s 25184/25057 rd/wr  99.8mb/s 0.486ms/op
Filebench /tmp/filebench/varmail.f:257.384: IO Summary: 4726961 ops 26258.844 ops/s 4040/4040 rd/wr 136.3mb/s 0.594ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7f8deffef640) exiting
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-13T17:28:51
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/lsvd-write/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:28:51.lsvd-nvme.240.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T17:28:51
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin triple-hdd lsvd-benchmark 80g
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark
Removing all objects from pool triple-hdd with prefix lsvd-benchmark
++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 10379/54793 objects
+ ./imgtool --create --rados --size=80g triple-hdd/lsvd-benchmark
+ rados -p triple-hdd stat lsvd-benchmark
triple-hdd/lsvd-benchmark mtime 2024-01-13T17:33:46.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/lsvd-write/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/lsvd-write/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=257698037760
+ LSVD_CACHE_SIZE=257698037760
+ rm -rf /mnt/nvme-remote/lsvd-write//lsvd-write/7edbc87c-6e04-403d-b025-ce5c82960534.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 17:34:08.241572] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 17:34:08.241694] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid97161 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 17:34:08.321900] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 17:34:08.458334] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 17:34:08.458403] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 17:34:08.458552] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
[2024-01-13 17:34:08.458547] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 17:34:13.923968] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 17:34:14.550427] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 245760 MiB in 16 shards, 15360 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote/lsvd-write//lsvd-write//256fe444-8f93-4e00-a523-1927827ab138.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 17:34:43.206835] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:28:51.lsvd-nvme.240.triple-hdd.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:28:51.lsvd-nvme.240.triple-hdd.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:28:51.lsvd-nvme.240.triple-hdd.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
+ printf '===Starting client benchmark\n\n'
===Starting client benchmark

+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.481255 s, 218 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 9f5ac485-7b40-463f-95f6-5366127aeccf
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: File-server Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.208: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.208: Removing bigfileset tree (if exists)
0.210: Pre-allocating directories in bigfileset tree
0.609: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 53/53 hits, 0 MiB read, 0.000 read amp, 53 total
[0m44.676: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
44.676: Population and pre-allocation of filesets completed
44.676: Starting 1 filereader instances
45.685: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 92/92 hits, 0 MiB read, 0.000 read amp, 92 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 95/95 hits, 0 MiB read, 0.000 read amp, 95 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 98/98 hits, 0 MiB read, 0.000 read amp, 98 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 100/100 hits, 0 MiB read, 0.000 read amp, 100 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 102/105 hits, 0 MiB read, 0.390 read amp, 105 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 103/106 hits, 0 MiB read, 0.387 read amp, 106 total
[0m225.703: Run took 180 seconds...
225.709: Per-Operation Breakdown
statfile1            692118ops     3845ops/s   0.0mb/s    0.012ms/op [0.004ms - 15.993ms]
deletefile1          692115ops     3845ops/s   0.0mb/s    0.258ms/op [0.030ms - 296.271ms]
closefile3           692120ops     3845ops/s   0.0mb/s    0.004ms/op [0.001ms - 11.959ms]
readfile1            692120ops     3845ops/s 502.2mb/s    0.112ms/op [0.004ms - 275.675ms]
openfile2            692120ops     3845ops/s   0.0mb/s    0.053ms/op [0.005ms - 234.815ms]
closefile2           692120ops     3845ops/s   0.0mb/s    0.004ms/op [0.001ms - 9.907ms]
appendfilerand1      692120ops     3845ops/s  30.0mb/s    0.820ms/op [0.001ms - 307.541ms]
openfile1            692120ops     3845ops/s   0.0mb/s    0.059ms/op [0.007ms - 234.951ms]
closefile1           692121ops     3845ops/s   0.0mb/s    0.007ms/op [0.001ms - 9.808ms]
wrtfile1             692125ops     3845ops/s 481.0mb/s   10.746ms/op [0.013ms - 321.385ms]
createfile1          692168ops     3845ops/s   0.0mb/s    0.157ms/op [0.024ms - 234.718ms]
225.709: IO Summary: 7613367 ops 42292.630 ops/s 3845/7690 rd/wr 1013.3mb/s 1.112ms/op
225.709: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:225.709: IO Summary: 7613367 ops 42292.630 ops/s 3845/7690 rd/wr 1013.3mb/s 1.112ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: OLTP Version 3.0  personality successfully loaded
0.003: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.006: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.160: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.161: Removing datafiles tree (if exists)
0.163: Pre-allocating directories in datafiles tree
0.163: Pre-allocating files in datafiles tree
58.251: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
58.251: Population and pre-allocation of filesets completed
58.252: Starting 200 shadow instances
58.365: Starting 10 dbwr instances
58.370: Starting 1 lgwr instances
59.383: Running...
239.427: Run took 180 seconds...
239.447: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     4176000ops    23195ops/s   0.0mb/s    7.361ms/op [0.019ms - 507.351ms]
shadow-post-lg       4176195ops    23196ops/s   0.0mb/s    0.057ms/op [0.002ms - 77.353ms]
shadowhog            4176197ops    23196ops/s   0.0mb/s    0.288ms/op [0.091ms - 103.055ms]
shadowread           4201800ops    23338ops/s  45.3mb/s    0.830ms/op [0.001ms - 220.914ms]
dbwr-aiowait         41751ops      232ops/s   0.0mb/s    2.113ms/op [0.004ms - 54.195ms]
dbwr-block           41755ops      232ops/s   0.0mb/s   16.380ms/op [0.003ms - 238.688ms]
dbwr-hog             41761ops      232ops/s   0.0mb/s    0.010ms/op [0.004ms - 12.683ms]
dbwrite-a            4177380ops    23202ops/s  45.3mb/s    0.004ms/op [0.001ms - 61.486ms]
lg-block             1305ops        7ops/s   0.0mb/s  137.870ms/op [33.520ms - 1182.430ms]
lg-aiowait           1306ops        7ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.027ms]
lg-write             1307ops        7ops/s   1.8mb/s    0.009ms/op [0.001ms - 0.098ms]
239.447: IO Summary: 8423544 ops 46786.679 ops/s 23338/23210 rd/wr  92.4mb/s 0.426ms/op
239.447: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:239.447: IO Summary: 8423544 ops 46786.679 ops/s 23338/23210 rd/wr  92.4mb/s 0.426ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: Varmail Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.975: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
0.975: Removing bigfileset tree (if exists)
0.977: Pre-allocating directories in bigfileset tree
0.977: Pre-allocating files in bigfileset tree
81.486: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
81.486: Population and pre-allocation of filesets completed
81.486: Starting 1 filereader instances
82.490: Running...
262.505: Run took 180 seconds...
262.507: Per-Operation Breakdown
closefile4           318704ops     1770ops/s   0.0mb/s    0.003ms/op [0.001ms - 0.481ms]
readfile4            318704ops     1770ops/s  46.7mb/s    0.706ms/op [0.005ms - 259.203ms]
openfile4            318704ops     1770ops/s   0.0mb/s    0.022ms/op [0.005ms - 4.902ms]
closefile3           318704ops     1770ops/s   0.0mb/s    0.006ms/op [0.001ms - 5.498ms]
fsyncfile3           318704ops     1770ops/s   0.0mb/s    2.780ms/op [0.229ms - 491.932ms]
appendfilerand3      318705ops     1770ops/s  13.8mb/s    0.087ms/op [0.001ms - 211.750ms]
readfile3            318705ops     1770ops/s  46.6mb/s    0.751ms/op [0.005ms - 297.595ms]
openfile3            318705ops     1770ops/s   0.0mb/s    0.022ms/op [0.005ms - 3.804ms]
closefile2           318705ops     1770ops/s   0.0mb/s    0.006ms/op [0.001ms - 3.129ms]
fsyncfile2           318705ops     1770ops/s   0.0mb/s    2.901ms/op [0.814ms - 491.883ms]
appendfilerand2      318711ops     1770ops/s  13.8mb/s    0.272ms/op [0.001ms - 208.355ms]
createfile2          318711ops     1770ops/s   0.0mb/s    0.671ms/op [0.031ms - 491.165ms]
deletefile1          318717ops     1771ops/s   0.0mb/s    0.672ms/op [0.043ms - 489.932ms]
262.507: IO Summary: 4143184 ops 23015.823 ops/s 3541/3541 rd/wr 121.0mb/s 0.685ms/op
262.507: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:262.507: IO Summary: 4143184 ops 23015.823 ops/s 3541/3541 rd/wr 121.0mb/s 0.685ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:28:51.lsvd-nvme.240.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:28:51.lsvd-nvme.240.triple-hdd.txt
Filebench /tmp/filebench/fileserver.f:225.709: IO Summary: 7613367 ops 42292.630 ops/s 3845/7690 rd/wr 1013.3mb/s 1.112ms/op
Filebench /tmp/filebench/oltp.f:239.447: IO Summary: 8423544 ops 46786.679 ops/s 23338/23210 rd/wr  92.4mb/s 0.426ms/op
Filebench /tmp/filebench/varmail.f:262.507: IO Summary: 4143184 ops 23015.823 ops/s 3541/3541 rd/wr 121.0mb/s 0.685ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7fab88fe9640) exiting
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=std
++ date +%FT%T
+ cur_time=2024-01-13T17:47:41
+ default_cache_size=21474836480
+ cache_size=21474836480
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/lsvd-write/
+ cache_size_gb=20
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:47:41.lsvd-std.20.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC io.cc
CC translate.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T17:47:41
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin rssd2 lsvd-benchmark 80g
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark
Removing all objects from pool rssd2 with prefix lsvd-benchmark
++++++++++++++++++++++++++++++++++++
Removed 10607/36452 objects
+ ./imgtool --create --rados --size=80g rssd2/lsvd-benchmark
+ rados -p rssd2 stat lsvd-benchmark
rssd2/lsvd-benchmark mtime 2024-01-13T17:48:48.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/lsvd-write/ 21474836480
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/lsvd-write/
+ local cache_size=21474836480
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=21474836480
+ LSVD_CACHE_SIZE=21474836480
+ rm -rf /mnt/nvme-remote/lsvd-write//lsvd-write/256fe444-8f93-4e00-a523-1927827ab138.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 17:49:09.511754] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 17:49:09.511873] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid117256 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 17:49:09.582004] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 17:49:09.724612] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 17:49:09.724714] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 17:49:09.724840] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-13 17:49:09.724847] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 17:49:15.232130] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 17:49:15.986853] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 20480 MiB in 16 shards, 1280 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote/lsvd-write//lsvd-write//255f61e8-783d-47f4-9298-5a391e39c77f.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 17:49:44.858649] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:47:41.lsvd-std.20.rssd2.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:47:41.lsvd-std.20.rssd2.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:47:41.lsvd-std.20.rssd2.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.348454 s, 301 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: f7328bbe-6225-4c7a-a370-c571416e6a1d
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.004: File-server Version 3.0 personality successfully loaded
0.004: Populating and pre-allocating filesets
0.263: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.264: Removing bigfileset tree (if exists)
0.266: Pre-allocating directories in bigfileset tree
0.663: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 53/53 hits, 0 MiB read, 0.000 read amp, 53 total
[0m36.562: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
36.562: Population and pre-allocation of filesets completed
36.562: Starting 1 filereader instances
37.570: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 112/112 hits, 0 MiB read, 0.000 read amp, 112 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 112/114 hits, 0 MiB read, 0.242 read amp, 114 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 115/121 hits, 0 MiB read, 0.691 read amp, 121 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 115/122 hits, 0 MiB read, 0.800 read amp, 122 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 116/124 hits, 0 MiB read, 0.901 read amp, 124 total
[0m217.587: Run took 180 seconds...
217.592: Per-Operation Breakdown
statfile1            1252562ops     6958ops/s   0.0mb/s    0.012ms/op [0.004ms - 5.729ms]
deletefile1          1252561ops     6958ops/s   0.0mb/s    0.211ms/op [0.029ms - 73.445ms]
closefile3           1252564ops     6958ops/s   0.0mb/s    0.004ms/op [0.001ms - 34.603ms]
readfile1            1252565ops     6958ops/s 917.5mb/s    0.097ms/op [0.004ms - 58.082ms]
openfile2            1252565ops     6958ops/s   0.0mb/s    0.056ms/op [0.005ms - 38.064ms]
closefile2           1252565ops     6958ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.090ms]
appendfilerand1      1252566ops     6958ops/s  54.4mb/s    0.419ms/op [0.001ms - 205.448ms]
openfile1            1252569ops     6958ops/s   0.0mb/s    0.060ms/op [0.007ms - 34.834ms]
closefile1           1252569ops     6958ops/s   0.0mb/s    0.006ms/op [0.001ms - 5.780ms]
wrtfile1             1252573ops     6958ops/s 870.6mb/s    5.541ms/op [0.013ms - 208.943ms]
createfile1          1252612ops     6958ops/s   0.0mb/s    0.124ms/op [0.024ms - 54.717ms]
217.592: IO Summary: 13778271 ops 76539.422 ops/s 6958/13916 rd/wr 1842.5mb/s 0.594ms/op
217.592: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:217.592: IO Summary: 13778271 ops 76539.422 ops/s 6958/13916 rd/wr 1842.5mb/s 0.594ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.002: OLTP Version 3.0  personality successfully loaded
0.002: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.005: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.176: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.176: Removing datafiles tree (if exists)
0.179: Pre-allocating directories in datafiles tree
0.179: Pre-allocating files in datafiles tree
37.519: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
37.519: Population and pre-allocation of filesets completed
37.519: Starting 200 shadow instances
37.648: Starting 10 dbwr instances
37.656: Starting 1 lgwr instances
38.659: Running...
218.706: Run took 180 seconds...
218.724: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     4904000ops    27238ops/s   0.0mb/s    5.873ms/op [0.019ms - 347.249ms]
shadow-post-lg       4904200ops    27239ops/s   0.0mb/s    0.071ms/op [0.002ms - 64.044ms]
shadowhog            4904200ops    27239ops/s   0.0mb/s    0.317ms/op [0.091ms - 118.902ms]
shadowread           4929800ops    27381ops/s  53.2mb/s    0.978ms/op [0.001ms - 140.755ms]
dbwr-aiowait         49032ops      272ops/s   0.0mb/s    2.278ms/op [0.005ms - 53.356ms]
dbwr-block           49034ops      272ops/s   0.0mb/s   17.935ms/op [0.003ms - 62.285ms]
dbwr-hog             49042ops      272ops/s   0.0mb/s    0.011ms/op [0.004ms - 21.224ms]
dbwrite-a            4905480ops    27246ops/s  53.2mb/s    0.004ms/op [0.000ms - 57.619ms]
lg-block             1532ops        9ops/s   0.0mb/s  117.422ms/op [30.151ms - 712.998ms]
lg-aiowait           1533ops        9ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.024ms]
lg-write             1534ops        9ops/s   2.1mb/s    0.009ms/op [0.001ms - 0.113ms]
218.724: IO Summary: 9887379 ops 54916.391 ops/s 27381/27254 rd/wr 108.5mb/s 0.501ms/op
218.724: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:218.724: IO Summary: 9887379 ops 54916.391 ops/s 27381/27254 rd/wr 108.5mb/s 0.501ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: Varmail Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.951: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
0.951: Removing bigfileset tree (if exists)
0.954: Pre-allocating directories in bigfileset tree
0.954: Pre-allocating files in bigfileset tree
76.960: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
76.960: Population and pre-allocation of filesets completed
76.960: Starting 1 filereader instances
77.964: Running...
257.980: Run took 180 seconds...
257.982: Per-Operation Breakdown
closefile4           359805ops     1999ops/s   0.0mb/s    0.003ms/op [0.001ms - 1.172ms]
readfile4            359805ops     1999ops/s  51.8mb/s    0.598ms/op [0.005ms - 68.095ms]
openfile4            359806ops     1999ops/s   0.0mb/s    0.022ms/op [0.005ms - 2.457ms]
closefile3           359806ops     1999ops/s   0.0mb/s    0.006ms/op [0.001ms - 4.736ms]
fsyncfile3           359807ops     1999ops/s   0.0mb/s    2.493ms/op [0.566ms - 211.269ms]
appendfilerand3      359809ops     1999ops/s  15.6mb/s    0.098ms/op [0.007ms - 11.781ms]
readfile3            359809ops     1999ops/s  51.9mb/s    0.644ms/op [0.005ms - 208.172ms]
openfile3            359809ops     1999ops/s   0.0mb/s    0.022ms/op [0.005ms - 6.112ms]
closefile2           359809ops     1999ops/s   0.0mb/s    0.006ms/op [0.001ms - 2.684ms]
fsyncfile2           359809ops     1999ops/s   0.0mb/s    2.601ms/op [0.781ms - 210.950ms]
appendfilerand2      359814ops     1999ops/s  15.6mb/s    0.238ms/op [0.014ms - 36.969ms]
createfile2          359817ops     1999ops/s   0.0mb/s    0.543ms/op [0.032ms - 207.453ms]
deletefile1          359818ops     1999ops/s   0.0mb/s    0.592ms/op [0.032ms - 207.437ms]
257.982: IO Summary: 4677522 ops 25983.964 ops/s 3998/3998 rd/wr 135.0mb/s 0.605ms/op
257.982: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:257.982: IO Summary: 4677522 ops 25983.964 ops/s 3998/3998 rd/wr 135.0mb/s 0.605ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:47:41.lsvd-std.20.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T17:47:41.lsvd-std.20.rssd2.txt
Filebench /tmp/filebench/fileserver.f:217.592: IO Summary: 13778271 ops 76539.422 ops/s 6958/13916 rd/wr 1842.5mb/s 0.594ms/op
Filebench /tmp/filebench/oltp.f:218.724: IO Summary: 9887379 ops 54916.391 ops/s 27381/27254 rd/wr 108.5mb/s 0.501ms/op
Filebench /tmp/filebench/varmail.f:257.982: IO Summary: 4677522 ops 25983.964 ops/s 3998/3998 rd/wr 135.0mb/s 0.605ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7f8b6dfeb640) exiting
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=std
++ date +%FT%T
+ cur_time=2024-01-13T18:02:05
+ default_cache_size=21474836480
+ cache_size=21474836480
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/lsvd-write/
+ cache_size_gb=20
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:02:05.lsvd-std.20.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T18:02:05
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin triple-hdd lsvd-benchmark 80g
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark
Removing all objects from pool triple-hdd with prefix lsvd-benchmark
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 10923/55337 objects
+ ./imgtool --create --rados --size=80g triple-hdd/lsvd-benchmark
+ rados -p triple-hdd stat lsvd-benchmark
triple-hdd/lsvd-benchmark mtime 2024-01-13T18:07:03.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/lsvd-write/ 21474836480
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/lsvd-write/
+ local cache_size=21474836480
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=21474836480
+ LSVD_CACHE_SIZE=21474836480
+ rm -rf /mnt/nvme-remote/lsvd-write//lsvd-write/255f61e8-783d-47f4-9298-5a391e39c77f.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 18:07:24.612023] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 18:07:24.612148] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid143928 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 18:07:24.695857] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 18:07:24.845921] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 18:07:24.846016] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 18:07:24.846109] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-13 18:07:24.846115] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 18:07:30.412120] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 18:07:31.050563] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 20480 MiB in 16 shards, 1280 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote/lsvd-write//lsvd-write//49f18f47-f8b3-46ea-bf4c-1977985b3c6f.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 18:07:36.556280] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:02:05.lsvd-std.20.triple-hdd.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:02:05.lsvd-std.20.triple-hdd.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:02:05.lsvd-std.20.triple-hdd.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.450625 s, 233 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 3ee0f3d0-8b7f-42f8-91f8-99c673fdd207
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: File-server Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.207: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.207: Removing bigfileset tree (if exists)
0.210: Pre-allocating directories in bigfileset tree
0.613: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 54/54 hits, 0 MiB read, 0.000 read amp, 54 total
[0m46.677: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
46.677: Population and pre-allocation of filesets completed
46.678: Starting 1 filereader instances
47.685: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 61/61 hits, 0 MiB read, 0.000 read amp, 61 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 95/95 hits, 0 MiB read, 0.000 read amp, 95 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 97/97 hits, 0 MiB read, 0.000 read amp, 97 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 98/101 hits, 0 MiB read, 0.403 read amp, 101 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 98/102 hits, 0 MiB read, 0.533 read amp, 102 total
[0m227.703: Run took 180 seconds...
227.707: Per-Operation Breakdown
statfile1            686161ops     3812ops/s   0.0mb/s    0.012ms/op [0.003ms - 15.053ms]
deletefile1          686155ops     3812ops/s   0.0mb/s    0.278ms/op [0.031ms - 273.565ms]
closefile3           686161ops     3812ops/s   0.0mb/s    0.005ms/op [0.001ms - 14.632ms]
readfile1            686161ops     3812ops/s 498.2mb/s    0.118ms/op [0.005ms - 290.514ms]
openfile2            686161ops     3812ops/s   0.0mb/s    0.075ms/op [0.006ms - 52.413ms]
closefile2           686161ops     3812ops/s   0.0mb/s    0.005ms/op [0.001ms - 16.522ms]
appendfilerand1      686161ops     3812ops/s  29.8mb/s    0.788ms/op [0.001ms - 317.429ms]
openfile1            686166ops     3812ops/s   0.0mb/s    0.083ms/op [0.007ms - 50.165ms]
closefile1           686166ops     3812ops/s   0.0mb/s    0.007ms/op [0.001ms - 15.384ms]
wrtfile1             686168ops     3812ops/s 476.9mb/s   10.310ms/op [0.013ms - 370.452ms]
createfile1          686210ops     3812ops/s   0.0mb/s    0.182ms/op [0.024ms - 124.801ms]
227.707: IO Summary: 7547831 ops 41928.523 ops/s 3812/7623 rd/wr 1004.9mb/s 1.078ms/op
227.707: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:227.707: IO Summary: 7547831 ops 41928.523 ops/s 3812/7623 rd/wr 1004.9mb/s 1.078ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: OLTP Version 3.0  personality successfully loaded
0.003: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.006: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.125: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.125: Removing datafiles tree (if exists)
0.128: Pre-allocating directories in datafiles tree
0.128: Pre-allocating files in datafiles tree
57.861: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
57.861: Population and pre-allocation of filesets completed
57.862: Starting 200 shadow instances
57.992: Starting 10 dbwr instances
57.997: Starting 1 lgwr instances
59.022: Running...
239.072: Run took 180 seconds...
239.091: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     4139000ops    22988ops/s   0.0mb/s    7.214ms/op [0.019ms - 626.605ms]
shadow-post-lg       4139194ops    22989ops/s   0.0mb/s    0.069ms/op [0.002ms - 59.983ms]
shadowhog            4139194ops    22989ops/s   0.0mb/s    0.310ms/op [0.093ms - 140.182ms]
shadowread           4164798ops    23132ops/s  44.9mb/s    1.016ms/op [0.001ms - 205.832ms]
dbwr-aiowait         41378ops      230ops/s   0.0mb/s    2.236ms/op [0.004ms - 57.557ms]
dbwr-block           41382ops      230ops/s   0.0mb/s   18.819ms/op [0.003ms - 613.825ms]
dbwr-hog             41388ops      230ops/s   0.0mb/s    0.011ms/op [0.004ms - 16.470ms]
dbwrite-a            4140080ops    22994ops/s  44.9mb/s    0.004ms/op [0.000ms - 57.956ms]
lg-block             1293ops        7ops/s   0.0mb/s  139.143ms/op [26.331ms - 1072.667ms]
lg-aiowait           1294ops        7ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.026ms]
lg-write             1295ops        7ops/s   1.8mb/s    0.009ms/op [0.001ms - 0.125ms]
239.091: IO Summary: 8348845 ops 46370.164 ops/s 23132/23002 rd/wr  91.6mb/s 0.520ms/op
239.091: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:239.091: IO Summary: 8348845 ops 46370.164 ops/s 23132/23002 rd/wr  91.6mb/s 0.520ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: Varmail Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.952: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
0.952: Removing bigfileset tree (if exists)
0.954: Pre-allocating directories in bigfileset tree
0.954: Pre-allocating files in bigfileset tree
79.438: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
79.438: Population and pre-allocation of filesets completed
79.438: Starting 1 filereader instances
80.442: Running...
260.459: Run took 180 seconds...
260.460: Per-Operation Breakdown
closefile4           321434ops     1786ops/s   0.0mb/s    0.003ms/op [0.001ms - 1.361ms]
readfile4            321434ops     1786ops/s  47.1mb/s    0.693ms/op [0.005ms - 497.372ms]
openfile4            321434ops     1786ops/s   0.0mb/s    0.023ms/op [0.005ms - 4.813ms]
closefile3           321434ops     1786ops/s   0.0mb/s    0.006ms/op [0.001ms - 2.909ms]
fsyncfile3           321443ops     1786ops/s   0.0mb/s    2.769ms/op [0.142ms - 562.720ms]
appendfilerand3      321446ops     1786ops/s  13.9mb/s    0.084ms/op [0.001ms - 209.342ms]
readfile3            321446ops     1786ops/s  47.0mb/s    0.743ms/op [0.005ms - 371.145ms]
openfile3            321446ops     1786ops/s   0.0mb/s    0.022ms/op [0.005ms - 6.563ms]
closefile2           321446ops     1786ops/s   0.0mb/s    0.006ms/op [0.001ms - 1.458ms]
fsyncfile2           321448ops     1786ops/s   0.0mb/s    2.902ms/op [0.775ms - 503.345ms]
appendfilerand2      321450ops     1786ops/s  14.0mb/s    0.267ms/op [0.015ms - 326.927ms]
createfile2          321450ops     1786ops/s   0.0mb/s    0.630ms/op [0.032ms - 327.155ms]
deletefile1          321450ops     1786ops/s   0.0mb/s    0.674ms/op [0.044ms - 497.768ms]
260.460: IO Summary: 4178761 ops 23213.254 ops/s 3571/3571 rd/wr 122.0mb/s 0.679ms/op
260.460: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:260.460: IO Summary: 4178761 ops 23213.254 ops/s 3571/3571 rd/wr 122.0mb/s 0.679ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:02:05.lsvd-std.20.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:02:05.lsvd-std.20.triple-hdd.txt
Filebench /tmp/filebench/fileserver.f:227.707: IO Summary: 7547831 ops 41928.523 ops/s 3812/7623 rd/wr 1004.9mb/s 1.078ms/op
Filebench /tmp/filebench/oltp.f:239.091: IO Summary: 8348845 ops 46370.164 ops/s 23132/23002 rd/wr  91.6mb/s 0.520ms/op
Filebench /tmp/filebench/varmail.f:260.460: IO Summary: 4178761 ops 23213.254 ops/s 3571/3571 rd/wr 122.0mb/s 0.679ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7f1cdf7fe640) exiting
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=std
++ date +%FT%T
+ cur_time=2024-01-13T18:20:31
+ default_cache_size=21474836480
+ cache_size=21474836480
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-malloc/lsvd-write/
+ cache_size_gb=20
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:20:31.lsvd-std.20.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T18:20:31
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin rssd2 lsvd-benchmark 80g
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark
Removing all objects from pool rssd2 with prefix lsvd-benchmark
++++++++++++++++++++++++++++++++++++
Removed 10212/36057 objects
+ ./imgtool --create --rados --size=80g rssd2/lsvd-benchmark
+ rados -p rssd2 stat lsvd-benchmark
rssd2/lsvd-benchmark mtime 2024-01-13T18:21:37.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-malloc/lsvd-write/ 21474836480
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-malloc/lsvd-write/
+ local cache_size=21474836480
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=21474836480
+ LSVD_CACHE_SIZE=21474836480
+ rm -rf /mnt/nvme-malloc/lsvd-write//lsvd-write/d8558be2-5ae5-4625-8b59-280ea327e9ce.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 18:21:40.659808] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 18:21:40.659923] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid165172 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 18:21:40.728731] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 18:21:40.831065] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 18:21:40.831135] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 18:21:40.831191] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
[2024-01-13 18:21:40.831188] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 18:21:46.392119] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 18:21:47.227037] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 20480 MiB in 16 shards, 1280 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-malloc/lsvd-write//lsvd-write//2f2f640d-dafb-41c5-9460-c49188eba9dd.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 18:21:54.185148] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:20:31.lsvd-std.20.rssd2.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:20:31.lsvd-std.20.rssd2.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:20:31.lsvd-std.20.rssd2.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.356046 s, 295 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: aa2e5588-63a8-49ea-9ba7-8ebb500ed4fa
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.004: File-server Version 3.0 personality successfully loaded
0.004: Populating and pre-allocating filesets
0.276: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.276: Removing bigfileset tree (if exists)
0.279: Pre-allocating directories in bigfileset tree
0.676: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52/52 hits, 0 MiB read, 0.000 read amp, 52 total
[0m36.773: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
36.773: Population and pre-allocation of filesets completed
36.774: Starting 1 filereader instances
37.782: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 101/101 hits, 0 MiB read, 0.000 read amp, 101 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 105/105 hits, 0 MiB read, 0.000 read amp, 105 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 106/108 hits, 0 MiB read, 0.254 read amp, 108 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 106/110 hits, 0 MiB read, 0.500 read amp, 110 total
[0m217.798: Run took 180 seconds...
217.802: Per-Operation Breakdown
statfile1            1228062ops     6822ops/s   0.0mb/s    0.012ms/op [0.003ms - 2.190ms]
deletefile1          1228063ops     6822ops/s   0.0mb/s    0.204ms/op [0.030ms - 73.019ms]
closefile3           1228063ops     6822ops/s   0.0mb/s    0.005ms/op [0.001ms - 229.263ms]
readfile1            1228063ops     6822ops/s 897.9mb/s    0.103ms/op [0.004ms - 66.854ms]
openfile2            1228063ops     6822ops/s   0.0mb/s    0.077ms/op [0.006ms - 341.948ms]
closefile2           1228063ops     6822ops/s   0.0mb/s    0.004ms/op [0.001ms - 9.664ms]
appendfilerand1      1228063ops     6822ops/s  53.3mb/s    0.390ms/op [0.001ms - 202.746ms]
openfile1            1228068ops     6822ops/s   0.0mb/s    0.082ms/op [0.007ms - 363.828ms]
closefile1           1228068ops     6822ops/s   0.0mb/s    0.006ms/op [0.001ms - 312.572ms]
wrtfile1             1228073ops     6822ops/s 854.2mb/s    5.184ms/op [0.013ms - 211.200ms]
createfile1          1228112ops     6822ops/s   0.0mb/s    0.149ms/op [0.023ms - 612.468ms]
217.802: IO Summary: 13508761 ops 75042.321 ops/s 6822/13644 rd/wr 1805.5mb/s 0.565ms/op
217.802: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:217.802: IO Summary: 13508761 ops 75042.321 ops/s 6822/13644 rd/wr 1805.5mb/s 0.565ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: OLTP Version 3.0  personality successfully loaded
0.003: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.006: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.163: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.163: Removing datafiles tree (if exists)
0.166: Pre-allocating directories in datafiles tree
0.166: Pre-allocating files in datafiles tree
37.135: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
37.135: Population and pre-allocation of filesets completed
37.135: Starting 200 shadow instances
37.260: Starting 10 dbwr instances
37.266: Starting 1 lgwr instances
38.269: Running...
218.317: Run took 180 seconds...
218.336: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     4716000ops    26193ops/s   0.0mb/s    6.065ms/op [0.019ms - 268.925ms]
shadow-post-lg       4716084ops    26194ops/s   0.0mb/s    0.076ms/op [0.002ms - 80.504ms]
shadowhog            4716086ops    26194ops/s   0.0mb/s    0.330ms/op [0.091ms - 149.024ms]
shadowread           4741790ops    26337ops/s  51.2mb/s    1.041ms/op [0.001ms - 128.595ms]
dbwr-aiowait         47154ops      262ops/s   0.0mb/s    2.127ms/op [0.005ms - 61.348ms]
dbwr-block           47156ops      262ops/s   0.0mb/s   20.736ms/op [0.003ms - 66.365ms]
dbwr-hog             47164ops      262ops/s   0.0mb/s    0.011ms/op [0.004ms - 18.328ms]
dbwrite-a            4717680ops    26203ops/s  51.2mb/s    0.003ms/op [0.000ms - 81.482ms]
lg-block             1473ops        8ops/s   0.0mb/s  122.110ms/op [35.132ms - 702.791ms]
lg-aiowait           1474ops        8ops/s   0.0mb/s    0.001ms/op [0.000ms - 0.022ms]
lg-write             1475ops        8ops/s   2.0mb/s    0.009ms/op [0.001ms - 0.108ms]
218.336: IO Summary: 9509573 ops 52817.424 ops/s 26337/26211 rd/wr 104.4mb/s 0.531ms/op
218.336: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:218.336: IO Summary: 9509573 ops 52817.424 ops/s 26337/26211 rd/wr 104.4mb/s 0.531ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: Varmail Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.958: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
0.958: Removing bigfileset tree (if exists)
0.961: Pre-allocating directories in bigfileset tree
0.961: Pre-allocating files in bigfileset tree
79.987: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
79.987: Population and pre-allocation of filesets completed
79.987: Starting 1 filereader instances
80.992: Running...
261.008: Run took 180 seconds...
261.009: Per-Operation Breakdown
closefile4           365157ops     2028ops/s   0.0mb/s    0.003ms/op [0.001ms - 0.474ms]
readfile4            365157ops     2028ops/s  52.5mb/s    0.622ms/op [0.005ms - 217.940ms]
openfile4            365157ops     2028ops/s   0.0mb/s    0.022ms/op [0.005ms - 3.918ms]
closefile3           365157ops     2028ops/s   0.0mb/s    0.006ms/op [0.001ms - 1.706ms]
fsyncfile3           365158ops     2028ops/s   0.0mb/s    2.352ms/op [0.229ms - 212.513ms]
appendfilerand3      365162ops     2029ops/s  15.8mb/s    0.105ms/op [0.007ms - 21.580ms]
readfile3            365162ops     2029ops/s  52.6mb/s    0.660ms/op [0.005ms - 209.452ms]
openfile3            365163ops     2029ops/s   0.0mb/s    0.022ms/op [0.005ms - 1.720ms]
closefile2           365163ops     2029ops/s   0.0mb/s    0.006ms/op [0.001ms - 3.587ms]
fsyncfile2           365166ops     2029ops/s   0.0mb/s    2.462ms/op [0.732ms - 212.499ms]
appendfilerand2      365172ops     2029ops/s  15.9mb/s    0.258ms/op [0.001ms - 215.309ms]
createfile2          365173ops     2029ops/s   0.0mb/s    0.597ms/op [0.032ms - 215.537ms]
deletefile1          365173ops     2029ops/s   0.0mb/s    0.634ms/op [0.043ms - 218.804ms]
261.009: IO Summary: 4747120 ops 26370.580 ops/s 4057/4057 rd/wr 136.8mb/s 0.596ms/op
261.009: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:261.009: IO Summary: 4747120 ops 26370.580 ops/s 4057/4057 rd/wr 136.8mb/s 0.596ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:20:31.lsvd-std.20.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:20:31.lsvd-std.20.rssd2.txt
Filebench /tmp/filebench/fileserver.f:217.802: IO Summary: 13508761 ops 75042.321 ops/s 6822/13644 rd/wr 1805.5mb/s 0.565ms/op
Filebench /tmp/filebench/oltp.f:218.336: IO Summary: 9509573 ops 52817.424 ops/s 26337/26211 rd/wr 104.4mb/s 0.531ms/op
Filebench /tmp/filebench/varmail.f:261.009: IO Summary: 4747120 ops 26370.580 ops/s 4057/4057 rd/wr 136.8mb/s 0.596ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7fc1667f4640) exiting
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=std
++ date +%FT%T
+ cur_time=2024-01-13T18:34:18
+ default_cache_size=21474836480
+ cache_size=21474836480
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-malloc/lsvd-write/
+ cache_size_gb=20
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:34:18.lsvd-std.20.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
make[1]: Entering directory '/home/isaackhor/code/lsvd-rbd/atc2024'
latexmk -C
Rc files read:
  /etc/LatexMk
  latexmkrc
Latexmk: This is Latexmk, John Collins, 20 November 2021, version: 4.76.
rm -f main.pdf
make[1]: Leaving directory '/home/isaackhor/code/lsvd-rbd/atc2024'
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-13T18:34:18
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thin triple-hdd lsvd-benchmark 80g
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark
Removing all objects from pool triple-hdd with prefix lsvd-benchmark
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 10942/55356 objects
+ ./imgtool --create --rados --size=80g triple-hdd/lsvd-benchmark
+ rados -p triple-hdd stat lsvd-benchmark
triple-hdd/lsvd-benchmark mtime 2024-01-13T18:39:22.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-malloc/lsvd-write/ 21474836480
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-malloc/lsvd-write/
+ local cache_size=21474836480
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-malloc/lsvd-write//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=21474836480
+ LSVD_CACHE_SIZE=21474836480
+ rm -rf /mnt/nvme-malloc/lsvd-write//lsvd-write/2f2f640d-dafb-41c5-9460-c49188eba9dd.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-13 18:39:24.053207] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-13 18:39:24.053332] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid188861 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-13 18:39:24.124107] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-13 18:39:24.260736] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-13 18:39:24.260857] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-13 18:39:24.260923] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-13 18:39:24.260929] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-13 18:39:29.675808] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-13 18:39:30.394367] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 20480 MiB in 16 shards, 1280 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-malloc/lsvd-write//lsvd-write//0b7e7c28-d18b-4fe7-bceb-a22d9a1733d6.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-13 18:39:36.365797] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:34:18.lsvd-std.20.triple-hdd.txt client-bench.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:34:18.lsvd-std.20.triple-hdd.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:34:18.lsvd-std.20.triple-hdd.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 0 '!=' read_entire_img=0 ']'
+ eval read_entire_img=0
++ read_entire_img=0
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
+ dev_name=/dev/nvme1n1
Using device /dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=0
+ [[ 0 -eq 1 ]]
+ set +x
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.46203 s, 227 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: c37020d5-5d11-4226-8a5c-0abf224cbefb
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: File-server Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.207: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.207: Removing bigfileset tree (if exists)
0.210: Pre-allocating directories in bigfileset tree
0.609: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52/52 hits, 0 MiB read, 0.000 read amp, 52 total
[0m43.564: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
43.564: Population and pre-allocation of filesets completed
43.564: Starting 1 filereader instances
44.572: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 97/97 hits, 0 MiB read, 0.000 read amp, 97 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 100/100 hits, 0 MiB read, 0.000 read amp, 100 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 104/104 hits, 0 MiB read, 0.000 read amp, 104 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 105/105 hits, 0 MiB read, 0.000 read amp, 105 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 105/107 hits, 0 MiB read, 0.256 read amp, 107 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 106/110 hits, 0 MiB read, 0.500 read amp, 110 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 106/111 hits, 0 MiB read, 0.620 read amp, 111 total
[0m224.590: Run took 180 seconds...
224.597: Per-Operation Breakdown
statfile1            689057ops     3828ops/s   0.0mb/s    0.013ms/op [0.003ms - 283.689ms]
deletefile1          689053ops     3828ops/s   0.0mb/s    0.273ms/op [0.032ms - 238.054ms]
closefile3           689057ops     3828ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.010ms]
readfile1            689057ops     3828ops/s 500.8mb/s    0.116ms/op [0.005ms - 244.780ms]
openfile2            689057ops     3828ops/s   0.0mb/s    0.052ms/op [0.006ms - 189.212ms]
closefile2           689057ops     3828ops/s   0.0mb/s    0.004ms/op [0.001ms - 34.039ms]
appendfilerand1      689059ops     3828ops/s  29.9mb/s    0.820ms/op [0.001ms - 291.138ms]
openfile1            689060ops     3828ops/s   0.0mb/s    0.058ms/op [0.007ms - 189.155ms]
closefile1           689060ops     3828ops/s   0.0mb/s    0.007ms/op [0.001ms - 38.077ms]
wrtfile1             689062ops     3828ops/s 479.1mb/s   10.777ms/op [0.014ms - 329.985ms]
createfile1          689107ops     3828ops/s   0.0mb/s    0.136ms/op [0.023ms - 189.163ms]
224.597: IO Summary: 7579686 ops 42105.469 ops/s 3828/7656 rd/wr 1009.9mb/s 1.115ms/op
224.597: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:224.597: IO Summary: 7579686 ops 42105.469 ops/s 3828/7656 rd/wr 1009.9mb/s 1.115ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: OLTP Version 3.0  personality successfully loaded
0.003: Populating and pre-allocating filesets
0.003: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.003: Removing logfile tree (if exists)
0.006: Pre-allocating directories in logfile tree
0.006: Pre-allocating files in logfile tree
0.115: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.115: Removing datafiles tree (if exists)
0.118: Pre-allocating directories in datafiles tree
0.118: Pre-allocating files in datafiles tree
58.083: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
58.083: Population and pre-allocation of filesets completed
58.083: Starting 200 shadow instances
58.207: Starting 10 dbwr instances
58.212: Starting 1 lgwr instances
59.216: Running...
239.257: Run took 180 seconds...
239.278: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     4029960ops    22384ops/s   0.0mb/s    7.473ms/op [0.019ms - 545.700ms]
shadow-post-lg       4029962ops    22384ops/s   0.0mb/s    0.067ms/op [0.002ms - 60.018ms]
shadowhog            4029988ops    22384ops/s   0.0mb/s    0.301ms/op [0.091ms - 107.383ms]
shadowread           4055701ops    22527ops/s  43.7mb/s    0.995ms/op [0.001ms - 365.095ms]
dbwr-aiowait         40287ops      224ops/s   0.0mb/s    2.327ms/op [0.005ms - 56.054ms]
dbwr-block           40289ops      224ops/s   0.0mb/s   18.880ms/op [0.003ms - 439.944ms]
dbwr-hog             40297ops      224ops/s   0.0mb/s    0.011ms/op [0.004ms - 21.832ms]
dbwrite-a            4030980ops    22390ops/s  43.7mb/s    0.004ms/op [0.000ms - 73.525ms]
lg-block             1259ops        7ops/s   0.0mb/s  142.900ms/op [33.993ms - 1112.285ms]
lg-aiowait           1260ops        7ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.017ms]
lg-write             1261ops        7ops/s   1.7mb/s    0.009ms/op [0.001ms - 0.151ms]
239.278: IO Summary: 8129489 ops 45154.159 ops/s 22527/22397 rd/wr  89.2mb/s 0.510ms/op
239.278: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:239.278: IO Summary: 8129489 ops 45154.159 ops/s 22527/22397 rd/wr  89.2mb/s 0.510ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.003: Varmail Version 3.0 personality successfully loaded
0.003: Populating and pre-allocating filesets
0.950: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
0.950: Removing bigfileset tree (if exists)
0.952: Pre-allocating directories in bigfileset tree
0.952: Pre-allocating files in bigfileset tree
79.312: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
79.312: Population and pre-allocation of filesets completed
79.312: Starting 1 filereader instances
80.317: Running...
260.333: Run took 180 seconds...
260.334: Per-Operation Breakdown
closefile4           311856ops     1732ops/s   0.0mb/s    0.003ms/op [0.001ms - 0.430ms]
readfile4            311856ops     1732ops/s  45.8mb/s    0.742ms/op [0.004ms - 314.978ms]
openfile4            311856ops     1732ops/s   0.0mb/s    0.023ms/op [0.005ms - 0.659ms]
closefile3           311856ops     1732ops/s   0.0mb/s    0.006ms/op [0.001ms - 1.638ms]
fsyncfile3           311857ops     1732ops/s   0.0mb/s    2.824ms/op [0.564ms - 563.394ms]
appendfilerand3      311864ops     1732ops/s  13.5mb/s    0.086ms/op [0.007ms - 314.044ms]
readfile3            311864ops     1732ops/s  45.7mb/s    0.790ms/op [0.005ms - 315.209ms]
openfile3            311864ops     1732ops/s   0.0mb/s    0.022ms/op [0.005ms - 5.058ms]
closefile2           311864ops     1732ops/s   0.0mb/s    0.006ms/op [0.001ms - 5.703ms]
fsyncfile2           311864ops     1732ops/s   0.0mb/s    2.991ms/op [0.788ms - 562.380ms]
appendfilerand2      311871ops     1732ops/s  13.5mb/s    0.277ms/op [0.001ms - 315.271ms]
createfile2          311871ops     1732ops/s   0.0mb/s    0.656ms/op [0.032ms - 316.544ms]
deletefile1          311872ops     1732ops/s   0.0mb/s    0.656ms/op [0.045ms - 314.153ms]
260.335: IO Summary: 4054215 ops 22521.447 ops/s 3465/3465 rd/wr 118.5mb/s 0.699ms/op
260.335: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:260.335: IO Summary: 4054215 ops 22521.447 ops/s 3465/3465 rd/wr 118.5mb/s 0.699ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:34:18.lsvd-std.20.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-13T18:34:18.lsvd-std.20.triple-hdd.txt
Filebench /tmp/filebench/fileserver.f:224.597: IO Summary: 7579686 ops 42105.469 ops/s 3828/7656 rd/wr 1009.9mb/s 1.115ms/op
Filebench /tmp/filebench/oltp.f:239.278: IO Summary: 8129489 ops 45154.159 ops/s 22527/22397 rd/wr  89.2mb/s 0.510ms/op
Filebench /tmp/filebench/varmail.f:260.335: IO Summary: 4054215 ops 22521.447 ops/s 3465/3465 rd/wr 118.5mb/s 0.699ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7fce367f4640) exiting
