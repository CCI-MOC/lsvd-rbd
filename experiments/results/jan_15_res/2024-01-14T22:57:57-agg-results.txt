+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
++ date +%FT%T
+ cur_time=2024-01-14T22:57:57
+ default_cache_size=128849018880
+ cache_size=128849018880
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=120
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T22:57:57.lsvd-multi.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=10g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-14T22:57:57
+ create_lsvd_thick rssd2 lsvd-benchmark.multi.1 10g
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.1
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ create_lsvd_thick rssd2 lsvd-benchmark.multi.2 10g
+ ./remove_objs.py rssd2 lsvd-benchmark.multi.1
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.2
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark.multi.2
+ create_lsvd_thick rssd2 lsvd-benchmark.multi.3 10g
+ wait
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.3
+ create_lsvd_thick rssd2 lsvd-benchmark.multi.4 10g
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark.multi.3
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.4
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark.multi.4
Removing all objects from pool rssd2 with prefix lsvd-benchmark.multi.2
+Removing all objects from pool rssd2 with prefix lsvd-benchmark.multi.4
+Removing all objects from pool rssd2 with prefix lsvd-benchmark.multi.1
+Removing all objects from pool rssd2 with prefix lsvd-benchmark.multi.3
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 0/62995 objects
++ ./thick-image --size=10g rssd2/lsvd-benchmark.multi.2

Removed 0/62995 objects
+ ./thick-image --size=10g rssd2/lsvd-benchmark.multi.4

Removed 0/62995 objects
++ ./thick-image --size=10g rssd2/lsvd-benchmark.multi.1

Removed 0/62995 objects
+ ./thick-image --size=10g rssd2/lsvd-benchmark.multi.3
Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 10% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 10% complete...Thick provisioning: 12% complete...Thick provisioning: 11% complete...Thick provisioning: 11% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 12% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 13% complete...Thick provisioning: 15% complete...Thick provisioning: 14% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 15% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 16% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 19% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 20% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 21% complete...Thick provisioning: 21% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 23% complete...Thick provisioning: 23% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 25% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 27% complete...Thick provisioning: 29% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 28% complete...Thick provisioning: 30% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 31% complete...Thick provisioning: 29% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 30% complete...Thick provisioning: 30% complete...Thick provisioning: 32% complete...Thick provisioning: 31% complete...Thick provisioning: 31% complete...Thick provisioning: 33% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 32% complete...Thick provisioning: 34% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 35% complete...Thick provisioning: 33% complete...Thick provisioning: 33% complete...Thick provisioning: 36% complete...Thick provisioning: 34% complete...Thick provisioning: 34% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 37% complete...Thick provisioning: 35% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 38% complete...Thick provisioning: 36% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 39% complete...Thick provisioning: 37% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 40% complete...Thick provisioning: 38% complete...Thick provisioning: 38% complete...Thick provisioning: 41% complete...Thick provisioning: 39% complete...Thick provisioning: 39% complete...Thick provisioning: 39% complete...Thick provisioning: 42% complete...Thick provisioning: 40% complete...Thick provisioning: 40% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 43% complete...Thick provisioning: 41% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 42% complete...Thick provisioning: 44% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 43% complete...Thick provisioning: 45% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 46% complete...Thick provisioning: 44% complete...Thick provisioning: 44% complete...Thick provisioning: 47% complete...Thick provisioning: 45% complete...Thick provisioning: 45% complete...Thick provisioning: 45% complete...Thick provisioning: 46% complete...Thick provisioning: 48% complete...Thick provisioning: 46% complete...Thick provisioning: 46% complete...Thick provisioning: 49% complete...Thick provisioning: 47% complete...Thick provisioning: 47% complete...Thick provisioning: 47% complete...Thick provisioning: 50% complete...Thick provisioning: 48% complete...Thick provisioning: 48% complete...Thick provisioning: 48% complete...Thick provisioning: 51% complete...Thick provisioning: 49% complete...Thick provisioning: 49% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 52% complete...Thick provisioning: 50% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 53% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 51% complete...Thick provisioning: 54% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 53% complete...Thick provisioning: 52% complete...Thick provisioning: 56% complete...Thick provisioning: 55% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 57% complete...Thick provisioning: 56% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 58% complete...Thick provisioning: 55% complete...Thick provisioning: 57% complete...Thick provisioning: 56% complete...Thick provisioning: 56% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 57% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 60% complete...Thick provisioning: 58% complete...Thick provisioning: 57% complete...Thick provisioning: 61% complete...Thick provisioning: 61% complete...Thick provisioning: 59% complete...Thick provisioning: 58% complete...Thick provisioning: 62% complete...Thick provisioning: 62% complete...Thick provisioning: 60% complete...Thick provisioning: 59% complete...Thick provisioning: 63% complete...Thick provisioning: 63% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 64% complete...Thick provisioning: 64% complete...Thick provisioning: 62% complete...Thick provisioning: 61% complete...Thick provisioning: 65% complete...Thick provisioning: 65% complete...Thick provisioning: 63% complete...Thick provisioning: 62% complete...Thick provisioning: 66% complete...Thick provisioning: 66% complete...Thick provisioning: 64% complete...Thick provisioning: 63% complete...Thick provisioning: 67% complete...Thick provisioning: 67% complete...Thick provisioning: 65% complete...Thick provisioning: 64% complete...Thick provisioning: 68% complete...Thick provisioning: 68% complete...Thick provisioning: 66% complete...Thick provisioning: 65% complete...Thick provisioning: 69% complete...Thick provisioning: 69% complete...Thick provisioning: 67% complete...Thick provisioning: 66% complete...Thick provisioning: 70% complete...Thick provisioning: 70% complete...Thick provisioning: 68% complete...Thick provisioning: 71% complete...Thick provisioning: 67% complete...Thick provisioning: 71% complete...Thick provisioning: 69% complete...Thick provisioning: 72% complete...Thick provisioning: 72% complete...Thick provisioning: 70% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 73% complete...Thick provisioning: 71% complete...Thick provisioning: 73% complete...Thick provisioning: 70% complete...Thick provisioning: 72% complete...Thick provisioning: 74% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 73% complete...Thick provisioning: 76% complete...Thick provisioning: 75% complete...Thick provisioning: 74% complete...Thick provisioning: 71% complete...Thick provisioning: 77% complete...Thick provisioning: 76% complete...Thick provisioning: 75% complete...Thick provisioning: 72% complete...Thick provisioning: 78% complete...Thick provisioning: 77% complete...Thick provisioning: 76% complete...Thick provisioning: 79% complete...Thick provisioning: 73% complete...Thick provisioning: 78% complete...Thick provisioning: 77% complete...Thick provisioning: 80% complete...Thick provisioning: 74% complete...Thick provisioning: 79% complete...Thick provisioning: 81% complete...Thick provisioning: 78% complete...Thick provisioning: 75% complete...Thick provisioning: 80% complete...Thick provisioning: 81% complete...Thick provisioning: 82% complete...Thick provisioning: 76% complete...Thick provisioning: 83% complete...Thick provisioning: 77% complete...Thick provisioning: 84% complete...Thick provisioning: 79% complete...Thick provisioning: 82% complete...Thick provisioning: 78% complete...Thick provisioning: 80% complete...Thick provisioning: 85% complete...Thick provisioning: 83% complete...Thick provisioning: 79% complete...Thick provisioning: 81% complete...Thick provisioning: 86% complete...Thick provisioning: 84% complete...Thick provisioning: 80% complete...Thick provisioning: 87% complete...Thick provisioning: 82% complete...Thick provisioning: 85% complete...Thick provisioning: 83% complete...Thick provisioning: 88% complete...Thick provisioning: 81% complete...Thick provisioning: 86% complete...Thick provisioning: 82% complete...Thick provisioning: 89% complete...Thick provisioning: 84% complete...Thick provisioning: 87% complete...Thick provisioning: 83% complete...Thick provisioning: 90% complete...Thick provisioning: 85% complete...Thick provisioning: 88% complete...Thick provisioning: 86% complete...Thick provisioning: 84% complete...Thick provisioning: 91% complete...Thick provisioning: 89% complete...Thick provisioning: 85% complete...Thick provisioning: 92% complete...Thick provisioning: 87% complete...Thick provisioning: 90% complete...Thick provisioning: 86% complete...Thick provisioning: 93% complete...Thick provisioning: 88% complete...Thick provisioning: 87% complete...Thick provisioning: 91% complete...Thick provisioning: 94% complete...Thick provisioning: 89% complete...Thick provisioning: 92% complete...Thick provisioning: 95% complete...Thick provisioning: 88% complete...Thick provisioning: 90% complete...Thick provisioning: 89% complete...Thick provisioning: 93% complete...Thick provisioning: 96% complete...Thick provisioning: 91% complete...Thick provisioning: 90% complete...Thick provisioning: 94% complete...Thick provisioning: 97% complete...Thick provisioning: 92% complete...Thick provisioning: 91% complete...Thick provisioning: 95% complete...Thick provisioning: 98% complete...Thick provisioning: 93% complete...Thick provisioning: 96% complete...Thick provisioning: 94% complete...Thick provisioning: 99% complete...Thick provisioning: 92% complete...Thick provisioning: 97% complete...Thick provisioning: 100% complete...Thick provisioning: 93% complete...Thick provisioning: 95% complete...Thick provisioning: 98% complete...Thick provisioning: 94% complete...Thick provisioning: 96% complete...Thick provisioning: 95% complete...Thick provisioning: 99% complete...Thick provisioning: 97% complete...Thick provisioning: 96% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
Thick provisioning: 98% complete...+ rados -p rssd2 stat lsvd-benchmark.multi.1
Thick provisioning: 97% complete...Thick provisioning: 100% complete...done
rssd2/lsvd-benchmark.multi.1 mtime 2024-01-14T22:59:09.000000+0000, size 4096
+ rados -p rssd2 stat lsvd-benchmark.multi.4
rssd2/lsvd-benchmark.multi.4 mtime 2024-01-14T22:59:09.000000+0000, size 4096
Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
Thick provisioning: 100% complete...done
+ rados -p rssd2 stat lsvd-benchmark.multi.3
+ rados -p rssd2 stat lsvd-benchmark.multi.2
rssd2/lsvd-benchmark.multi.3 mtime 2024-01-14T22:59:10.000000+0000, size 4096
rssd2/lsvd-benchmark.multi.2 mtime 2024-01-14T22:59:10.000000+0000, size 4096
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/ 128849018880
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/
+ local cache_size=128849018880
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=128849018880
+ LSVD_CACHE_SIZE=128849018880
+ rm -rf /mnt/nvme-remote//lsvd-write/090b32a3-d0d6-4c19-9b9f-4d6d61938c16.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-14 22:59:16.780555] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-14 22:59:16.780677] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid805031 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-14 22:59:16.859637] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-14 22:59:16.965498] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-14 22:59:16.965576] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-14 22:59:16.965649] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-14 22:59:16.965652] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-14 22:59:22.532479] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-14 22:59:23.283126] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 lsvd-benchmark.multi.1
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.1
+ local bdev=bdev_lsvd-benchmark.multi.1
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark.multi.1 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.1
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 122880 MiB in 16 shards, 7680 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//80557905-ecc4-4e33-afbc-67e41675f422.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.1, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 22:59:29.944162] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.1 rbd disk to lun
bdev_lsvd-benchmark.multi.1
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.1
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 1/2 hits, 0 MiB read, 1.778 read amp, 2 total
[0m+ add_rbd_img rssd2 lsvd-benchmark.multi.2
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.2
+ local bdev=bdev_lsvd-benchmark.multi.2
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark.multi.2 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.2
[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//9bc7bcbe-3898-43f0-a63f-648ad26d155b.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.2, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 22:59:31.379508] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.2 rbd disk to lun
bdev_lsvd-benchmark.multi.2
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.2
+ add_rbd_img rssd2 lsvd-benchmark.multi.3
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.3
+ local bdev=bdev_lsvd-benchmark.multi.3
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark.multi.3 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.3
[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//1eef8595-1ea7-47e3-884a-5f3029b5ea32.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.3, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 22:59:32.844252] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.3 rbd disk to lun
bdev_lsvd-benchmark.multi.3
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.3
+ add_rbd_img rssd2 lsvd-benchmark.multi.4
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark.multi.4
+ local bdev=bdev_lsvd-benchmark.multi.4
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark.multi.4 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.4
[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//9c32f078-6bd0-45da-8527-a50676572343.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.4, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 22:59:34.425132] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.4 rbd disk to lun
bdev_lsvd-benchmark.multi.4
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.4
+ trap 'cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T22:57:57.lsvd-multi.rssd2.txt multi-client/client-bench-multi.bash read_entire_img=1
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T22:57:57.lsvd-multi.rssd2.txt
+ local benchscript=multi-client/client-bench-multi.bash
+ local additional_args=read_entire_img=1
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=1'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T22:57:57.lsvd-multi.rssd2.txt
===Starting client benchmark

NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
device: nvme1
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n2          SPDK00000000000001   SPDK_Controller1                         2          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n3          SPDK00000000000001   SPDK_Controller1                         3          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n4          SPDK00000000000001   SPDK_Controller1                         4          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
Using device /dev/nvme1n1
/dev/nvme1n2
/dev/nvme1n3
/dev/nvme1n4


===Reading entire image to warm cache===

106954752 bytes (107 MB, 102 MiB) copied, 1 s, 107 MB/s110100480 bytes (110 MB, 105 MiB) copied, 1 s, 110 MB/s104857600 bytes (105 MB, 100 MiB) copied, 1 s, 104 MB/s114294784 bytes (114 MB, 109 MiB) copied, 1 s, 114 MB/s218103808 bytes (218 MB, 208 MiB) copied, 2 s, 109 MB/s208666624 bytes (209 MB, 199 MiB) copied, 2 s, 104 MB/s222298112 bytes (222 MB, 212 MiB) copied, 2 s, 111 MB/s208666624 bytes (209 MB, 199 MiB) copied, 2 s, 104 MB/s335544320 bytes (336 MB, 320 MiB) copied, 3 s, 112 MB/s315621376 bytes (316 MB, 301 MiB) copied, 3 s, 105 MB/s327155712 bytes (327 MB, 312 MiB) copied, 3 s, 109 MB/s317718528 bytes (318 MB, 303 MiB) copied, 3 s, 106 MB/s447741952 bytes (448 MB, 427 MiB) copied, 4 s, 112 MB/s423624704 bytes (424 MB, 404 MiB) copied, 4 s, 106 MB/s429916160 bytes (430 MB, 410 MiB) copied, 4 s, 107 MB/s437256192 bytes (437 MB, 417 MiB) copied, 4 s, 109 MB/s532676608 bytes (533 MB, 508 MiB) copied, 5 s, 106 MB/s548405248 bytes (548 MB, 523 MiB) copied, 5 s, 110 MB/s519045120 bytes (519 MB, 495 MiB) copied, 5 s, 104 MB/s540016640 bytes (540 MB, 515 MiB) copied, 5 s, 108 MB/s643825664 bytes (644 MB, 614 MiB) copied, 6 s, 107 MB/s615514112 bytes (616 MB, 587 MiB) copied, 6 s, 103 MB/s619708416 bytes (620 MB, 591 MiB) copied, 6 s, 103 MB/s641728512 bytes (642 MB, 612 MiB) copied, 6 s, 107 MB/s724566016 bytes (725 MB, 691 MiB) copied, 7 s, 103 MB/s758120448 bytes (758 MB, 723 MiB) copied, 7 s, 108 MB/s735051776 bytes (735 MB, 701 MiB) copied, 7 s, 105 MB/s750780416 bytes (751 MB, 716 MiB) copied, 7 s, 107 MB/s869269504 bytes (869 MB, 829 MiB) copied, 8 s, 109 MB/s835715072 bytes (836 MB, 797 MiB) copied, 8 s, 104 MB/s857735168 bytes (858 MB, 818 MiB) copied, 8 s, 107 MB/s839909376 bytes (840 MB, 801 MiB) copied, 8 s, 105 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 27187/82407 hits, 3426 MiB read, 1.007 read amp, 82407 total
[0m970981376 bytes (971 MB, 926 MiB) copied, 9 s, 108 MB/s935329792 bytes (935 MB, 892 MiB) copied, 9 s, 104 MB/s921698304 bytes (922 MB, 879 MiB) copied, 9 s, 102 MB/s942669824 bytes (943 MB, 899 MiB) copied, 9 s, 105 MB/s1031798784 bytes (1.0 GB, 984 MiB) copied, 10 s, 103 MB/s1082130432 bytes (1.1 GB, 1.0 GiB) copied, 10 s, 108 MB/s1048576000 bytes (1.0 GB, 1000 MiB) copied, 10 s, 105 MB/s1046478848 bytes (1.0 GB, 998 MiB) copied, 10 s, 105 MB/s1140850688 bytes (1.1 GB, 1.1 GiB) copied, 11 s, 104 MB/s1149239296 bytes (1.1 GB, 1.1 GiB) copied, 11 s, 104 MB/s 1185939456 bytes (1.2 GB, 1.1 GiB) copied, 11 s, 108 MB/s1160773632 bytes (1.2 GB, 1.1 GiB) copied, 11 s, 105 MB/s1248854016 bytes (1.2 GB, 1.2 GiB) copied, 12 s, 104 MB/s1241513984 bytes (1.2 GB, 1.2 GiB) copied, 12 s, 103 MB/s1263534080 bytes (1.3 GB, 1.2 GiB) copied, 12 s, 105 MB/s1285554176 bytes (1.3 GB, 1.2 GiB) copied, 12 s, 107 MB/s1380974592 bytes (1.4 GB, 1.3 GiB) copied, 13 s, 106 MB/s1374683136 bytes (1.4 GB, 1.3 GiB) copied, 13 s, 106 MB/s1337982976 bytes (1.3 GB, 1.2 GiB) copied, 13 s, 103 MB/s1357905920 bytes (1.4 GB, 1.3 GiB) copied, 13 s, 104 MB/s1470103552 bytes (1.5 GB, 1.4 GiB) copied, 14 s, 105 MB/s1484783616 bytes (1.5 GB, 1.4 GiB) copied, 14 s, 106 MB/s1449132032 bytes (1.4 GB, 1.3 GiB) copied, 14 s, 103 MB/s1496317952 bytes (1.5 GB, 1.4 GiB) copied, 14 s, 107 MB/s1558183936 bytes (1.6 GB, 1.5 GiB) copied, 15 s, 104 MB/s1588592640 bytes (1.6 GB, 1.5 GiB) copied, 15 s, 106 MB/s1598029824 bytes (1.6 GB, 1.5 GiB) copied, 15 s, 106 MB/s1569718272 bytes (1.6 GB, 1.5 GiB) copied, 15 s, 105 MB/s1665138688 bytes (1.7 GB, 1.6 GiB) copied, 16 s, 104 MB/s1693450240 bytes (1.7 GB, 1.6 GiB) copied, 16 s, 106 MB/s1670381568 bytes (1.7 GB, 1.6 GiB) copied, 16 s, 104 MB/s1700790272 bytes (1.7 GB, 1.6 GiB) copied, 16 s, 106 MB/s1794113536 bytes (1.8 GB, 1.7 GiB) copied, 17 s, 106 MB/s1754267648 bytes (1.8 GB, 1.6 GiB) copied, 17 s, 103 MB/s1763704832 bytes (1.8 GB, 1.6 GiB) copied, 17 s, 104 MB/s1803550720 bytes (1.8 GB, 1.7 GiB) copied, 17 s, 106 MB/s1873805312 bytes (1.9 GB, 1.7 GiB) copied, 18 s, 104 MB/s1906311168 bytes (1.9 GB, 1.8 GiB) copied, 18 s, 106 MB/s1904214016 bytes (1.9 GB, 1.8 GiB) copied, 18 s, 106 MB/s1861222400 bytes (1.9 GB, 1.7 GiB) copied, 18 s, 103 MB/s1994391552 bytes (2.0 GB, 1.9 GiB) copied, 19 s, 105 MB/s2009071616 bytes (2.0 GB, 1.9 GiB) copied, 19 s, 106 MB/s1970274304 bytes (2.0 GB, 1.8 GiB) copied, 19 s, 104 MB/s1965031424 bytes (2.0 GB, 1.8 GiB) copied, 19 s, 103 MB/s2105540608 bytes (2.1 GB, 2.0 GiB) copied, 20 s, 105 MB/s2122317824 bytes (2.1 GB, 2.0 GiB) copied, 20 s, 106 MB/s2085617664 bytes (2.1 GB, 1.9 GiB) copied, 20 s, 104 MB/s2078277632 bytes (2.1 GB, 1.9 GiB) copied, 20 s, 104 MB/s2195718144 bytes (2.2 GB, 2.0 GiB) copied, 21 s, 105 MB/s2190475264 bytes (2.2 GB, 2.0 GiB) copied, 21 s, 104 MB/s2232418304 bytes (2.2 GB, 2.1 GiB) copied, 21 s, 106 MB/s2213543936 bytes (2.2 GB, 2.1 GiB) copied, 21 s, 105 MB/s2299527168 bytes (2.3 GB, 2.1 GiB) copied, 22 s, 105 MB/s2320498688 bytes (2.3 GB, 2.2 GiB) copied, 22 s, 105 MB/s2343567360 bytes (2.3 GB, 2.2 GiB) copied, 22 s, 107 MB/s2311061504 bytes (2.3 GB, 2.2 GiB) copied, 22 s, 105 MB/s2421161984 bytes (2.4 GB, 2.3 GiB) copied, 23 s, 105 MB/s2448424960 bytes (2.4 GB, 2.3 GiB) copied, 23 s, 106 MB/s2400190464 bytes (2.4 GB, 2.2 GiB) copied, 23 s, 104 MB/s2413821952 bytes (2.4 GB, 2.2 GiB) copied, 23 s, 105 MB/s2516582400 bytes (2.5 GB, 2.3 GiB) copied, 24 s, 105 MB/s2518679552 bytes (2.5 GB, 2.3 GiB) copied, 24 s, 105 MB/s2548039680 bytes (2.5 GB, 2.4 GiB) copied, 24 s, 106 MB/s2505048064 bytes (2.5 GB, 2.3 GiB) copied, 24 s, 104 MB/s2618294272 bytes (2.6 GB, 2.4 GiB) copied, 25 s, 105 MB/s2629828608 bytes (2.6 GB, 2.4 GiB) copied, 25 s, 105 MB/s2666528768 bytes (2.7 GB, 2.5 GiB) copied, 25 s, 107 MB/s2635071488 bytes (2.6 GB, 2.5 GiB) copied, 25 s, 105 MB/s2749366272 bytes (2.7 GB, 2.6 GiB) copied, 26 s, 106 MB/s2718957568 bytes (2.7 GB, 2.5 GiB) copied, 26 s, 105 MB/s2781872128 bytes (2.8 GB, 2.6 GiB) copied, 26 s, 107 MB/s2740977664 bytes (2.7 GB, 2.6 GiB) copied, 26 s, 105 MB/s2835349504 bytes (2.8 GB, 2.6 GiB) copied, 27 s, 105 MB/s2895118336 bytes (2.9 GB, 2.7 GiB) copied, 27 s, 107 MB/s2853175296 bytes (2.9 GB, 2.7 GiB) copied, 27 s, 106 MB/s2854223872 bytes (2.9 GB, 2.7 GiB) copied, 27 s, 106 MB/s2934964224 bytes (2.9 GB, 2.7 GiB) copied, 28 s, 105 MB/s2959081472 bytes (3.0 GB, 2.8 GiB) copied, 28 s, 106 MB/s2951741440 bytes (3.0 GB, 2.7 GiB) copied, 28 s, 105 MB/s2999975936 bytes (3.0 GB, 2.8 GiB) copied, 28 s, 107 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 276082 total
[0m3044016128 bytes (3.0 GB, 2.8 GiB) copied, 29 s, 105 MB/s3097493504 bytes (3.1 GB, 2.9 GiB) copied, 29 s, 107 MB/s3067084800 bytes (3.1 GB, 2.9 GiB) copied, 29 s, 106 MB/s3035627520 bytes (3.0 GB, 2.8 GiB) copied, 29 s, 105 MB/s3149922304 bytes (3.1 GB, 2.9 GiB) copied, 30 s, 105 MB/s3174039552 bytes (3.2 GB, 3.0 GiB) copied, 30 s, 106 MB/s3141533696 bytes (3.1 GB, 2.9 GiB) copied, 30 s, 105 MB/s3204448256 bytes (3.2 GB, 3.0 GiB) copied, 30 s, 107 MB/s3313500160 bytes (3.3 GB, 3.1 GiB) copied, 31 s, 107 MB/s3279945728 bytes (3.3 GB, 3.1 GiB) copied, 31 s, 106 MB/s3239051264 bytes (3.2 GB, 3.0 GiB) copied, 31 s, 104 MB/s3253731328 bytes (3.3 GB, 3.0 GiB) copied, 31 s, 105 MB/s3359637504 bytes (3.4 GB, 3.1 GiB) copied, 32 s, 105 MB/s3419406336 bytes (3.4 GB, 3.2 GiB) copied, 32 s, 107 MB/s3352297472 bytes (3.4 GB, 3.1 GiB) copied, 32 s, 105 MB/s3395289088 bytes (3.4 GB, 3.2 GiB) copied, 32 s, 106 MB/s3527409664 bytes (3.5 GB, 3.3 GiB) copied, 33 s, 107 MB/s3485466624 bytes (3.5 GB, 3.2 GiB) copied, 33 s, 106 MB/s3461349376 bytes (3.5 GB, 3.2 GiB) copied, 33 s, 105 MB/s3457155072 bytes (3.5 GB, 3.2 GiB) copied, 33 s, 105 MB/s3598712832 bytes (3.6 GB, 3.4 GiB) copied, 34 s, 106 MB/s3571449856 bytes (3.6 GB, 3.3 GiB) copied, 34 s, 105 MB/s3629121536 bytes (3.6 GB, 3.4 GiB) copied, 34 s, 107 MB/s3553624064 bytes (3.6 GB, 3.3 GiB) copied, 34 s, 104 MB/s3711959040 bytes (3.7 GB, 3.5 GiB) copied, 35 s, 106 MB/s3683647488 bytes (3.7 GB, 3.4 GiB) copied, 35 s, 105 MB/s3666870272 bytes (3.7 GB, 3.4 GiB) copied, 35 s, 105 MB/s3740270592 bytes (3.7 GB, 3.5 GiB) copied, 35 s, 107 MB/s3821010944 bytes (3.8 GB, 3.6 GiB) copied, 36 s, 106 MB/s3795845120 bytes (3.8 GB, 3.5 GiB) copied, 36 s, 105 MB/s3776970752 bytes (3.8 GB, 3.5 GiB) copied, 36 s, 105 MB/s3849322496 bytes (3.8 GB, 3.6 GiB) copied, 36 s, 107 MB/s3895459840 bytes (3.9 GB, 3.6 GiB) copied, 37 s, 105 MB/s3927965696 bytes (3.9 GB, 3.7 GiB) copied, 37 s, 106 MB/s3954180096 bytes (4.0 GB, 3.7 GiB) copied, 37 s, 107 MB/s3878682624 bytes (3.9 GB, 3.6 GiB) copied, 37 s, 105 MB/s3986685952 bytes (4.0 GB, 3.7 GiB) copied, 38 s, 105 MB/s4028628992 bytes (4.0 GB, 3.8 GiB) copied, 38 s, 106 MB/s3995074560 bytes (4.0 GB, 3.7 GiB) copied, 38 s, 105 MB/s4060086272 bytes (4.1 GB, 3.8 GiB) copied, 38 s, 107 MB/s4135583744 bytes (4.1 GB, 3.9 GiB) copied, 39 s, 106 MB/s4095737856 bytes (4.1 GB, 3.8 GiB) copied, 39 s, 105 MB/s4084203520 bytes (4.1 GB, 3.8 GiB) copied, 39 s, 105 MB/s4154458112 bytes (4.2 GB, 3.9 GiB) copied, 39 s, 106 MB/s4205838336 bytes (4.2 GB, 3.9 GiB) copied, 40 s, 105 MB/s4190109696 bytes (4.2 GB, 3.9 GiB) copied, 40 s, 105 MB/s4256169984 bytes (4.3 GB, 4.0 GiB) copied, 40 s, 106 MB/s4246732800 bytes (4.2 GB, 4.0 GiB) copied, 40 s, 106 MB/s4294967296 bytes (4.3 GB, 4.0 GiB) copied, 41 s, 105 MB/s4316987392 bytes (4.3 GB, 4.0 GiB) copied, 41 s, 105 MB/s4352638976 bytes (4.4 GB, 4.1 GiB) copied, 41 s, 106 MB/s4361027584 bytes (4.4 GB, 4.1 GiB) copied, 41 s, 106 MB/s4460642304 bytes (4.5 GB, 4.2 GiB) copied, 42 s, 106 MB/s4398776320 bytes (4.4 GB, 4.1 GiB) copied, 42 s, 105 MB/s4466933760 bytes (4.5 GB, 4.2 GiB) copied, 42 s, 106 MB/s4415553536 bytes (4.4 GB, 4.1 GiB) copied, 42 s, 105 MB/s4572839936 bytes (4.6 GB, 4.3 GiB) copied, 43 s, 106 MB/s4564451328 bytes (4.6 GB, 4.3 GiB) copied, 43 s, 106 MB/s4514119680 bytes (4.5 GB, 4.2 GiB) copied, 43 s, 105 MB/s4501536768 bytes (4.5 GB, 4.2 GiB) copied, 43 s, 105 MB/s4671406080 bytes (4.7 GB, 4.4 GiB) copied, 44 s, 106 MB/s4625268736 bytes (4.6 GB, 4.3 GiB) copied, 44 s, 105 MB/s4677697536 bytes (4.7 GB, 4.4 GiB) copied, 44 s, 106 MB/s4615831552 bytes (4.6 GB, 4.3 GiB) copied, 44 s, 105 MB/s4781506560 bytes (4.8 GB, 4.5 GiB) copied, 45 s, 106 MB/s4779409408 bytes (4.8 GB, 4.5 GiB) copied, 45 s, 106 MB/s4730126336 bytes (4.7 GB, 4.4 GiB) copied, 45 s, 105 MB/s4718592000 bytes (4.7 GB, 4.4 GiB) copied, 45 s, 105 MB/s4822401024 bytes (4.8 GB, 4.5 GiB) copied, 46 s, 105 MB/s4895801344 bytes (4.9 GB, 4.6 GiB) copied, 46 s, 106 MB/s4890558464 bytes (4.9 GB, 4.6 GiB) copied, 46 s, 106 MB/s4840226816 bytes (4.8 GB, 4.5 GiB) copied, 46 s, 105 MB/s4955570176 bytes (5.0 GB, 4.6 GiB) copied, 47 s, 105 MB/s5002756096 bytes (5.0 GB, 4.7 GiB) copied, 47 s, 106 MB/s5010096128 bytes (5.0 GB, 4.7 GiB) copied, 47 s, 107 MB/s4932501504 bytes (4.9 GB, 4.6 GiB) copied, 47 s, 105 MB/s5103419392 bytes (5.1 GB, 4.8 GiB) copied, 48 s, 106 MB/s5056233472 bytes (5.1 GB, 4.7 GiB) copied, 48 s, 105 MB/s5022679040 bytes (5.0 GB, 4.7 GiB) copied, 48 s, 105 MB/s5100273664 bytes (5.1 GB, 4.8 GiB) copied, 48 s, 106 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 469294 total
[0m5124390912 bytes (5.1 GB, 4.8 GiB) copied, 49 s, 105 MB/s5200936960 bytes (5.2 GB, 4.8 GiB) copied, 49 s, 106 MB/s5201985536 bytes (5.2 GB, 4.8 GiB) copied, 49 s, 106 MB/s5161091072 bytes (5.2 GB, 4.8 GiB) copied, 49 s, 105 MB/s5312086016 bytes (5.3 GB, 4.9 GiB) copied, 50 s, 106 MB/s5271191552 bytes (5.3 GB, 4.9 GiB) copied, 50 s, 105 MB/s5237637120 bytes (5.2 GB, 4.9 GiB) copied, 50 s, 105 MB/s5316280320 bytes (5.3 GB, 5.0 GiB) copied, 50 s, 106 MB/s5416943616 bytes (5.4 GB, 5.0 GiB) copied, 51 s, 106 MB/s5349834752 bytes (5.3 GB, 5.0 GiB) copied, 51 s, 105 MB/s5379194880 bytes (5.4 GB, 5.0 GiB) copied, 51 s, 105 MB/s5415895040 bytes (5.4 GB, 5.0 GiB) copied, 51 s, 106 MB/s5449449472 bytes (5.4 GB, 5.1 GiB) copied, 52 s, 105 MB/s5481955328 bytes (5.5 GB, 5.1 GiB) copied, 52 s, 105 MB/s5506072576 bytes (5.5 GB, 5.1 GiB) copied, 52 s, 106 MB/s5516558336 bytes (5.5 GB, 5.1 GiB) copied, 52 s, 106 MB/s5613027328 bytes (5.6 GB, 5.2 GiB) copied, 53 s, 106 MB/s5557452800 bytes (5.6 GB, 5.2 GiB) copied, 53 s, 105 MB/s5591007232 bytes (5.6 GB, 5.2 GiB) copied, 53 s, 105 MB/s5621415936 bytes (5.6 GB, 5.2 GiB) copied, 53 s, 106 MB/s5660213248 bytes (5.7 GB, 5.3 GiB) copied, 54 s, 105 MB/s5725224960 bytes (5.7 GB, 5.3 GiB) copied, 54 s, 106 MB/s5715787776 bytes (5.7 GB, 5.3 GiB) copied, 54 s, 106 MB/s5700059136 bytes (5.7 GB, 5.3 GiB) copied, 54 s, 106 MB/s5797576704 bytes (5.8 GB, 5.4 GiB) copied, 55 s, 105 MB/s5767168000 bytes (5.8 GB, 5.4 GiB) copied, 55 s, 105 MB/s5826936832 bytes (5.8 GB, 5.4 GiB) copied, 55 s, 106 MB/s5817499648 bytes (5.8 GB, 5.4 GiB) copied, 55 s, 106 MB/s5878317056 bytes (5.9 GB, 5.5 GiB) copied, 56 s, 105 MB/s5904531456 bytes (5.9 GB, 5.5 GiB) copied, 56 s, 105 MB/s5923405824 bytes (5.9 GB, 5.5 GiB) copied, 56 s, 106 MB/s5939134464 bytes (5.9 GB, 5.5 GiB) copied, 56 s, 106 MB/s6002049024 bytes (6.0 GB, 5.6 GiB) copied, 57 s, 105 MB/s5978980352 bytes (6.0 GB, 5.6 GiB) copied, 57 s, 105 MB/s6051332096 bytes (6.1 GB, 5.6 GiB) copied, 57 s, 106 MB/s6016729088 bytes (6.0 GB, 5.6 GiB) copied, 57 s, 106 MB/s6112149504 bytes (6.1 GB, 5.7 GiB) copied, 58 s, 105 MB/s6129975296 bytes (6.1 GB, 5.7 GiB) copied, 58 s, 106 MB/s6157238272 bytes (6.2 GB, 5.7 GiB) copied, 58 s, 106 MB/s6079643648 bytes (6.1 GB, 5.7 GiB) copied, 58 s, 105 MB/s6235881472 bytes (6.2 GB, 5.8 GiB) copied, 59 s, 106 MB/s6263144448 bytes (6.3 GB, 5.8 GiB) copied, 59 s, 106 MB/s6187646976 bytes (6.2 GB, 5.8 GiB) copied, 59 s, 105 MB/s6220152832 bytes (6.2 GB, 5.8 GiB) copied, 59 s, 105 MB/s6377439232 bytes (6.4 GB, 5.9 GiB) copied, 60 s, 106 MB/s6334447616 bytes (6.3 GB, 5.9 GiB) copied, 60 s, 106 MB/s6297747456 bytes (6.3 GB, 5.9 GiB) copied, 60 s, 105 MB/s6351224832 bytes (6.4 GB, 5.9 GiB) copied, 60 s, 106 MB/s6486491136 bytes (6.5 GB, 6.0 GiB) copied, 61 s, 106 MB/s6412042240 bytes (6.4 GB, 6.0 GiB) copied, 61 s, 105 MB/s6444548096 bytes (6.4 GB, 6.0 GiB) copied, 61 s, 106 MB/s6458179584 bytes (6.5 GB, 6.0 GiB) copied, 61 s, 106 MB/s6599737344 bytes (6.6 GB, 6.1 GiB) copied, 62 s, 106 MB/s6552551424 bytes (6.6 GB, 6.1 GiB) copied, 62 s, 106 MB/s6520045568 bytes (6.5 GB, 6.1 GiB) copied, 62 s, 105 MB/s6567231488 bytes (6.6 GB, 6.1 GiB) copied, 62 s, 106 MB/s6707740672 bytes (6.7 GB, 6.2 GiB) copied, 63 s, 106 MB/s6658457600 bytes (6.7 GB, 6.2 GiB) copied, 63 s, 106 MB/s6664749056 bytes (6.7 GB, 6.2 GiB) copied, 63 s, 106 MB/s6627000320 bytes (6.6 GB, 6.2 GiB) copied, 63 s, 105 MB/s6776946688 bytes (6.8 GB, 6.3 GiB) copied, 64 s, 106 MB/s6733955072 bytes (6.7 GB, 6.3 GiB) copied, 64 s, 105 MB/s6810501120 bytes (6.8 GB, 6.3 GiB) copied, 64 s, 106 MB/s6760169472 bytes (6.8 GB, 6.3 GiB) copied, 64 s, 106 MB/s6876561408 bytes (6.9 GB, 6.4 GiB) copied, 65 s, 106 MB/s6915358720 bytes (6.9 GB, 6.4 GiB) copied, 65 s, 106 MB/s6872367104 bytes (6.9 GB, 6.4 GiB) copied, 65 s, 106 MB/s6835666944 bytes (6.8 GB, 6.4 GiB) copied, 65 s, 105 MB/s6976176128 bytes (7.0 GB, 6.5 GiB) copied, 66 s, 106 MB/s6978273280 bytes (7.0 GB, 6.5 GiB) copied, 66 s, 106 MB/s6943670272 bytes (6.9 GB, 6.5 GiB) copied, 66 s, 105 MB/s7019167744 bytes (7.0 GB, 6.5 GiB) copied, 66 s, 106 MB/s7132413952 bytes (7.1 GB, 6.6 GiB) copied, 67 s, 106 MB/s7085228032 bytes (7.1 GB, 6.6 GiB) copied, 67 s, 106 MB/s7078936576 bytes (7.1 GB, 6.6 GiB) copied, 67 s, 106 MB/s7043284992 bytes (7.0 GB, 6.6 GiB) copied, 67 s, 105 MB/s7191134208 bytes (7.2 GB, 6.7 GiB) copied, 68 s, 106 MB/s7242514432 bytes (7.2 GB, 6.7 GiB) copied, 68 s, 107 MB/s7201619968 bytes (7.2 GB, 6.7 GiB) copied, 68 s, 106 MB/s7159676928 bytes (7.2 GB, 6.7 GiB) copied, 68 s, 105 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 664624 total
[0m7311720448 bytes (7.3 GB, 6.8 GiB) copied, 69 s, 106 MB/s7356809216 bytes (7.4 GB, 6.9 GiB) copied, 69 s, 107 MB/s7270825984 bytes (7.3 GB, 6.8 GiB) copied, 69 s, 105 MB/s7303331840 bytes (7.3 GB, 6.8 GiB) copied, 69 s, 106 MB/s7459569664 bytes (7.5 GB, 6.9 GiB) copied, 70 s, 107 MB/s7409238016 bytes (7.4 GB, 6.9 GiB) copied, 70 s, 106 MB/s7417626624 bytes (7.4 GB, 6.9 GiB) copied, 70 s, 106 MB/s7375683584 bytes (7.4 GB, 6.9 GiB) copied, 70 s, 105 MB/s7495221248 bytes (7.5 GB, 7.0 GiB) copied, 71 s, 106 MB/s7538212864 bytes (7.5 GB, 7.0 GiB) copied, 71 s, 106 MB/s7476346880 bytes (7.5 GB, 7.0 GiB) copied, 71 s, 105 MB/s7511998464 bytes (7.5 GB, 7.0 GiB) copied, 71 s, 106 MB/s7632584704 bytes (7.6 GB, 7.1 GiB) copied, 72 s, 106 MB/s7579107328 bytes (7.6 GB, 7.1 GiB) copied, 72 s, 105 MB/s7589593088 bytes (7.6 GB, 7.1 GiB) copied, 72 s, 105 MB/s7616856064 bytes (7.6 GB, 7.1 GiB) copied, 72 s, 106 MB/s7733248000 bytes (7.7 GB, 7.2 GiB) copied, 73 s, 106 MB/s7668236288 bytes (7.7 GB, 7.1 GiB) copied, 73 s, 105 MB/s7693402112 bytes (7.7 GB, 7.2 GiB) copied, 73 s, 105 MB/s7724859392 bytes (7.7 GB, 7.2 GiB) copied, 73 s, 106 MB/s7804551168 bytes (7.8 GB, 7.3 GiB) copied, 74 s, 105 MB/s7840202752 bytes (7.8 GB, 7.3 GiB) copied, 74 s, 106 MB/s7839154176 bytes (7.8 GB, 7.3 GiB) copied, 74 s, 106 MB/s7784628224 bytes (7.8 GB, 7.2 GiB) copied, 74 s, 105 MB/s7951351808 bytes (8.0 GB, 7.4 GiB) copied, 75 s, 106 MB/s7948206080 bytes (7.9 GB, 7.4 GiB) copied, 75 s, 106 MB/s7881097216 bytes (7.9 GB, 7.3 GiB) copied, 75 s, 105 MB/s7905214464 bytes (7.9 GB, 7.4 GiB) copied, 75 s, 105 MB/s8056209408 bytes (8.1 GB, 7.5 GiB) copied, 76 s, 106 MB/s7984906240 bytes (8.0 GB, 7.4 GiB) copied, 76 s, 105 MB/s8053063680 bytes (8.1 GB, 7.5 GiB) copied, 76 s, 106 MB/s8010072064 bytes (8.0 GB, 7.5 GiB) copied, 76 s, 105 MB/s8118075392 bytes (8.1 GB, 7.6 GiB) copied, 77 s, 105 MB/s8097103872 bytes (8.1 GB, 7.5 GiB) copied, 77 s, 105 MB/s8166309888 bytes (8.2 GB, 7.6 GiB) copied, 77 s, 106 MB/s8163164160 bytes (8.2 GB, 7.6 GiB) copied, 77 s, 106 MB/s8204058624 bytes (8.2 GB, 7.6 GiB) copied, 78 s, 105 MB/s8276410368 bytes (8.3 GB, 7.7 GiB) copied, 78 s, 106 MB/s8268021760 bytes (8.3 GB, 7.7 GiB) copied, 78 s, 106 MB/s8225030144 bytes (8.2 GB, 7.7 GiB) copied, 78 s, 105 MB/s8385462272 bytes (8.4 GB, 7.8 GiB) copied, 79 s, 106 MB/s8328839168 bytes (8.3 GB, 7.8 GiB) copied, 79 s, 105 MB/s8384413696 bytes (8.4 GB, 7.8 GiB) copied, 79 s, 106 MB/s8316256256 bytes (8.3 GB, 7.7 GiB) copied, 79 s, 105 MB/s8416919552 bytes (8.4 GB, 7.8 GiB) copied, 80 s, 105 MB/s8486125568 bytes (8.5 GB, 7.9 GiB) copied, 80 s, 106 MB/s8428453888 bytes (8.4 GB, 7.8 GiB) copied, 80 s, 105 MB/s8485076992 bytes (8.5 GB, 7.9 GiB) copied, 80 s, 106 MB/s8588886016 bytes (8.6 GB, 8.0 GiB) copied, 81 s, 106 MB/s8595177472 bytes (8.6 GB, 8.0 GiB) copied, 81 s, 106 MB/s8516534272 bytes (8.5 GB, 7.9 GiB) copied, 81 s, 105 MB/s8532262912 bytes (8.5 GB, 7.9 GiB) copied, 81 s, 105 MB/s8597274624 bytes (8.6 GB, 8.0 GiB) copied, 82 s, 105 MB/s8684306432 bytes (8.7 GB, 8.1 GiB) copied, 82 s, 106 MB/s8615100416 bytes (8.6 GB, 8.0 GiB) copied, 82 s, 105 MB/s8675917824 bytes (8.7 GB, 8.1 GiB) copied, 82 s, 106 MB/s8777629696 bytes (8.8 GB, 8.2 GiB) copied, 83 s, 106 MB/s8714715136 bytes (8.7 GB, 8.1 GiB) copied, 83 s, 105 MB/s8697937920 bytes (8.7 GB, 8.1 GiB) copied, 83 s, 105 MB/s8784969728 bytes (8.8 GB, 8.2 GiB) copied, 83 s, 106 MB/s8883535872 bytes (8.9 GB, 8.3 GiB) copied, 84 s, 106 MB/s8814329856 bytes (8.8 GB, 8.2 GiB) copied, 84 s, 105 MB/s8885633024 bytes (8.9 GB, 8.3 GiB) copied, 84 s, 106 MB/s8797552640 bytes (8.8 GB, 8.2 GiB) copied, 84 s, 105 MB/s8919187456 bytes (8.9 GB, 8.3 GiB) copied, 85 s, 105 MB/s8895070208 bytes (8.9 GB, 8.3 GiB) copied, 85 s, 105 MB/s8995733504 bytes (9.0 GB, 8.4 GiB) copied, 85 s, 106 MB/s8993636352 bytes (9.0 GB, 8.4 GiB) copied, 85 s, 106 MB/s9028239360 bytes (9.0 GB, 8.4 GiB) copied, 86 s, 105 MB/s9102688256 bytes (9.1 GB, 8.5 GiB) copied, 86 s, 106 MB/s9102688256 bytes (9.1 GB, 8.5 GiB) copied, 86 s, 106 MB/s9003073536 bytes (9.0 GB, 8.4 GiB) copied, 86 s, 105 MB/s9099542528 bytes (9.1 GB, 8.5 GiB) copied, 87 s, 105 MB/s9211740160 bytes (9.2 GB, 8.6 GiB) copied, 87 s, 106 MB/s9203351552 bytes (9.2 GB, 8.6 GiB) copied, 87 s, 106 MB/s9142534144 bytes (9.1 GB, 8.5 GiB) copied, 87 s, 105 MB/s9259974656 bytes (9.3 GB, 8.6 GiB) copied, 88 s, 105 MB/s9315549184 bytes (9.3 GB, 8.7 GiB) copied, 88 s, 106 MB/s9212788736 bytes (9.2 GB, 8.6 GiB) copied, 88 s, 105 MB/s9326034944 bytes (9.3 GB, 8.7 GiB) copied, 88 s, 106 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 854482 total
[0m9308209152 bytes (9.3 GB, 8.7 GiB) copied, 89 s, 105 MB/s9363783680 bytes (9.4 GB, 8.7 GiB) copied, 89 s, 105 MB/s9428795392 bytes (9.4 GB, 8.8 GiB) copied, 89 s, 106 MB/s9429843968 bytes (9.4 GB, 8.8 GiB) copied, 89 s, 106 MB/s9543090176 bytes (9.5 GB, 8.9 GiB) copied, 90 s, 106 MB/s9417261056 bytes (9.4 GB, 8.8 GiB) copied, 90 s, 105 MB/s9481224192 bytes (9.5 GB, 8.8 GiB) copied, 90 s, 105 MB/s9545187328 bytes (9.5 GB, 8.9 GiB) copied, 90 s, 106 MB/s9522118656 bytes (9.5 GB, 8.9 GiB) copied, 91 s, 105 MB/s9645850624 bytes (9.6 GB, 9.0 GiB) copied, 91 s, 106 MB/s9650044928 bytes (9.7 GB, 9.0 GiB) copied, 91 s, 106 MB/s9589227520 bytes (9.6 GB, 8.9 GiB) copied, 91 s, 105 MB/s9624879104 bytes (9.6 GB, 9.0 GiB) copied, 92 s, 105 MB/s9690939392 bytes (9.7 GB, 9.0 GiB) copied, 92 s, 105 MB/s9753853952 bytes (9.8 GB, 9.1 GiB) copied, 92 s, 106 MB/s9753853952 bytes (9.8 GB, 9.1 GiB) copied, 92 s, 106 MB/s9733931008 bytes (9.7 GB, 9.1 GiB) copied, 93 s, 105 MB/s9807331328 bytes (9.8 GB, 9.1 GiB) copied, 93 s, 105 MB/s9862905856 bytes (9.9 GB, 9.2 GiB) copied, 93 s, 106 MB/s9865003008 bytes (9.9 GB, 9.2 GiB) copied, 93 s, 106 MB/s9959374848 bytes (10 GB, 9.3 GiB) copied, 94 s, 106 MB/s 9910091776 bytes (9.9 GB, 9.2 GiB) copied, 94 s, 105 MB/s9834594304 bytes (9.8 GB, 9.2 GiB) copied, 94 s, 105 MB/s9962520576 bytes (10 GB, 9.3 GiB) copied, 94 s, 106 MB/s 10009706496 bytes (10 GB, 9.3 GiB) copied, 95 s, 105 MB/s9928966144 bytes (9.9 GB, 9.2 GiB) copied, 95 s, 105 MB/s10060038144 bytes (10 GB, 9.4 GiB) copied, 95 s, 106 MB/s10055843840 bytes (10 GB, 9.4 GiB) copied, 95 s, 106 MB/s10115612672 bytes (10 GB, 9.4 GiB) copied, 96 s, 105 MB/s10034872320 bytes (10 GB, 9.3 GiB) copied, 96 s, 105 MB/s10168041472 bytes (10 GB, 9.5 GiB) copied, 96 s, 106 MB/s10166992896 bytes (10 GB, 9.5 GiB) copied, 96 s, 106 MB/s10268704768 bytes (10 GB, 9.6 GiB) copied, 97 s, 106 MB/s10232004608 bytes (10 GB, 9.5 GiB) copied, 97 s, 105 MB/s10147069952 bytes (10 GB, 9.5 GiB) copied, 97 s, 105 MB/s10272899072 bytes (10 GB, 9.6 GiB) copied, 97 s, 106 MB/s10255073280 bytes (10 GB, 9.6 GiB) copied, 98 s, 105 MB/s10330570752 bytes (10 GB, 9.6 GiB) copied, 98 s, 105 MB/s10381950976 bytes (10 GB, 9.7 GiB) copied, 98 s, 106 MB/s10379853824 bytes (10 GB, 9.7 GiB) copied, 98 s, 106 MB/s10486808576 bytes (10 GB, 9.8 GiB) copied, 99 s, 106 MB/s10492051456 bytes (10 GB, 9.8 GiB) copied, 99 s, 106 MB/s10443816960 bytes (10 GB, 9.7 GiB) copied, 99 s, 105 MB/s10364125184 bytes (10 GB, 9.7 GiB) copied, 99 s, 105 MB/s10556014592 bytes (11 GB, 9.8 GiB) copied, 100 s, 106 MB/s10600054784 bytes (11 GB, 9.9 GiB) copied, 100 s, 106 MB/s10589569024 bytes (11 GB, 9.9 GiB) copied, 100 s, 106 MB/s10474225664 bytes (10 GB, 9.8 GiB) copied, 100 s, 105 MB/s10692329472 bytes (11 GB, 10 GiB) copied, 101 s, 106 MB/s 10703863808 bytes (11 GB, 10 GiB) copied, 101 s, 106 MB/s 10579083264 bytes (11 GB, 9.9 GiB) copied, 101 s, 105 MB/s10650386432 bytes (11 GB, 9.9 GiB) copied, 101 s, 105 MB/s10680795136 bytes (11 GB, 9.9 GiB) copied, 102 s, 105 MB/s
10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 102.646 s, 105 MB/s

10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 102.83 s, 104 MB/s

10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 103.187 s, 104 MB/s

10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 103.721 s, 104 MB/s


===Fio: workload=randread, time=60, iodepth=256, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 4 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 1837880 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 5929199 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 10049053 total
[0m
j1: (groupid=0, jobs=4): err= 0: pid=3302: Sun Jan 14 23:02:26 2024
  read: IOPS=192k, BW=748MiB/s (784MB/s)(43.8GiB/60002msec)
    slat (usec): min=4, max=2396, avg=18.51, stdev=51.64
    clat (usec): min=184, max=15645, avg=5325.85, stdev=1751.32
     lat (usec): min=374, max=15667, avg=5344.63, stdev=1757.84
    clat percentiles (usec):
     |  1.00th=[ 3752],  5.00th=[ 3884], 10.00th=[ 3982], 20.00th=[ 4146],
     | 30.00th=[ 4293], 40.00th=[ 4424], 50.00th=[ 4555], 60.00th=[ 4752],
     | 70.00th=[ 5014], 80.00th=[ 7177], 90.00th=[ 8455], 95.00th=[ 9110],
     | 99.00th=[10028], 99.50th=[10421], 99.90th=[12256], 99.95th=[12518],
     | 99.99th=[13042]
   bw (  KiB/s): min=412208, max=1027568, per=100.00%, avg=767220.44, stdev=45769.55, samples=476
   iops        : min=103052, max=256892, avg=191805.09, stdev=11442.39, samples=476
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=11.19%, 10=87.82%, 20=0.99%
  cpu          : usr=13.57%, sys=38.12%, ctx=1058810, majf=0, minf=1470
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=11491504,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=748MiB/s (784MB/s), 748MiB/s-748MiB/s (784MB/s-784MB/s), io=43.8GiB (47.1GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=192k, BW=748MiB/s (784MB/s)(43.8GiB/60002msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 13326690 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 14683534 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 16022103 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=3356: Sun Jan 14 23:03:28 2024
  read: IOPS=63.7k, BW=249MiB/s (261MB/s)(14.6GiB/60001msec)
    slat (usec): min=4, max=8642, avg=12.94, stdev=37.03
    clat (usec): min=189, max=10894, avg=1993.95, stdev=328.13
     lat (usec): min=197, max=10900, avg=2007.24, stdev=327.03
    clat percentiles (usec):
     |  1.00th=[ 1172],  5.00th=[ 1450], 10.00th=[ 1598], 20.00th=[ 1778],
     | 30.00th=[ 1860], 40.00th=[ 1926], 50.00th=[ 1991], 60.00th=[ 2057],
     | 70.00th=[ 2114], 80.00th=[ 2212], 90.00th=[ 2343], 95.00th=[ 2507],
     | 99.00th=[ 3064], 99.50th=[ 3130], 99.90th=[ 3326], 99.95th=[ 3556],
     | 99.99th=[ 4490]
   bw (  KiB/s): min=220016, max=291480, per=99.97%, avg=254743.53, stdev=12313.63, samples=119
   iops        : min=55004, max=72870, avg=63685.87, stdev=3078.41, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=51.87%, 4=48.09%, 10=0.02%, 20=0.01%
  cpu          : usr=16.11%, sys=61.51%, ctx=105103, majf=0, minf=137
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3822335,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=249MiB/s (261MB/s), 249MiB/s-249MiB/s (261MB/s-261MB/s), io=14.6GiB (15.7GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=63.7k, BW=249MiB/s (261MB/s)(14.6GiB/60001msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 17254223 total
[0m

===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 19141946 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 21792177 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 24428362 total
[0m
j1: (groupid=0, jobs=2): err= 0: pid=3402: Sun Jan 14 23:04:30 2024
  read: IOPS=114k, BW=444MiB/s (465MB/s)(26.0GiB/60001msec)
    slat (usec): min=4, max=8217, avg=15.18, stdev=58.51
    clat (usec): min=143, max=12817, avg=2235.95, stdev=786.14
     lat (usec): min=151, max=12831, avg=2251.42, stdev=790.01
    clat percentiles (usec):
     |  1.00th=[ 1352],  5.00th=[ 1614], 10.00th=[ 1680], 20.00th=[ 1778],
     | 30.00th=[ 1860], 40.00th=[ 1942], 50.00th=[ 2024], 60.00th=[ 2114],
     | 70.00th=[ 2212], 80.00th=[ 2376], 90.00th=[ 2868], 95.00th=[ 4621],
     | 99.00th=[ 5014], 99.50th=[ 5145], 99.90th=[ 5473], 99.95th=[ 5604],
     | 99.99th=[ 5997]
   bw (  KiB/s): min=219624, max=557088, per=100.00%, avg=454525.24, stdev=52333.69, samples=238
   iops        : min=54906, max=139272, avg=113631.28, stdev=13083.41, samples=238
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=47.69%, 4=45.16%, 10=7.14%, 20=0.01%
  cpu          : usr=14.70%, sys=44.31%, ctx=324598, majf=0, minf=282
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=6816961,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=444MiB/s (465MB/s), 444MiB/s-444MiB/s (465MB/s-465MB/s), io=26.0GiB (27.9GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=114k, BW=444MiB/s (465MB/s)(26.0GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 27496909 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 30978958 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 34869804 total
[0m
j1: (groupid=0, jobs=3): err= 0: pid=3450: Sun Jan 14 23:05:33 2024
  read: IOPS=169k, BW=661MiB/s (693MB/s)(38.7GiB/60001msec)
    slat (usec): min=4, max=7732, avg=15.25, stdev=43.61
    clat (usec): min=87, max=9677, avg=2252.86, stdev=671.47
     lat (usec): min=186, max=9682, avg=2268.39, stdev=675.78
    clat percentiles (usec):
     |  1.00th=[ 1500],  5.00th=[ 1696], 10.00th=[ 1778], 20.00th=[ 1876],
     | 30.00th=[ 1958], 40.00th=[ 2008], 50.00th=[ 2073], 60.00th=[ 2147],
     | 70.00th=[ 2245], 80.00th=[ 2343], 90.00th=[ 2737], 95.00th=[ 3949],
     | 99.00th=[ 5080], 99.50th=[ 5473], 99.90th=[ 5800], 99.95th=[ 5932],
     | 99.99th=[ 6456]
   bw (  KiB/s): min=323228, max=837711, per=99.99%, avg=676513.66, stdev=42986.78, samples=357
   iops        : min=80807, max=209427, avg=169128.05, stdev=10746.67, samples=357
  lat (usec)   : 100=0.01%, 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=37.68%, 4=57.60%, 10=4.71%
  cpu          : usr=16.58%, sys=44.77%, ctx=603845, majf=0, minf=910
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=10148690,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=661MiB/s (693MB/s), 661MiB/s-661MiB/s (693MB/s-693MB/s), io=38.7GiB (41.6GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=169k, BW=661MiB/s (693MB/s)(38.7GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 38138731 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 42007049 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 45788331 total
[0m
j1: (groupid=0, jobs=4): err= 0: pid=3507: Sun Jan 14 23:06:35 2024
  read: IOPS=179k, BW=700MiB/s (734MB/s)(41.0GiB/60001msec)
    slat (usec): min=4, max=5398, avg=19.80, stdev=60.61
    clat (usec): min=376, max=22389, avg=2835.23, stdev=1209.85
     lat (usec): min=424, max=22409, avg=2855.33, stdev=1218.86
    clat percentiles (usec):
     |  1.00th=[ 1745],  5.00th=[ 1860], 10.00th=[ 1942], 20.00th=[ 2040],
     | 30.00th=[ 2114], 40.00th=[ 2180], 50.00th=[ 2278], 60.00th=[ 2376],
     | 70.00th=[ 2638], 80.00th=[ 3982], 90.00th=[ 4686], 95.00th=[ 5342],
     | 99.00th=[ 6849], 99.50th=[ 7308], 99.90th=[ 7832], 99.95th=[ 8160],
     | 99.99th=[ 8979]
   bw (  KiB/s): min=314144, max=1013848, per=100.00%, avg=717416.92, stdev=52673.19, samples=476
   iops        : min=78536, max=253464, avg=179354.13, stdev=13168.30, samples=476
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=16.06%, 4=64.09%, 10=19.85%, 20=0.01%, 50=0.01%
  cpu          : usr=13.10%, sys=37.19%, ctx=1085426, majf=0, minf=1766
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=10751804,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=700MiB/s (734MB/s), 700MiB/s-700MiB/s (734MB/s-734MB/s), io=41.0GiB (44.0GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=179k, BW=700MiB/s (734MB/s)(41.0GiB/60001msec)
umount: /mnt/fsbench: not mounted.
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T22:57:57.lsvd-multi.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T22:57:57.lsvd-multi.rssd2.txt
Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=192k, BW=748MiB/s (784MB/s)(43.8GiB/60002msec)
Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=63.7k, BW=249MiB/s (261MB/s)(14.6GiB/60001msec)
Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=114k, BW=444MiB/s (465MB/s)(26.0GiB/60001msec)
Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=169k, BW=661MiB/s (693MB/s)(38.7GiB/60001msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=179k, BW=700MiB/s (734MB/s)(41.0GiB/60001msec)
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.4
[0m[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.3
[0m+ exit
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.2
[0m[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.1
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0mflush thread (7f17097f2640) exiting
flush thread (7f171eff5640) exiting
flush thread (7f173bfff640) exiting
flush thread (7f175d7f2640) exiting
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
++ date +%FT%T
+ cur_time=2024-01-14T23:06:47
+ default_cache_size=128849018880
+ cache_size=128849018880
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=120
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:06:47.lsvd-multi.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=10g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC nvme.cc
CC mkcache.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-14T23:06:47
+ create_lsvd_thick triple-hdd lsvd-benchmark.multi.1 10g
+ local pool=triple-hdd
+ create_lsvd_thick triple-hdd lsvd-benchmark.multi.2 10g
+ local img=lsvd-benchmark.multi.1
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ local pool=triple-hdd
+ ./remove_objs.py triple-hdd lsvd-benchmark.multi.1
+ local img=lsvd-benchmark.multi.2
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark.multi.2
+ create_lsvd_thick triple-hdd lsvd-benchmark.multi.3 10g
+ wait
+ create_lsvd_thick triple-hdd lsvd-benchmark.multi.4 10g
+ local pool=triple-hdd
+ local img=lsvd-benchmark.multi.3
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ local pool=triple-hdd
+ local img=lsvd-benchmark.multi.4
+ local size=10g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark.multi.3
+ ./remove_objs.py triple-hdd lsvd-benchmark.multi.4
Removing all objects from pool triple-hdd with prefix lsvd-benchmark.multi.2
+Removing all objects from pool triple-hdd with prefix lsvd-benchmark.multi.1
+Removing all objects from pool triple-hdd with prefix lsvd-benchmark.multi.3
+Removing all objects from pool triple-hdd with prefix lsvd-benchmark.multi.4
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 0/56871 objects
+ ./thick-image --size=10g triple-hdd/lsvd-benchmark.multi.2
+
Removed 0/56871 objects
+ ./thick-image --size=10g triple-hdd/lsvd-benchmark.multi.1
+
Removed 0/56871 objects
+ ./thick-image --size=10g triple-hdd/lsvd-benchmark.multi.3
+
Removed 0/56871 objects
+ ./thick-image --size=10g triple-hdd/lsvd-benchmark.multi.4
Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 6% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 7% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 8% complete...Thick provisioning: 10% complete...Thick provisioning: 10% complete...Thick provisioning: 9% complete...Thick provisioning: 11% complete...Thick provisioning: 11% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 12% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 13% complete...Thick provisioning: 13% complete...Thick provisioning: 15% complete...Thick provisioning: 12% complete...Thick provisioning: 14% complete...Thick provisioning: 13% complete...Thick provisioning: 16% complete...Thick provisioning: 15% complete...Thick provisioning: 14% complete...Thick provisioning: 14% complete...Thick provisioning: 16% complete...Thick provisioning: 15% complete...Thick provisioning: 15% complete...Thick provisioning: 17% complete...Thick provisioning: 17% complete...Thick provisioning: 16% complete...Thick provisioning: 18% complete...Thick provisioning: 18% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 19% complete...Thick provisioning: 17% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 18% complete...Thick provisioning: 18% complete...Thick provisioning: 20% complete...Thick provisioning: 19% complete...Thick provisioning: 19% complete...Thick provisioning: 21% complete...Thick provisioning: 21% complete...Thick provisioning: 20% complete...Thick provisioning: 20% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 21% complete...Thick provisioning: 21% complete...Thick provisioning: 23% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 23% complete...Thick provisioning: 25% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 26% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 25% complete...Thick provisioning: 24% complete...Thick provisioning: 26% complete...Thick provisioning: 25% complete...Thick provisioning: 27% complete...Thick provisioning: 26% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 27% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 28% complete...Thick provisioning: 28% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 29% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 30% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 33% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 34% complete...Thick provisioning: 33% complete...Thick provisioning: 35% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 34% complete...Thick provisioning: 36% complete...Thick provisioning: 35% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 36% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 37% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 38% complete...Thick provisioning: 38% complete...Thick provisioning: 38% complete...Thick provisioning: 39% complete...Thick provisioning: 39% complete...Thick provisioning: 39% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 40% complete...Thick provisioning: 40% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 41% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 42% complete...Thick provisioning: 41% complete...Thick provisioning: 43% complete...Thick provisioning: 42% complete...Thick provisioning: 44% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 45% complete...Thick provisioning: 43% complete...Thick provisioning: 45% complete...Thick provisioning: 44% complete...Thick provisioning: 46% complete...Thick provisioning: 44% complete...Thick provisioning: 46% complete...Thick provisioning: 45% complete...Thick provisioning: 47% complete...Thick provisioning: 45% complete...Thick provisioning: 47% complete...Thick provisioning: 46% complete...Thick provisioning: 48% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 46% complete...Thick provisioning: 49% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 50% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 48% complete...Thick provisioning: 51% complete...Thick provisioning: 50% complete...Thick provisioning: 49% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 50% complete...Thick provisioning: 53% complete...Thick provisioning: 52% complete...Thick provisioning: 51% complete...Thick provisioning: 54% complete...Thick provisioning: 53% complete...Thick provisioning: 52% complete...Thick provisioning: 55% complete...Thick provisioning: 54% complete...Thick provisioning: 53% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 56% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 54% complete...Thick provisioning: 57% complete...Thick provisioning: 55% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 56% complete...Thick provisioning: 55% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 57% complete...Thick provisioning: 56% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 60% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 61% complete...Thick provisioning: 61% complete...Thick provisioning: 60% complete...Thick provisioning: 59% complete...Thick provisioning: 62% complete...Thick provisioning: 62% complete...Thick provisioning: 61% complete...Thick provisioning: 60% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 61% complete...Thick provisioning: 63% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 64% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 65% complete...Thick provisioning: 65% complete...Thick provisioning: 64% complete...Thick provisioning: 66% complete...Thick provisioning: 66% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 69% complete...Thick provisioning: 71% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 70% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 71% complete...Thick provisioning: 72% complete...Thick provisioning: 72% complete...Thick provisioning: 72% complete...Thick provisioning: 71% complete...Thick provisioning: 73% complete...Thick provisioning: 73% complete...Thick provisioning: 73% complete...Thick provisioning: 72% complete...Thick provisioning: 74% complete...Thick provisioning: 74% complete...Thick provisioning: 73% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 75% complete...Thick provisioning: 74% complete...Thick provisioning: 76% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 79% complete...Thick provisioning: 78% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 80% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 81% complete...Thick provisioning: 80% complete...Thick provisioning: 80% complete...Thick provisioning: 79% complete...Thick provisioning: 82% complete...Thick provisioning: 81% complete...Thick provisioning: 81% complete...Thick provisioning: 83% complete...Thick provisioning: 82% complete...Thick provisioning: 80% complete...Thick provisioning: 82% complete...Thick provisioning: 84% complete...Thick provisioning: 81% complete...Thick provisioning: 85% complete...Thick provisioning: 83% complete...Thick provisioning: 86% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 84% complete...Thick provisioning: 83% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 84% complete...Thick provisioning: 87% complete...Thick provisioning: 85% complete...Thick provisioning: 88% complete...Thick provisioning: 87% complete...Thick provisioning: 89% complete...Thick provisioning: 86% complete...Thick provisioning: 88% complete...Thick provisioning: 87% complete...Thick provisioning: 90% complete...Thick provisioning: 85% complete...Thick provisioning: 89% complete...Thick provisioning: 86% complete...Thick provisioning: 88% complete...Thick provisioning: 91% complete...Thick provisioning: 90% complete...Thick provisioning: 89% complete...Thick provisioning: 87% complete...Thick provisioning: 92% complete...Thick provisioning: 91% complete...Thick provisioning: 93% complete...Thick provisioning: 90% complete...Thick provisioning: 94% complete...Thick provisioning: 88% complete...Thick provisioning: 92% complete...Thick provisioning: 95% complete...Thick provisioning: 93% complete...Thick provisioning: 91% complete...Thick provisioning: 89% complete...Thick provisioning: 90% complete...Thick provisioning: 92% complete...Thick provisioning: 96% complete...Thick provisioning: 94% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 91% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 92% complete...Thick provisioning: 97% complete...Thick provisioning: 95% complete...Thick provisioning: 98% complete...Thick provisioning: 97% complete...Thick provisioning: 96% complete...Thick provisioning: 93% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 94% complete...Thick provisioning: 100% complete...Thick provisioning: 99% complete...Thick provisioning: 97% complete...Thick provisioning: 95% complete...Thick provisioning: 100% complete...Thick provisioning: 96% complete...Thick provisioning: 98% complete...Thick provisioning: 97% complete...Thick provisioning: 100% complete...done
+ rados -p triple-hdd stat lsvd-benchmark.multi.1
triple-hdd/lsvd-benchmark.multi.1 mtime 2024-01-14T23:08:36.000000+0000, size 4096
Thick provisioning: 99% complete...Thick provisioning: 98% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p triple-hdd stat lsvd-benchmark.multi.2
triple-hdd/lsvd-benchmark.multi.2 mtime 2024-01-14T23:08:37.000000+0000, size 4096
Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p triple-hdd stat lsvd-benchmark.multi.4
triple-hdd/lsvd-benchmark.multi.4 mtime 2024-01-14T23:08:38.000000+0000, size 4096
Thick provisioning: 100% complete...done
+ rados -p triple-hdd stat lsvd-benchmark.multi.3
triple-hdd/lsvd-benchmark.multi.3 mtime 2024-01-14T23:08:38.000000+0000, size 4096
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/ 128849018880
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/
+ local cache_size=128849018880
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=128849018880
+ LSVD_CACHE_SIZE=128849018880
+ rm -rf /mnt/nvme-remote//lsvd-write/1eef8595-1ea7-47e3-884a-5f3029b5ea32.wcache /mnt/nvme-remote//lsvd-write/80557905-ecc4-4e33-afbc-67e41675f422.wcache /mnt/nvme-remote//lsvd-write/9bc7bcbe-3898-43f0-a63f-648ad26d155b.wcache /mnt/nvme-remote//lsvd-write/9c32f078-6bd0-45da-8527-a50676572343.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-14 23:08:45.643542] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-14 23:08:45.643663] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid806899 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-14 23:08:45.713332] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-14 23:08:45.862875] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-14 23:08:45.862967] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-14 23:08:45.863062] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-14 23:08:45.863067] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-14 23:08:51.313073] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-14 23:08:52.007689] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd lsvd-benchmark.multi.1
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark.multi.1
+ local bdev=bdev_lsvd-benchmark.multi.1
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark.multi.1 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.1
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 122880 MiB in 16 shards, 7680 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//0da4e91c-73ac-485f-9f25-8fecabf45688.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.1, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 23:09:05.128978] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.1 rbd disk to lun
bdev_lsvd-benchmark.multi.1
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.1
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 1/2 hits, 0 MiB read, 1.778 read amp, 2 total
[0m+ add_rbd_img triple-hdd lsvd-benchmark.multi.2
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark.multi.2
+ local bdev=bdev_lsvd-benchmark.multi.2
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark.multi.2 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.2
[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//2da42638-ced5-4036-a27e-8e73f1e81a9c.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.2, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 23:09:06.557222] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.2 rbd disk to lun
bdev_lsvd-benchmark.multi.2
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.2
+ add_rbd_img triple-hdd lsvd-benchmark.multi.3
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark.multi.3
+ local bdev=bdev_lsvd-benchmark.multi.3
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark.multi.3 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.3
[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//abb33856-e24c-415d-9661-d13abc846e28.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.3, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 23:09:08.143382] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.3 rbd disk to lun
bdev_lsvd-benchmark.multi.3
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.3
+ add_rbd_img triple-hdd lsvd-benchmark.multi.4
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark.multi.4
+ local bdev=bdev_lsvd-benchmark.multi.4
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark.multi.4 4096 -c rbd_cluster -b bdev_lsvd-benchmark.multi.4
[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//7ec5c67a-54f7-4fae-a685-6205f8856fe5.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark.multi.4, size 10737418240
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 23:09:09.647356] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark.multi.4 rbd disk to lun
bdev_lsvd-benchmark.multi.4
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark.multi.4
+ trap 'cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:06:47.lsvd-multi.triple-hdd.txt multi-client/client-bench-multi.bash read_entire_img=1
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:06:47.lsvd-multi.triple-hdd.txt
+ local benchscript=multi-client/client-bench-multi.bash
+ local additional_args=read_entire_img=1
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=1'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:06:47.lsvd-multi.triple-hdd.txt
===Starting client benchmark

NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
device: nvme1
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n2          SPDK00000000000001   SPDK_Controller1                         2          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n3          SPDK00000000000001   SPDK_Controller1                         3          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n4          SPDK00000000000001   SPDK_Controller1                         4          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
Using device /dev/nvme1n1
/dev/nvme1n2
/dev/nvme1n3
/dev/nvme1n4


===Reading entire image to warm cache===

60817408 bytes (61 MB, 58 MiB) copied, 1 s, 60.6 MB/s81788928 bytes (82 MB, 78 MiB) copied, 1 s, 81.2 MB/s59768832 bytes (60 MB, 57 MiB) copied, 1 s, 59.2 MB/s76546048 bytes (77 MB, 73 MiB) copied, 1 s, 73.9 MB/s137363456 bytes (137 MB, 131 MiB) copied, 2 s, 68.6 MB/s135266304 bytes (135 MB, 129 MiB) copied, 2 s, 67.5 MB/s144703488 bytes (145 MB, 138 MiB) copied, 2 s, 71.9 MB/s136314880 bytes (136 MB, 130 MiB) copied, 2 s, 66.4 MB/s213909504 bytes (214 MB, 204 MiB) copied, 3 s, 71.2 MB/s197132288 bytes (197 MB, 188 MiB) copied, 3 s, 65.6 MB/s211812352 bytes (212 MB, 202 MiB) copied, 3 s, 70.4 MB/s214958080 bytes (215 MB, 205 MiB) copied, 3 s, 71.3 MB/s295698432 bytes (296 MB, 282 MiB) copied, 4 s, 73.8 MB/s276824064 bytes (277 MB, 264 MiB) copied, 4 s, 69.1 MB/s292552704 bytes (293 MB, 279 MiB) copied, 4 s, 73.0 MB/s263192576 bytes (263 MB, 251 MiB) copied, 4 s, 64.9 MB/s377487360 bytes (377 MB, 360 MiB) copied, 5 s, 75.4 MB/s293601280 bytes (294 MB, 280 MiB) copied, 5 s, 58.6 MB/s337641472 bytes (338 MB, 322 MiB) copied, 5 s, 67.4 MB/s311427072 bytes (311 MB, 297 MiB) copied, 5 s, 62.0 MB/s417333248 bytes (417 MB, 398 MiB) copied, 6 s, 69.5 MB/s391118848 bytes (391 MB, 373 MiB) copied, 6 s, 65.1 MB/s370147328 bytes (370 MB, 353 MiB) copied, 6 s, 61.6 MB/s454033408 bytes (454 MB, 433 MiB) copied, 6 s, 75.5 MB/s483393536 bytes (483 MB, 461 MiB) copied, 7 s, 69.0 MB/s525336576 bytes (525 MB, 501 MiB) copied, 7 s, 75.0 MB/s439353344 bytes (439 MB, 419 MiB) copied, 7 s, 62.7 MB/s471859200 bytes (472 MB, 450 MiB) copied, 7 s, 67.3 MB/s564133888 bytes (564 MB, 538 MiB) copied, 8 s, 70.5 MB/s603979776 bytes (604 MB, 576 MiB) copied, 8 s, 75.5 MB/s525336576 bytes (525 MB, 501 MiB) copied, 8 s, 65.6 MB/s546308096 bytes (546 MB, 521 MiB) copied, 8 s, 68.2 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 18508/55960 hits, 2324 MiB read, 1.007 read amp, 55960 total
[0m683671552 bytes (684 MB, 652 MiB) copied, 9 s, 75.9 MB/s608174080 bytes (608 MB, 580 MiB) copied, 9 s, 67.5 MB/s623902720 bytes (624 MB, 595 MiB) copied, 9 s, 69.3 MB/s646971392 bytes (647 MB, 617 MiB) copied, 9 s, 71.4 MB/s708837376 bytes (709 MB, 676 MiB) copied, 10 s, 70.9 MB/s707788800 bytes (708 MB, 675 MiB) copied, 10 s, 70.7 MB/s674234368 bytes (674 MB, 643 MiB) copied, 10 s, 67.4 MB/s771751936 bytes (772 MB, 736 MiB) copied, 10 s, 77.1 MB/s794820608 bytes (795 MB, 758 MiB) copied, 11 s, 72.2 MB/s792723456 bytes (793 MB, 756 MiB) copied, 11 s, 72.0 MB/s846200832 bytes (846 MB, 807 MiB) copied, 11 s, 76.9 MB/s750780416 bytes (751 MB, 716 MiB) copied, 11 s, 68.1 MB/s876609536 bytes (877 MB, 836 MiB) copied, 12 s, 73.0 MB/s921698304 bytes (922 MB, 879 MiB) copied, 12 s, 76.7 MB/s875560960 bytes (876 MB, 835 MiB) copied, 12 s, 72.8 MB/s817889280 bytes (818 MB, 780 MiB) copied, 12 s, 68.0 MB/s951058432 bytes (951 MB, 907 MiB) copied, 13 s, 73.2 MB/s960495616 bytes (960 MB, 916 MiB) copied, 13 s, 73.8 MB/s870318080 bytes (870 MB, 830 MiB) copied, 13 s, 66.9 MB/s999292928 bytes (999 MB, 953 MiB) copied, 13 s, 76.8 MB/s935329792 bytes (935 MB, 892 MiB) copied, 14 s, 66.8 MB/s1052770304 bytes (1.1 GB, 1004 MiB) copied, 14 s, 75.2 MB/s1064304640 bytes (1.1 GB, 1015 MiB) copied, 14 s, 76.0 MB/s1028653056 bytes (1.0 GB, 981 MiB) copied, 14 s, 73.4 MB/s1015021568 bytes (1.0 GB, 968 MiB) copied, 15 s, 67.6 MB/s1107296256 bytes (1.1 GB, 1.0 GiB) copied, 15 s, 73.8 MB/s 1086324736 bytes (1.1 GB, 1.0 GiB) copied, 15 s, 72.4 MB/s1130364928 bytes (1.1 GB, 1.1 GiB) copied, 15 s, 75.3 MB/s 1196425216 bytes (1.2 GB, 1.1 GiB) copied, 16 s, 74.7 MB/s1092616192 bytes (1.1 GB, 1.0 GiB) copied, 16 s, 68.2 MB/s1158676480 bytes (1.2 GB, 1.1 GiB) copied, 16 s, 72.3 MB/s1150287872 bytes (1.2 GB, 1.1 GiB) copied, 16 s, 71.8 MB/s1277165568 bytes (1.3 GB, 1.2 GiB) copied, 17 s, 75.1 MB/s1216348160 bytes (1.2 GB, 1.1 GiB) copied, 17 s, 71.5 MB/s1177550848 bytes (1.2 GB, 1.1 GiB) copied, 17 s, 69.2 MB/s1209008128 bytes (1.2 GB, 1.1 GiB) copied, 17 s, 71.0 MB/s1248854016 bytes (1.2 GB, 1.2 GiB) copied, 18 s, 69.4 MB/s1274019840 bytes (1.3 GB, 1.2 GiB) copied, 18 s, 70.8 MB/s1291845632 bytes (1.3 GB, 1.2 GiB) copied, 18 s, 71.8 MB/s1366294528 bytes (1.4 GB, 1.3 GiB) copied, 18 s, 75.9 MB/s1421869056 bytes (1.4 GB, 1.3 GiB) copied, 19 s, 74.8 MB/s1339031552 bytes (1.3 GB, 1.2 GiB) copied, 19 s, 70.4 MB/s1376780288 bytes (1.4 GB, 1.3 GiB) copied, 19 s, 72.4 MB/s1334837248 bytes (1.3 GB, 1.2 GiB) copied, 19 s, 70.2 MB/s1498415104 bytes (1.5 GB, 1.4 GiB) copied, 20 s, 74.9 MB/s1410334720 bytes (1.4 GB, 1.3 GiB) copied, 20 s, 70.5 MB/s1415577600 bytes (1.4 GB, 1.3 GiB) copied, 20 s, 70.8 MB/s1452277760 bytes (1.5 GB, 1.4 GiB) copied, 20 s, 72.6 MB/s1585446912 bytes (1.6 GB, 1.5 GiB) copied, 21 s, 75.5 MB/s1490026496 bytes (1.5 GB, 1.4 GiB) copied, 21 s, 70.9 MB/s1536163840 bytes (1.5 GB, 1.4 GiB) copied, 21 s, 73.1 MB/s1496317952 bytes (1.5 GB, 1.4 GiB) copied, 21 s, 71.2 MB/s1571815424 bytes (1.6 GB, 1.5 GiB) copied, 22 s, 71.4 MB/s1570766848 bytes (1.6 GB, 1.5 GiB) copied, 22 s, 71.4 MB/s1612709888 bytes (1.6 GB, 1.5 GiB) copied, 22 s, 73.2 MB/s1672478720 bytes (1.7 GB, 1.6 GiB) copied, 22 s, 75.9 MB/s1654652928 bytes (1.7 GB, 1.5 GiB) copied, 23 s, 71.9 MB/s1741684736 bytes (1.7 GB, 1.6 GiB) copied, 23 s, 75.7 MB/s1669332992 bytes (1.7 GB, 1.6 GiB) copied, 23 s, 72.6 MB/s1644167168 bytes (1.6 GB, 1.5 GiB) copied, 23 s, 71.5 MB/s1743781888 bytes (1.7 GB, 1.6 GiB) copied, 24 s, 72.7 MB/s1736441856 bytes (1.7 GB, 1.6 GiB) copied, 24 s, 72.3 MB/s1721761792 bytes (1.7 GB, 1.6 GiB) copied, 24 s, 71.7 MB/s1826619392 bytes (1.8 GB, 1.7 GiB) copied, 24 s, 76.1 MB/s1818230784 bytes (1.8 GB, 1.7 GiB) copied, 25 s, 72.7 MB/s1796210688 bytes (1.8 GB, 1.7 GiB) copied, 25 s, 71.8 MB/s1909456896 bytes (1.9 GB, 1.8 GiB) copied, 25 s, 76.3 MB/s1804599296 bytes (1.8 GB, 1.7 GiB) copied, 25 s, 72.1 MB/s1897922560 bytes (1.9 GB, 1.8 GiB) copied, 26 s, 73.0 MB/s1983905792 bytes (2.0 GB, 1.8 GiB) copied, 26 s, 76.3 MB/s1871708160 bytes (1.9 GB, 1.7 GiB) copied, 26 s, 72.0 MB/s1885339648 bytes (1.9 GB, 1.8 GiB) copied, 26 s, 72.5 MB/s2063597568 bytes (2.1 GB, 1.9 GiB) copied, 27 s, 76.4 MB/s1951399936 bytes (2.0 GB, 1.8 GiB) copied, 27 s, 72.3 MB/s1950351360 bytes (2.0 GB, 1.8 GiB) copied, 27 s, 72.2 MB/s1971322880 bytes (2.0 GB, 1.8 GiB) copied, 27 s, 73.0 MB/s2036334592 bytes (2.0 GB, 1.9 GiB) copied, 28 s, 72.7 MB/s2018508800 bytes (2.0 GB, 1.9 GiB) copied, 28 s, 72.1 MB/s2026897408 bytes (2.0 GB, 1.9 GiB) copied, 28 s, 72.4 MB/s2125463552 bytes (2.1 GB, 2.0 GiB) copied, 28 s, 75.8 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52496/160000 hits, 6667 MiB read, 1.008 read amp, 192064 total
[0m2113929216 bytes (2.1 GB, 2.0 GiB) copied, 29 s, 72.9 MB/s2117074944 bytes (2.1 GB, 2.0 GiB) copied, 29 s, 73.0 MB/s2188378112 bytes (2.2 GB, 2.0 GiB) copied, 29 s, 75.4 MB/s2089811968 bytes (2.1 GB, 1.9 GiB) copied, 29 s, 71.9 MB/s2164260864 bytes (2.2 GB, 2.0 GiB) copied, 30 s, 72.1 MB/s2196766720 bytes (2.2 GB, 2.0 GiB) copied, 30 s, 73.2 MB/s2195718144 bytes (2.2 GB, 2.0 GiB) copied, 30 s, 73.2 MB/s2274361344 bytes (2.3 GB, 2.1 GiB) copied, 30 s, 75.8 MB/s2263875584 bytes (2.3 GB, 2.1 GiB) copied, 31 s, 73.0 MB/s2245001216 bytes (2.2 GB, 2.1 GiB) copied, 31 s, 72.4 MB/s2348810240 bytes (2.3 GB, 2.2 GiB) copied, 31 s, 75.7 MB/s2278555648 bytes (2.3 GB, 2.1 GiB) copied, 31 s, 73.5 MB/s2353004544 bytes (2.4 GB, 2.2 GiB) copied, 32 s, 73.5 MB/s2289041408 bytes (2.3 GB, 2.1 GiB) copied, 32 s, 71.5 MB/s2425356288 bytes (2.4 GB, 2.3 GiB) copied, 32 s, 75.8 MB/s2323644416 bytes (2.3 GB, 2.2 GiB) copied, 32 s, 72.5 MB/s2383413248 bytes (2.4 GB, 2.2 GiB) copied, 33 s, 72.2 MB/s2361393152 bytes (2.4 GB, 2.2 GiB) copied, 33 s, 71.6 MB/s2429550592 bytes (2.4 GB, 2.3 GiB) copied, 33 s, 73.6 MB/s2503999488 bytes (2.5 GB, 2.3 GiB) copied, 33 s, 75.9 MB/s2447376384 bytes (2.4 GB, 2.3 GiB) copied, 34 s, 72.0 MB/s2422210560 bytes (2.4 GB, 2.3 GiB) copied, 34 s, 71.2 MB/s2576351232 bytes (2.6 GB, 2.4 GiB) copied, 34 s, 75.7 MB/s2501902336 bytes (2.5 GB, 2.3 GiB) copied, 34 s, 73.5 MB/s2500853760 bytes (2.5 GB, 2.3 GiB) copied, 35 s, 71.4 MB/s2644508672 bytes (2.6 GB, 2.5 GiB) copied, 35 s, 75.5 MB/s2496659456 bytes (2.5 GB, 2.3 GiB) copied, 35 s, 71.3 MB/s2572156928 bytes (2.6 GB, 2.4 GiB) copied, 35 s, 73.5 MB/s2709520384 bytes (2.7 GB, 2.5 GiB) copied, 36 s, 75.3 MB/s2617245696 bytes (2.6 GB, 2.4 GiB) copied, 36 s, 72.7 MB/s2576351232 bytes (2.6 GB, 2.4 GiB) copied, 36 s, 71.5 MB/s2559574016 bytes (2.6 GB, 2.4 GiB) copied, 36 s, 71.0 MB/s2788163584 bytes (2.8 GB, 2.6 GiB) copied, 37 s, 75.3 MB/s2696937472 bytes (2.7 GB, 2.5 GiB) copied, 37 s, 72.9 MB/s2631925760 bytes (2.6 GB, 2.5 GiB) copied, 37 s, 71.1 MB/s2660237312 bytes (2.7 GB, 2.5 GiB) copied, 37 s, 71.8 MB/s2758803456 bytes (2.8 GB, 2.6 GiB) copied, 38 s, 72.6 MB/s2850029568 bytes (2.9 GB, 2.7 GiB) copied, 38 s, 75.0 MB/s2740977664 bytes (2.7 GB, 2.6 GiB) copied, 38 s, 72.1 MB/s2713714688 bytes (2.7 GB, 2.5 GiB) copied, 38 s, 71.4 MB/s2840592384 bytes (2.8 GB, 2.6 GiB) copied, 39 s, 72.8 MB/s2788163584 bytes (2.8 GB, 2.6 GiB) copied, 39 s, 71.5 MB/s2920284160 bytes (2.9 GB, 2.7 GiB) copied, 39 s, 74.8 MB/s2811232256 bytes (2.8 GB, 2.6 GiB) copied, 39 s, 72.0 MB/s2871001088 bytes (2.9 GB, 2.7 GiB) copied, 40 s, 71.8 MB/s2853175296 bytes (2.9 GB, 2.7 GiB) copied, 40 s, 71.3 MB/s2989490176 bytes (3.0 GB, 2.8 GiB) copied, 40 s, 74.7 MB/s2924478464 bytes (2.9 GB, 2.7 GiB) copied, 40 s, 73.1 MB/s2950692864 bytes (3.0 GB, 2.7 GiB) copied, 41 s, 72.0 MB/s3072327680 bytes (3.1 GB, 2.9 GiB) copied, 41 s, 74.9 MB/s2915041280 bytes (2.9 GB, 2.7 GiB) copied, 41 s, 71.1 MB/s2998927360 bytes (3.0 GB, 2.8 GiB) copied, 41 s, 73.1 MB/s3013607424 bytes (3.0 GB, 2.8 GiB) copied, 42 s, 71.8 MB/s2992635904 bytes (3.0 GB, 2.8 GiB) copied, 42 s, 71.2 MB/s3148873728 bytes (3.1 GB, 2.9 GiB) copied, 42 s, 75.0 MB/s3055550464 bytes (3.1 GB, 2.8 GiB) copied, 42 s, 72.7 MB/s3075473408 bytes (3.1 GB, 2.9 GiB) copied, 43 s, 71.5 MB/s3139436544 bytes (3.1 GB, 2.9 GiB) copied, 43 s, 73.0 MB/s3230662656 bytes (3.2 GB, 3.0 GiB) copied, 43 s, 75.1 MB/s3096444928 bytes (3.1 GB, 2.9 GiB) copied, 43 s, 72.0 MB/s3207593984 bytes (3.2 GB, 3.0 GiB) copied, 44 s, 72.9 MB/s3175088128 bytes (3.2 GB, 3.0 GiB) copied, 44 s, 72.1 MB/s3309305856 bytes (3.3 GB, 3.1 GiB) copied, 44 s, 75.2 MB/s3143630848 bytes (3.1 GB, 2.9 GiB) copied, 44 s, 71.4 MB/s3380609024 bytes (3.4 GB, 3.1 GiB) copied, 45 s, 75.1 MB/s3275751424 bytes (3.3 GB, 3.1 GiB) copied, 45 s, 72.8 MB/s3223322624 bytes (3.2 GB, 3.0 GiB) copied, 45 s, 71.6 MB/s3255828480 bytes (3.3 GB, 3.0 GiB) copied, 45 s, 72.3 MB/s3335520256 bytes (3.3 GB, 3.1 GiB) copied, 46 s, 72.5 MB/s3451912192 bytes (3.5 GB, 3.2 GiB) copied, 46 s, 75.0 MB/s3298820096 bytes (3.3 GB, 3.1 GiB) copied, 46 s, 71.7 MB/s3331325952 bytes (3.3 GB, 3.1 GiB) copied, 46 s, 72.4 MB/s3378511872 bytes (3.4 GB, 3.1 GiB) copied, 47 s, 71.9 MB/s3526361088 bytes (3.5 GB, 3.3 GiB) copied, 47 s, 75.0 MB/s3411017728 bytes (3.4 GB, 3.2 GiB) copied, 47 s, 72.6 MB/s3409969152 bytes (3.4 GB, 3.2 GiB) copied, 47 s, 72.5 MB/s3482320896 bytes (3.5 GB, 3.2 GiB) copied, 48 s, 72.5 MB/s3446669312 bytes (3.4 GB, 3.2 GiB) copied, 48 s, 71.8 MB/s3599761408 bytes (3.6 GB, 3.4 GiB) copied, 48 s, 75.0 MB/s3490709504 bytes (3.5 GB, 3.3 GiB) copied, 48 s, 72.7 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6667 MiB read, 1.008 read amp, 324580 total
[0m3666870272 bytes (3.7 GB, 3.4 GiB) copied, 49 s, 74.8 MB/s3542089728 bytes (3.5 GB, 3.3 GiB) copied, 49 s, 72.3 MB/s3516923904 bytes (3.5 GB, 3.3 GiB) copied, 49 s, 71.7 MB/s3570401280 bytes (3.6 GB, 3.3 GiB) copied, 49 s, 72.8 MB/s3638558720 bytes (3.6 GB, 3.4 GiB) copied, 50 s, 72.8 MB/s3592421376 bytes (3.6 GB, 3.3 GiB) copied, 50 s, 71.8 MB/s3742367744 bytes (3.7 GB, 3.5 GiB) copied, 50 s, 74.8 MB/s3625975808 bytes (3.6 GB, 3.4 GiB) copied, 50 s, 72.5 MB/s3663724544 bytes (3.7 GB, 3.4 GiB) copied, 51 s, 71.8 MB/s3825205248 bytes (3.8 GB, 3.6 GiB) copied, 51 s, 75.0 MB/s3703570432 bytes (3.7 GB, 3.4 GiB) copied, 51 s, 72.6 MB/s3724541952 bytes (3.7 GB, 3.5 GiB) copied, 51 s, 73.0 MB/s3762290688 bytes (3.8 GB, 3.5 GiB) copied, 52 s, 72.4 MB/s3900702720 bytes (3.9 GB, 3.6 GiB) copied, 52 s, 75.0 MB/s3787456512 bytes (3.8 GB, 3.5 GiB) copied, 52 s, 72.8 MB/s3715104768 bytes (3.7 GB, 3.5 GiB) copied, 52 s, 71.4 MB/s3965714432 bytes (4.0 GB, 3.7 GiB) copied, 53 s, 74.8 MB/s3787456512 bytes (3.8 GB, 3.5 GiB) copied, 53 s, 71.5 MB/s3836739584 bytes (3.8 GB, 3.6 GiB) copied, 53 s, 72.4 MB/s3856662528 bytes (3.9 GB, 3.6 GiB) copied, 53 s, 72.8 MB/s3896508416 bytes (3.9 GB, 3.6 GiB) copied, 54 s, 72.2 MB/s4025483264 bytes (4.0 GB, 3.7 GiB) copied, 54 s, 74.5 MB/s3917479936 bytes (3.9 GB, 3.6 GiB) copied, 54 s, 72.5 MB/s3859808256 bytes (3.9 GB, 3.6 GiB) copied, 54 s, 71.5 MB/s3930062848 bytes (3.9 GB, 3.7 GiB) copied, 55 s, 71.4 MB/s4085252096 bytes (4.1 GB, 3.8 GiB) copied, 55 s, 74.3 MB/s3975151616 bytes (4.0 GB, 3.7 GiB) copied, 55 s, 72.3 MB/s3968860160 bytes (4.0 GB, 3.7 GiB) copied, 55 s, 72.1 MB/s4008706048 bytes (4.0 GB, 3.7 GiB) copied, 56 s, 71.6 MB/s4018143232 bytes (4.0 GB, 3.7 GiB) copied, 56 s, 71.7 MB/s4056940544 bytes (4.1 GB, 3.8 GiB) copied, 56 s, 72.4 MB/s4127195136 bytes (4.1 GB, 3.8 GiB) copied, 56 s, 73.7 MB/s4088397824 bytes (4.1 GB, 3.8 GiB) copied, 57 s, 71.7 MB/s4085252096 bytes (4.1 GB, 3.8 GiB) copied, 57 s, 71.7 MB/s4211081216 bytes (4.2 GB, 3.9 GiB) copied, 57 s, 73.9 MB/s4128243712 bytes (4.1 GB, 3.8 GiB) copied, 57 s, 72.4 MB/s4264558592 bytes (4.3 GB, 4.0 GiB) copied, 58 s, 73.5 MB/s4204789760 bytes (4.2 GB, 3.9 GiB) copied, 58 s, 72.5 MB/s4151312384 bytes (4.2 GB, 3.9 GiB) copied, 58 s, 71.6 MB/s4149215232 bytes (4.1 GB, 3.9 GiB) copied, 58 s, 71.5 MB/s4192206848 bytes (4.2 GB, 3.9 GiB) copied, 59 s, 71.1 MB/s4314890240 bytes (4.3 GB, 4.0 GiB) copied, 59 s, 73.1 MB/s4218421248 bytes (4.2 GB, 3.9 GiB) copied, 59 s, 71.5 MB/s4276092928 bytes (4.3 GB, 4.0 GiB) copied, 59 s, 72.5 MB/s4272947200 bytes (4.3 GB, 4.0 GiB) copied, 60 s, 71.2 MB/s4297064448 bytes (4.3 GB, 4.0 GiB) copied, 60 s, 71.6 MB/s4363124736 bytes (4.4 GB, 4.1 GiB) copied, 60 s, 72.7 MB/s4388290560 bytes (4.4 GB, 4.1 GiB) copied, 60 s, 73.1 MB/s4461690880 bytes (4.5 GB, 4.2 GiB) copied, 61 s, 73.1 MB/s4359979008 bytes (4.4 GB, 4.1 GiB) copied, 61 s, 71.5 MB/s4356833280 bytes (4.4 GB, 4.1 GiB) copied, 61 s, 71.4 MB/s4421844992 bytes (4.4 GB, 4.1 GiB) copied, 61 s, 72.5 MB/s4404019200 bytes (4.4 GB, 4.1 GiB) copied, 62 s, 71.0 MB/s4487905280 bytes (4.5 GB, 4.2 GiB) copied, 62 s, 72.4 MB/s4512022528 bytes (4.5 GB, 4.2 GiB) copied, 62 s, 72.8 MB/s4427087872 bytes (4.4 GB, 4.1 GiB) copied, 62 s, 71.4 MB/s4534042624 bytes (4.5 GB, 4.2 GiB) copied, 63 s, 72.0 MB/s4491051008 bytes (4.5 GB, 4.2 GiB) copied, 63 s, 71.3 MB/s4469030912 bytes (4.5 GB, 4.2 GiB) copied, 63 s, 70.9 MB/s4551868416 bytes (4.6 GB, 4.2 GiB) copied, 63 s, 72.2 MB/s4541382656 bytes (4.5 GB, 4.2 GiB) copied, 64 s, 71.0 MB/s4573888512 bytes (4.6 GB, 4.3 GiB) copied, 64 s, 71.5 MB/s4603248640 bytes (4.6 GB, 4.3 GiB) copied, 64 s, 71.9 MB/s4614782976 bytes (4.6 GB, 4.3 GiB) copied, 64 s, 72.1 MB/s4681891840 bytes (4.7 GB, 4.4 GiB) copied, 65 s, 72.0 MB/s4631560192 bytes (4.6 GB, 4.3 GiB) copied, 65 s, 71.2 MB/s4668260352 bytes (4.7 GB, 4.3 GiB) copied, 65 s, 71.8 MB/s4591714304 bytes (4.6 GB, 4.3 GiB) copied, 65 s, 70.6 MB/s4726980608 bytes (4.7 GB, 4.4 GiB) copied, 66 s, 71.6 MB/s4725932032 bytes (4.7 GB, 4.4 GiB) copied, 66 s, 71.6 MB/s4709154816 bytes (4.7 GB, 4.4 GiB) copied, 66 s, 71.3 MB/s4653580288 bytes (4.7 GB, 4.3 GiB) copied, 66 s, 70.5 MB/s4803526656 bytes (4.8 GB, 4.5 GiB) copied, 67 s, 71.7 MB/s4788846592 bytes (4.8 GB, 4.5 GiB) copied, 67 s, 71.4 MB/s4784652288 bytes (4.8 GB, 4.5 GiB) copied, 67 s, 71.4 MB/s4719640576 bytes (4.7 GB, 4.4 GiB) copied, 67 s, 70.4 MB/s4750049280 bytes (4.8 GB, 4.4 GiB) copied, 68 s, 69.8 MB/s4841275392 bytes (4.8 GB, 4.5 GiB) copied, 68 s, 71.2 MB/s4825546752 bytes (4.8 GB, 4.5 GiB) copied, 68 s, 71.0 MB/s4849664000 bytes (4.8 GB, 4.5 GiB) copied, 68 s, 71.3 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52512/160000 hits, 6666 MiB read, 1.008 read amp, 442582 total
[0m4896849920 bytes (4.9 GB, 4.6 GiB) copied, 69 s, 71.0 MB/s4854906880 bytes (4.9 GB, 4.5 GiB) copied, 69 s, 70.4 MB/s4857004032 bytes (4.9 GB, 4.5 GiB) copied, 69 s, 70.4 MB/s4763680768 bytes (4.8 GB, 4.4 GiB) copied, 69 s, 69.0 MB/s4918870016 bytes (4.9 GB, 4.6 GiB) copied, 70 s, 70.3 MB/s4829741056 bytes (4.8 GB, 4.5 GiB) copied, 70 s, 69.0 MB/s4963958784 bytes (5.0 GB, 4.6 GiB) copied, 70 s, 70.9 MB/s4928307200 bytes (4.9 GB, 4.6 GiB) copied, 70 s, 70.4 MB/s4892655616 bytes (4.9 GB, 4.6 GiB) copied, 71 s, 68.9 MB/s5006950400 bytes (5.0 GB, 4.7 GiB) copied, 71 s, 70.5 MB/s5000658944 bytes (5.0 GB, 4.7 GiB) copied, 71 s, 70.4 MB/s5050990592 bytes (5.1 GB, 4.7 GiB) copied, 71 s, 71.1 MB/s5131730944 bytes (5.1 GB, 4.8 GiB) copied, 72 s, 71.3 MB/s5087690752 bytes (5.1 GB, 4.7 GiB) copied, 72 s, 70.7 MB/s4981784576 bytes (5.0 GB, 4.6 GiB) copied, 72 s, 69.2 MB/s5071962112 bytes (5.1 GB, 4.7 GiB) copied, 72 s, 70.4 MB/s5168431104 bytes (5.2 GB, 4.8 GiB) copied, 73 s, 70.8 MB/s5148508160 bytes (5.1 GB, 4.8 GiB) copied, 73 s, 70.5 MB/s5057282048 bytes (5.1 GB, 4.7 GiB) copied, 73 s, 69.3 MB/s5191499776 bytes (5.2 GB, 4.8 GiB) copied, 73 s, 71.1 MB/s5218762752 bytes (5.2 GB, 4.9 GiB) copied, 74 s, 70.5 MB/s5126488064 bytes (5.1 GB, 4.8 GiB) copied, 74 s, 69.3 MB/s5264900096 bytes (5.3 GB, 4.9 GiB) copied, 74 s, 71.1 MB/s5218762752 bytes (5.2 GB, 4.9 GiB) copied, 74 s, 70.5 MB/s5281677312 bytes (5.3 GB, 4.9 GiB) copied, 75 s, 70.4 MB/s5191499776 bytes (5.2 GB, 4.8 GiB) copied, 75 s, 69.2 MB/s5335154688 bytes (5.3 GB, 5.0 GiB) copied, 75 s, 71.1 MB/s5277483008 bytes (5.3 GB, 4.9 GiB) copied, 75 s, 70.4 MB/s5365563392 bytes (5.4 GB, 5.0 GiB) copied, 76 s, 70.6 MB/s5277483008 bytes (5.3 GB, 4.9 GiB) copied, 76 s, 69.4 MB/s5422186496 bytes (5.4 GB, 5.0 GiB) copied, 76 s, 71.3 MB/s5352980480 bytes (5.4 GB, 5.0 GiB) copied, 76 s, 70.4 MB/s5450498048 bytes (5.5 GB, 5.1 GiB) copied, 77 s, 70.8 MB/s5321523200 bytes (5.3 GB, 5.0 GiB) copied, 77 s, 69.1 MB/s5466226688 bytes (5.5 GB, 5.1 GiB) copied, 77 s, 71.0 MB/s5421137920 bytes (5.4 GB, 5.0 GiB) copied, 77 s, 70.3 MB/s5536481280 bytes (5.5 GB, 5.2 GiB) copied, 78 s, 71.0 MB/s5379194880 bytes (5.4 GB, 5.0 GiB) copied, 78 s, 69.0 MB/s5536481280 bytes (5.5 GB, 5.2 GiB) copied, 78 s, 71.0 MB/s5474615296 bytes (5.5 GB, 5.1 GiB) copied, 78 s, 70.2 MB/s5616173056 bytes (5.6 GB, 5.2 GiB) copied, 79 s, 71.1 MB/s5599395840 bytes (5.6 GB, 5.2 GiB) copied, 79 s, 70.9 MB/s5458886656 bytes (5.5 GB, 5.1 GiB) copied, 79 s, 69.1 MB/s5540675584 bytes (5.5 GB, 5.2 GiB) copied, 79 s, 70.1 MB/s5661261824 bytes (5.7 GB, 5.3 GiB) copied, 80 s, 70.8 MB/s5543821312 bytes (5.5 GB, 5.2 GiB) copied, 80 s, 69.3 MB/s5629804544 bytes (5.6 GB, 5.2 GiB) copied, 80 s, 70.4 MB/s5678039040 bytes (5.7 GB, 5.3 GiB) copied, 80 s, 71.0 MB/s5697961984 bytes (5.7 GB, 5.3 GiB) copied, 81 s, 70.3 MB/s5747245056 bytes (5.7 GB, 5.4 GiB) copied, 81 s, 71.0 MB/s5621415936 bytes (5.6 GB, 5.2 GiB) copied, 81 s, 69.4 MB/s5747245056 bytes (5.7 GB, 5.4 GiB) copied, 81 s, 70.9 MB/s5764022272 bytes (5.8 GB, 5.4 GiB) copied, 82 s, 70.3 MB/s5705302016 bytes (5.7 GB, 5.3 GiB) copied, 82 s, 69.6 MB/s5814353920 bytes (5.8 GB, 5.4 GiB) copied, 82 s, 70.9 MB/s5814353920 bytes (5.8 GB, 5.4 GiB) copied, 82 s, 70.9 MB/s5842665472 bytes (5.8 GB, 5.4 GiB) copied, 83 s, 70.4 MB/s5772410880 bytes (5.8 GB, 5.4 GiB) copied, 83 s, 69.5 MB/s5883559936 bytes (5.9 GB, 5.5 GiB) copied, 83 s, 70.9 MB/s5877268480 bytes (5.9 GB, 5.5 GiB) copied, 83 s, 70.8 MB/s5960105984 bytes (6.0 GB, 5.6 GiB) copied, 84 s, 70.9 MB/s5857345536 bytes (5.9 GB, 5.5 GiB) copied, 84 s, 69.7 MB/s5915017216 bytes (5.9 GB, 5.5 GiB) copied, 84 s, 70.4 MB/s5948571648 bytes (5.9 GB, 5.5 GiB) copied, 84 s, 70.8 MB/s6035603456 bytes (6.0 GB, 5.6 GiB) copied, 85 s, 71.0 MB/s6026166272 bytes (6.0 GB, 5.6 GiB) copied, 85 s, 70.9 MB/s5933891584 bytes (5.9 GB, 5.5 GiB) copied, 85 s, 69.8 MB/s5998903296 bytes (6.0 GB, 5.6 GiB) copied, 85 s, 70.5 MB/s6012534784 bytes (6.0 GB, 5.6 GiB) copied, 86 s, 69.9 MB/s6101663744 bytes (6.1 GB, 5.7 GiB) copied, 86 s, 70.9 MB/s6068109312 bytes (6.1 GB, 5.7 GiB) copied, 86 s, 70.5 MB/s6116343808 bytes (6.1 GB, 5.7 GiB) copied, 86 s, 71.1 MB/s6071255040 bytes (6.1 GB, 5.7 GiB) copied, 87 s, 69.8 MB/s6185549824 bytes (6.2 GB, 5.8 GiB) copied, 87 s, 71.1 MB/s6154092544 bytes (6.2 GB, 5.7 GiB) copied, 87 s, 70.7 MB/s6133121024 bytes (6.1 GB, 5.7 GiB) copied, 87 s, 70.5 MB/s6160384000 bytes (6.2 GB, 5.7 GiB) copied, 88 s, 70.0 MB/s6207569920 bytes (6.2 GB, 5.8 GiB) copied, 88 s, 70.5 MB/s6219104256 bytes (6.2 GB, 5.8 GiB) copied, 88 s, 70.7 MB/s6267338752 bytes (6.3 GB, 5.8 GiB) copied, 88 s, 71.2 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 573472 total
[0m6289358848 bytes (6.3 GB, 5.9 GiB) copied, 89 s, 70.7 MB/s6349127680 bytes (6.3 GB, 5.9 GiB) copied, 89 s, 71.3 MB/s6229590016 bytes (6.2 GB, 5.8 GiB) copied, 89 s, 70.0 MB/s6291456000 bytes (6.3 GB, 5.9 GiB) copied, 89 s, 70.7 MB/s6305087488 bytes (6.3 GB, 5.9 GiB) copied, 90 s, 70.1 MB/s6365904896 bytes (6.4 GB, 5.9 GiB) copied, 90 s, 70.7 MB/s6435110912 bytes (6.4 GB, 6.0 GiB) copied, 90 s, 71.5 MB/s6368002048 bytes (6.4 GB, 5.9 GiB) copied, 90 s, 70.7 MB/s6449790976 bytes (6.4 GB, 6.0 GiB) copied, 91 s, 70.9 MB/s6514802688 bytes (6.5 GB, 6.1 GiB) copied, 91 s, 71.6 MB/s6439305216 bytes (6.4 GB, 6.0 GiB) copied, 91 s, 70.8 MB/s6394216448 bytes (6.4 GB, 6.0 GiB) copied, 91 s, 70.3 MB/s6467616768 bytes (6.5 GB, 6.0 GiB) copied, 92 s, 70.3 MB/s6600785920 bytes (6.6 GB, 6.1 GiB) copied, 92 s, 71.7 MB/s6525288448 bytes (6.5 GB, 6.1 GiB) copied, 92 s, 70.9 MB/s6518996992 bytes (6.5 GB, 6.1 GiB) copied, 92 s, 70.8 MB/s6590300160 bytes (6.6 GB, 6.1 GiB) copied, 93 s, 70.9 MB/s6528434176 bytes (6.5 GB, 6.1 GiB) copied, 93 s, 70.2 MB/s6603931648 bytes (6.6 GB, 6.2 GiB) copied, 93 s, 71.0 MB/s6676283392 bytes (6.7 GB, 6.2 GiB) copied, 93 s, 71.8 MB/s6660554752 bytes (6.7 GB, 6.2 GiB) copied, 94 s, 70.9 MB/s6667894784 bytes (6.7 GB, 6.2 GiB) copied, 94 s, 70.9 MB/s6725566464 bytes (6.7 GB, 6.3 GiB) copied, 94 s, 71.5 MB/s6576668672 bytes (6.6 GB, 6.1 GiB) copied, 94 s, 69.9 MB/s6726615040 bytes (6.7 GB, 6.3 GiB) copied, 95 s, 70.8 MB/s6808403968 bytes (6.8 GB, 6.3 GiB) copied, 95 s, 71.7 MB/s6742343680 bytes (6.7 GB, 6.3 GiB) copied, 95 s, 71.0 MB/s6644826112 bytes (6.6 GB, 6.2 GiB) copied, 95 s, 69.9 MB/s6881804288 bytes (6.9 GB, 6.4 GiB) copied, 96 s, 71.7 MB/s6808403968 bytes (6.8 GB, 6.3 GiB) copied, 96 s, 70.9 MB/s6717177856 bytes (6.7 GB, 6.3 GiB) copied, 96 s, 70.0 MB/s6824132608 bytes (6.8 GB, 6.4 GiB) copied, 96 s, 71.1 MB/s6788481024 bytes (6.8 GB, 6.3 GiB) copied, 97 s, 70.0 MB/s6899630080 bytes (6.9 GB, 6.4 GiB) copied, 97 s, 71.1 MB/s6961496064 bytes (7.0 GB, 6.5 GiB) copied, 97 s, 71.8 MB/s6887047168 bytes (6.9 GB, 6.4 GiB) copied, 97 s, 71.0 MB/s6957301760 bytes (7.0 GB, 6.5 GiB) copied, 98 s, 71.0 MB/s7031750656 bytes (7.0 GB, 6.5 GiB) copied, 98 s, 71.7 MB/s6924795904 bytes (6.9 GB, 6.4 GiB) copied, 98 s, 70.6 MB/s6840909824 bytes (6.8 GB, 6.4 GiB) copied, 98 s, 69.8 MB/s6898581504 bytes (6.9 GB, 6.4 GiB) copied, 99 s, 69.7 MB/s7029653504 bytes (7.0 GB, 6.5 GiB) copied, 99 s, 71.0 MB/s7093616640 bytes (7.1 GB, 6.6 GiB) copied, 99 s, 71.6 MB/s6991904768 bytes (7.0 GB, 6.5 GiB) copied, 99 s, 70.6 MB/s7047479296 bytes (7.0 GB, 6.6 GiB) copied, 100 s, 70.5 MB/s7116685312 bytes (7.1 GB, 6.6 GiB) copied, 100 s, 71.2 MB/s7171211264 bytes (7.2 GB, 6.7 GiB) copied, 100 s, 71.7 MB/s6962544640 bytes (7.0 GB, 6.5 GiB) copied, 100 s, 69.6 MB/s7197425664 bytes (7.2 GB, 6.7 GiB) copied, 101 s, 71.3 MB/s7245660160 bytes (7.2 GB, 6.7 GiB) copied, 101 s, 71.7 MB/s7047479296 bytes (7.0 GB, 6.6 GiB) copied, 101 s, 69.8 MB/s7118782464 bytes (7.1 GB, 6.6 GiB) copied, 101 s, 70.5 MB/s7192182784 bytes (7.2 GB, 6.7 GiB) copied, 102 s, 70.5 MB/s7131365376 bytes (7.1 GB, 6.6 GiB) copied, 102 s, 69.9 MB/s7328497664 bytes (7.3 GB, 6.8 GiB) copied, 102 s, 71.8 MB/s7270825984 bytes (7.3 GB, 6.8 GiB) copied, 102 s, 71.3 MB/s7387217920 bytes (7.4 GB, 6.9 GiB) copied, 103 s, 71.7 MB/s7192182784 bytes (7.2 GB, 6.7 GiB) copied, 103 s, 69.8 MB/s7337934848 bytes (7.3 GB, 6.8 GiB) copied, 103 s, 71.2 MB/s7257194496 bytes (7.3 GB, 6.8 GiB) copied, 103 s, 70.4 MB/s7249854464 bytes (7.2 GB, 6.8 GiB) copied, 104 s, 69.7 MB/s7345274880 bytes (7.3 GB, 6.8 GiB) copied, 104 s, 70.6 MB/s7461666816 bytes (7.5 GB, 6.9 GiB) copied, 104 s, 71.7 MB/s7391412224 bytes (7.4 GB, 6.9 GiB) copied, 104 s, 71.1 MB/s7326400512 bytes (7.3 GB, 6.8 GiB) copied, 105 s, 69.8 MB/s7464812544 bytes (7.5 GB, 7.0 GiB) copied, 105 s, 71.1 MB/s7531921408 bytes (7.5 GB, 7.0 GiB) copied, 105 s, 71.7 MB/s7426015232 bytes (7.4 GB, 6.9 GiB) copied, 105 s, 70.6 MB/s7602176000 bytes (7.6 GB, 7.1 GiB) copied, 106 s, 71.7 MB/s7411335168 bytes (7.4 GB, 6.9 GiB) copied, 106 s, 69.9 MB/s7532969984 bytes (7.5 GB, 7.0 GiB) copied, 106 s, 71.1 MB/s7474249728 bytes (7.5 GB, 7.0 GiB) copied, 106 s, 70.5 MB/s7686062080 bytes (7.7 GB, 7.2 GiB) copied, 107 s, 71.8 MB/s7495221248 bytes (7.5 GB, 7.0 GiB) copied, 107 s, 70.0 MB/s7545552896 bytes (7.5 GB, 7.0 GiB) copied, 107 s, 70.5 MB/s7609516032 bytes (7.6 GB, 7.1 GiB) copied, 107 s, 71.1 MB/s7671382016 bytes (7.7 GB, 7.1 GiB) copied, 108 s, 71.0 MB/s7561281536 bytes (7.6 GB, 7.0 GiB) copied, 108 s, 70.0 MB/s7752122368 bytes (7.8 GB, 7.2 GiB) copied, 108 s, 71.8 MB/s7609516032 bytes (7.6 GB, 7.1 GiB) copied, 108 s, 70.4 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52496/160000 hits, 6667 MiB read, 1.008 read amp, 704626 total
[0m7633633280 bytes (7.6 GB, 7.1 GiB) copied, 109 s, 70.0 MB/s7833911296 bytes (7.8 GB, 7.3 GiB) copied, 109 s, 71.9 MB/s7755268096 bytes (7.8 GB, 7.2 GiB) copied, 109 s, 71.1 MB/s7693402112 bytes (7.7 GB, 7.2 GiB) copied, 109 s, 70.6 MB/s7918845952 bytes (7.9 GB, 7.4 GiB) copied, 110 s, 72.0 MB/s7750025216 bytes (7.8 GB, 7.2 GiB) copied, 110 s, 70.5 MB/s7841251328 bytes (7.8 GB, 7.3 GiB) copied, 110 s, 71.3 MB/s7693402112 bytes (7.7 GB, 7.2 GiB) copied, 110 s, 69.9 MB/s7825522688 bytes (7.8 GB, 7.3 GiB) copied, 111 s, 70.5 MB/s7924088832 bytes (7.9 GB, 7.4 GiB) copied, 111 s, 71.4 MB/s7998537728 bytes (8.0 GB, 7.4 GiB) copied, 111 s, 72.1 MB/s7772045312 bytes (7.8 GB, 7.2 GiB) copied, 111 s, 70.0 MB/s7843348480 bytes (7.8 GB, 7.3 GiB) copied, 112 s, 70.0 MB/s8081375232 bytes (8.1 GB, 7.5 GiB) copied, 112 s, 72.2 MB/s8001683456 bytes (8.0 GB, 7.5 GiB) copied, 112 s, 71.4 MB/s7899971584 bytes (7.9 GB, 7.4 GiB) copied, 112 s, 70.5 MB/s8070889472 bytes (8.1 GB, 7.5 GiB) copied, 113 s, 71.4 MB/s7907311616 bytes (7.9 GB, 7.4 GiB) copied, 113 s, 70.0 MB/s7954497536 bytes (8.0 GB, 7.4 GiB) copied, 113 s, 70.4 MB/s8139046912 bytes (8.1 GB, 7.6 GiB) copied, 113 s, 72.0 MB/s7960788992 bytes (8.0 GB, 7.4 GiB) copied, 114 s, 69.8 MB/s8131706880 bytes (8.1 GB, 7.6 GiB) copied, 114 s, 71.3 MB/s8002732032 bytes (8.0 GB, 7.5 GiB) copied, 114 s, 70.2 MB/s8188329984 bytes (8.2 GB, 7.6 GiB) copied, 114 s, 71.8 MB/s8212447232 bytes (8.2 GB, 7.6 GiB) copied, 115 s, 71.4 MB/s8251244544 bytes (8.3 GB, 7.7 GiB) copied, 115 s, 71.7 MB/s8041529344 bytes (8.0 GB, 7.5 GiB) copied, 115 s, 69.9 MB/s8067743744 bytes (8.1 GB, 7.5 GiB) copied, 115 s, 70.2 MB/s8109686784 bytes (8.1 GB, 7.6 GiB) copied, 116 s, 69.9 MB/s8303673344 bytes (8.3 GB, 7.7 GiB) copied, 116 s, 71.6 MB/s8146386944 bytes (8.1 GB, 7.6 GiB) copied, 116 s, 70.2 MB/s8322547712 bytes (8.3 GB, 7.8 GiB) copied, 116 s, 71.7 MB/s8169455616 bytes (8.2 GB, 7.6 GiB) copied, 117 s, 69.8 MB/s8410628096 bytes (8.4 GB, 7.8 GiB) copied, 117 s, 71.9 MB/s8204058624 bytes (8.2 GB, 7.6 GiB) copied, 117 s, 70.1 MB/s8381267968 bytes (8.4 GB, 7.8 GiB) copied, 117 s, 71.6 MB/s8230273024 bytes (8.2 GB, 7.7 GiB) copied, 118 s, 69.7 MB/s8460959744 bytes (8.5 GB, 7.9 GiB) copied, 118 s, 71.7 MB/s8474591232 bytes (8.5 GB, 7.9 GiB) copied, 118 s, 71.8 MB/s8262778880 bytes (8.3 GB, 7.7 GiB) copied, 118 s, 70.0 MB/s8540651520 bytes (8.5 GB, 8.0 GiB) copied, 119 s, 71.8 MB/s8306819072 bytes (8.3 GB, 7.7 GiB) copied, 119 s, 69.8 MB/s8340373504 bytes (8.3 GB, 7.8 GiB) copied, 119 s, 70.1 MB/s8553234432 bytes (8.6 GB, 8.0 GiB) copied, 119 s, 71.9 MB/s8633974784 bytes (8.6 GB, 8.0 GiB) copied, 120 s, 71.9 MB/s8424259584 bytes (8.4 GB, 7.8 GiB) copied, 120 s, 70.2 MB/s8381267968 bytes (8.4 GB, 7.8 GiB) copied, 120 s, 69.8 MB/s8616148992 bytes (8.6 GB, 8.0 GiB) copied, 120 s, 71.8 MB/s8691646464 bytes (8.7 GB, 8.1 GiB) copied, 121 s, 71.8 MB/s8441036800 bytes (8.4 GB, 7.9 GiB) copied, 121 s, 69.8 MB/s8497659904 bytes (8.5 GB, 7.9 GiB) copied, 121 s, 70.2 MB/s8666480640 bytes (8.7 GB, 8.1 GiB) copied, 121 s, 71.6 MB/s8519680000 bytes (8.5 GB, 7.9 GiB) copied, 122 s, 69.8 MB/s8772386816 bytes (8.8 GB, 8.2 GiB) copied, 122 s, 71.9 MB/s8735686656 bytes (8.7 GB, 8.1 GiB) copied, 122 s, 71.6 MB/s8583643136 bytes (8.6 GB, 8.0 GiB) copied, 122 s, 70.4 MB/s8851030016 bytes (8.9 GB, 8.2 GiB) copied, 123 s, 72.0 MB/s8818524160 bytes (8.8 GB, 8.2 GiB) copied, 123 s, 71.7 MB/s8665432064 bytes (8.7 GB, 8.1 GiB) copied, 123 s, 70.4 MB/s8600420352 bytes (8.6 GB, 8.0 GiB) copied, 123 s, 69.9 MB/s8743026688 bytes (8.7 GB, 8.1 GiB) copied, 124 s, 70.5 MB/s8935964672 bytes (8.9 GB, 8.3 GiB) copied, 124 s, 72.1 MB/s8679063552 bytes (8.7 GB, 8.1 GiB) copied, 124 s, 70.0 MB/s8894021632 bytes (8.9 GB, 8.3 GiB) copied, 124 s, 71.7 MB/s8733589504 bytes (8.7 GB, 8.1 GiB) copied, 125 s, 69.9 MB/s8943304704 bytes (8.9 GB, 8.3 GiB) copied, 125 s, 71.5 MB/s8999927808 bytes (9.0 GB, 8.4 GiB) copied, 125 s, 72.0 MB/s8825864192 bytes (8.8 GB, 8.2 GiB) copied, 125 s, 70.6 MB/s9082765312 bytes (9.1 GB, 8.5 GiB) copied, 126 s, 72.1 MB/s8796504064 bytes (8.8 GB, 8.2 GiB) copied, 126 s, 69.8 MB/s8888778752 bytes (8.9 GB, 8.3 GiB) copied, 126 s, 70.5 MB/s8997830656 bytes (9.0 GB, 8.4 GiB) copied, 126 s, 71.4 MB/s9148825600 bytes (9.1 GB, 8.5 GiB) copied, 127 s, 72.0 MB/s8961130496 bytes (9.0 GB, 8.3 GiB) copied, 127 s, 70.6 MB/s8872001536 bytes (8.9 GB, 8.3 GiB) copied, 127 s, 69.9 MB/s9058648064 bytes (9.1 GB, 8.4 GiB) copied, 127 s, 71.3 MB/s8952741888 bytes (9.0 GB, 8.3 GiB) copied, 128 s, 69.9 MB/s9129951232 bytes (9.1 GB, 8.5 GiB) copied, 128 s, 71.3 MB/s9219080192 bytes (9.2 GB, 8.6 GiB) copied, 128 s, 72.0 MB/s9022996480 bytes (9.0 GB, 8.4 GiB) copied, 128 s, 70.5 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 835420 total
[0m9032433664 bytes (9.0 GB, 8.4 GiB) copied, 129 s, 70.0 MB/s9289334784 bytes (9.3 GB, 8.7 GiB) copied, 129 s, 72.0 MB/s9215934464 bytes (9.2 GB, 8.6 GiB) copied, 129 s, 71.4 MB/s9093251072 bytes (9.1 GB, 8.5 GiB) copied, 129 s, 70.5 MB/s9353297920 bytes (9.4 GB, 8.7 GiB) copied, 130 s, 71.9 MB/s9155117056 bytes (9.2 GB, 8.5 GiB) copied, 130 s, 70.4 MB/s9099542528 bytes (9.1 GB, 8.5 GiB) copied, 130 s, 70.0 MB/s9278849024 bytes (9.3 GB, 8.6 GiB) copied, 130 s, 71.4 MB/s9356443648 bytes (9.4 GB, 8.7 GiB) copied, 131 s, 71.4 MB/s9431941120 bytes (9.4 GB, 8.8 GiB) copied, 131 s, 72.0 MB/s9176088576 bytes (9.2 GB, 8.5 GiB) copied, 131 s, 70.0 MB/s9236905984 bytes (9.2 GB, 8.6 GiB) copied, 131 s, 70.5 MB/s9261023232 bytes (9.3 GB, 8.6 GiB) copied, 132 s, 70.2 MB/s9437184000 bytes (9.4 GB, 8.8 GiB) copied, 132 s, 71.5 MB/s9304014848 bytes (9.3 GB, 8.7 GiB) copied, 132 s, 70.5 MB/s9513730048 bytes (9.5 GB, 8.9 GiB) copied, 132 s, 72.1 MB/s9382658048 bytes (9.4 GB, 8.7 GiB) copied, 133 s, 70.5 MB/s9590276096 bytes (9.6 GB, 8.9 GiB) copied, 133 s, 72.1 MB/s9341763584 bytes (9.3 GB, 8.7 GiB) copied, 133 s, 70.2 MB/s9513730048 bytes (9.5 GB, 8.9 GiB) copied, 133 s, 71.5 MB/s9586081792 bytes (9.6 GB, 8.9 GiB) copied, 134 s, 71.5 MB/s9410969600 bytes (9.4 GB, 8.8 GiB) copied, 134 s, 70.2 MB/s9669967872 bytes (9.7 GB, 9.0 GiB) copied, 134 s, 72.2 MB/s9462349824 bytes (9.5 GB, 8.8 GiB) copied, 134 s, 70.6 MB/s9729736704 bytes (9.7 GB, 9.1 GiB) copied, 135 s, 72.1 MB/s9472835584 bytes (9.5 GB, 8.8 GiB) copied, 135 s, 70.2 MB/s9643753472 bytes (9.6 GB, 9.0 GiB) copied, 135 s, 71.4 MB/s9523167232 bytes (9.5 GB, 8.9 GiB) copied, 135 s, 70.5 MB/s9560915968 bytes (9.6 GB, 8.9 GiB) copied, 136 s, 70.3 MB/s9715056640 bytes (9.7 GB, 9.0 GiB) copied, 136 s, 71.4 MB/s9592373248 bytes (9.6 GB, 8.9 GiB) copied, 136 s, 70.5 MB/s9815719936 bytes (9.8 GB, 9.1 GiB) copied, 136 s, 72.2 MB/s9787408384 bytes (9.8 GB, 9.1 GiB) copied, 137 s, 71.4 MB/s9667870720 bytes (9.7 GB, 9.0 GiB) copied, 137 s, 70.6 MB/s9637462016 bytes (9.6 GB, 9.0 GiB) copied, 137 s, 70.3 MB/s9891217408 bytes (9.9 GB, 9.2 GiB) copied, 137 s, 72.2 MB/s9707716608 bytes (9.7 GB, 9.0 GiB) copied, 138 s, 70.3 MB/s9974054912 bytes (10 GB, 9.3 GiB) copied, 138 s, 72.3 MB/s 9860808704 bytes (9.9 GB, 9.2 GiB) copied, 138 s, 71.4 MB/s9756999680 bytes (9.8 GB, 9.1 GiB) copied, 138 s, 70.7 MB/s10038018048 bytes (10 GB, 9.3 GiB) copied, 139 s, 72.2 MB/s9939451904 bytes (9.9 GB, 9.3 GiB) copied, 139 s, 71.5 MB/s9836691456 bytes (9.8 GB, 9.2 GiB) copied, 139 s, 70.8 MB/s9790554112 bytes (9.8 GB, 9.1 GiB) copied, 139 s, 70.4 MB/s9923723264 bytes (9.9 GB, 9.2 GiB) copied, 140 s, 70.9 MB/s10014949376 bytes (10 GB, 9.3 GiB) copied, 140 s, 71.5 MB/s9866051584 bytes (9.9 GB, 9.2 GiB) copied, 140 s, 70.5 MB/s10126098432 bytes (10 GB, 9.4 GiB) copied, 140 s, 72.3 MB/s10212081664 bytes (10 GB, 9.5 GiB) copied, 141 s, 72.4 MB/s9948889088 bytes (9.9 GB, 9.3 GiB) copied, 141 s, 70.6 MB/s9997123584 bytes (10 GB, 9.3 GiB) copied, 141 s, 70.9 MB/s 10094641152 bytes (10 GB, 9.4 GiB) copied, 141 s, 71.6 MB/s10290724864 bytes (10 GB, 9.6 GiB) copied, 142 s, 72.5 MB/s10074718208 bytes (10 GB, 9.4 GiB) copied, 142 s, 70.9 MB/s10168041472 bytes (10 GB, 9.5 GiB) copied, 142 s, 71.6 MB/s10017046528 bytes (10 GB, 9.3 GiB) copied, 142 s, 70.5 MB/s10235150336 bytes (10 GB, 9.5 GiB) copied, 143 s, 71.6 MB/s10362028032 bytes (10 GB, 9.7 GiB) copied, 143 s, 72.5 MB/s10092544000 bytes (10 GB, 9.4 GiB) copied, 143 s, 70.6 MB/s10142875648 bytes (10 GB, 9.4 GiB) copied, 143 s, 70.9 MB/s10173284352 bytes (10 GB, 9.5 GiB) copied, 144 s, 70.6 MB/s10433331200 bytes (10 GB, 9.7 GiB) copied, 144 s, 72.4 MB/s10218373120 bytes (10 GB, 9.5 GiB) copied, 144 s, 71.0 MB/s10316939264 bytes (10 GB, 9.6 GiB) copied, 144 s, 71.6 MB/s10506731520 bytes (11 GB, 9.8 GiB) copied, 145 s, 72.5 MB/s10235150336 bytes (10 GB, 9.5 GiB) copied, 145 s, 70.6 MB/s10368319488 bytes (10 GB, 9.7 GiB) copied, 145 s, 71.5 MB/s10302259200 bytes (10 GB, 9.6 GiB) copied, 145 s, 71.0 MB/s10302259200 bytes (10 GB, 9.6 GiB) copied, 146 s, 70.6 MB/s10374610944 bytes (10 GB, 9.7 GiB) copied, 146 s, 71.1 MB/s10450108416 bytes (10 GB, 9.7 GiB) copied, 146 s, 71.6 MB/s10580131840 bytes (11 GB, 9.9 GiB) copied, 146 s, 72.5 MB/s10638852096 bytes (11 GB, 9.9 GiB) copied, 147 s, 72.4 MB/s10532945920 bytes (11 GB, 9.8 GiB) copied, 147 s, 71.6 MB/s10453254144 bytes (10 GB, 9.7 GiB) copied, 147 s, 71.1 MB/s10369368064 bytes (10 GB, 9.7 GiB) copied, 147 s, 70.5 MB/s10611589120 bytes (11 GB, 9.9 GiB) copied, 148 s, 71.7 MB/s10529800192 bytes (11 GB, 9.8 GiB) copied, 148 s, 71.1 MB/s10724835328 bytes (11 GB, 10 GiB) copied, 148 s, 72.5 MB/s 10440671232 bytes (10 GB, 9.7 GiB) copied, 148 s, 70.5 MB/s[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52511/160000 hits, 6666 MiB read, 1.008 read amp, 972040 total
[0m10691280896 bytes (11 GB, 10 GiB) copied, 149 s, 71.8 MB/s 10523508736 bytes (11 GB, 9.8 GiB) copied, 149 s, 70.6 MB/s10607394816 bytes (11 GB, 9.9 GiB) copied, 149 s, 71.2 MB/s
10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 149.591 s, 71.8 MB/s
10686038016 bytes (11 GB, 10 GiB) copied, 150 s, 71.2 MB/s 10601103360 bytes (11 GB, 9.9 GiB) copied, 150 s, 70.7 MB/s
10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 150.91 s, 71.2 MB/s
10676600832 bytes (11 GB, 9.9 GiB) copied, 151 s, 70.7 MB/s
10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 151.823 s, 70.7 MB/s

10240+0 records in
10240+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 152.958 s, 70.2 MB/s


===Fio: workload=randread, time=60, iodepth=256, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 4 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 3636250 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 7618121 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 11259183 total
[0m
j1: (groupid=0, jobs=4): err= 0: pid=3778: Sun Jan 14 23:12:50 2024
  read: IOPS=175k, BW=685MiB/s (718MB/s)(40.1GiB/60002msec)
    slat (usec): min=4, max=4570, avg=20.41, stdev=64.94
    clat (usec): min=370, max=16164, avg=5815.89, stdev=2173.10
     lat (usec): min=380, max=16172, avg=5836.58, stdev=2181.22
    clat percentiles (usec):
     |  1.00th=[ 3752],  5.00th=[ 3884], 10.00th=[ 3982], 20.00th=[ 4178],
     | 30.00th=[ 4359], 40.00th=[ 4490], 50.00th=[ 4686], 60.00th=[ 5014],
     | 70.00th=[ 6915], 80.00th=[ 8160], 90.00th=[ 8979], 95.00th=[ 9765],
     | 99.00th=[12518], 99.50th=[13435], 99.90th=[14353], 99.95th=[14615],
     | 99.99th=[15533]
   bw (  KiB/s): min=320920, max=1031816, per=100.00%, avg=702055.84, stdev=46613.70, samples=476
   iops        : min=80230, max=257954, avg=175513.76, stdev=11653.41, samples=476
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=10.26%, 10=85.82%, 20=3.91%
  cpu          : usr=12.31%, sys=34.91%, ctx=1014965, majf=0, minf=2601
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=10522919,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=685MiB/s (718MB/s), 685MiB/s-685MiB/s (718MB/s-718MB/s), io=40.1GiB (43.1GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=175k, BW=685MiB/s (718MB/s)(40.1GiB/60002msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 13002469 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 14374205 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 15801847 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=3835: Sun Jan 14 23:13:52 2024
  read: IOPS=64.9k, BW=253MiB/s (266MB/s)(14.8GiB/60001msec)
    slat (usec): min=4, max=1964, avg=12.91, stdev=48.94
    clat (usec): min=134, max=4699, avg=1958.15, stdev=357.12
     lat (usec): min=165, max=4707, avg=1971.37, stdev=354.04
    clat percentiles (usec):
     |  1.00th=[ 1106],  5.00th=[ 1270], 10.00th=[ 1450], 20.00th=[ 1680],
     | 30.00th=[ 1795], 40.00th=[ 1893], 50.00th=[ 1991], 60.00th=[ 2073],
     | 70.00th=[ 2180], 80.00th=[ 2245], 90.00th=[ 2376], 95.00th=[ 2474],
     | 99.00th=[ 2737], 99.50th=[ 2835], 99.90th=[ 3097], 99.95th=[ 3195],
     | 99.99th=[ 3425]
   bw (  KiB/s): min=236056, max=298912, per=100.00%, avg=259725.92, stdev=16665.00, samples=119
   iops        : min=59014, max=74728, avg=64931.46, stdev=4166.27, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.11%
  lat (msec)   : 2=51.18%, 4=48.71%, 10=0.01%
  cpu          : usr=16.91%, sys=51.17%, ctx=100527, majf=0, minf=137
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3892140,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=253MiB/s (266MB/s), 253MiB/s-253MiB/s (266MB/s-266MB/s), io=14.8GiB (15.9GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=64.9k, BW=253MiB/s (266MB/s)(14.8GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 17516431 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 19867759 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 22420915 total
[0m
j1: (groupid=0, jobs=2): err= 0: pid=3881: Sun Jan 14 23:14:54 2024
  read: IOPS=114k, BW=446MiB/s (468MB/s)(26.2GiB/60001msec)
    slat (usec): min=4, max=7506, avg=15.11, stdev=57.91
    clat (usec): min=139, max=9716, avg=2222.60, stdev=728.45
     lat (usec): min=161, max=9725, avg=2238.00, stdev=731.99
    clat percentiles (usec):
     |  1.00th=[ 1483],  5.00th=[ 1614], 10.00th=[ 1696], 20.00th=[ 1778],
     | 30.00th=[ 1876], 40.00th=[ 1958], 50.00th=[ 2024], 60.00th=[ 2114],
     | 70.00th=[ 2245], 80.00th=[ 2376], 90.00th=[ 2737], 95.00th=[ 4228],
     | 99.00th=[ 5014], 99.50th=[ 5145], 99.90th=[ 5538], 99.95th=[ 5800],
     | 99.99th=[ 6652]
   bw (  KiB/s): min=216128, max=570120, per=99.94%, avg=456927.93, stdev=46446.62, samples=238
   iops        : min=54032, max=142530, avg=114231.98, stdev=11611.63, samples=238
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=46.29%, 4=48.16%, 10=5.54%
  cpu          : usr=14.26%, sys=43.72%, ctx=330350, majf=0, minf=281
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=6857896,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=446MiB/s (468MB/s), 446MiB/s-446MiB/s (468MB/s-468MB/s), io=26.2GiB (28.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=114k, BW=446MiB/s (468MB/s)(26.2GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 24763900 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 27943585 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 31251338 total
[0m
j1: (groupid=0, jobs=3): err= 0: pid=3929: Sun Jan 14 23:15:57 2024
  read: IOPS=154k, BW=602MiB/s (631MB/s)(35.2GiB/60001msec)
    slat (usec): min=4, max=2989, avg=17.05, stdev=61.93
    clat (usec): min=184, max=9114, avg=2474.58, stdev=964.03
     lat (usec): min=198, max=9125, avg=2491.93, stdev=970.40
    clat percentiles (usec):
     |  1.00th=[ 1500],  5.00th=[ 1696], 10.00th=[ 1778], 20.00th=[ 1893],
     | 30.00th=[ 1975], 40.00th=[ 2040], 50.00th=[ 2114], 60.00th=[ 2212],
     | 70.00th=[ 2343], 80.00th=[ 2769], 90.00th=[ 4047], 95.00th=[ 4686],
     | 99.00th=[ 5800], 99.50th=[ 6390], 99.90th=[ 7308], 99.95th=[ 7439],
     | 99.99th=[ 8029]
   bw (  KiB/s): min=265032, max=835976, per=100.00%, avg=617364.37, stdev=52999.01, samples=357
   iops        : min=66258, max=208994, avg=154341.01, stdev=13249.73, samples=357
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=34.58%, 4=54.90%, 10=10.52%
  cpu          : usr=13.25%, sys=40.58%, ctx=602314, majf=0, minf=428
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=9239282,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=602MiB/s (631MB/s), 602MiB/s-602MiB/s (631MB/s-631MB/s), io=35.2GiB (37.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=154k, BW=602MiB/s (631MB/s)(35.2GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 34526349 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 38080402 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 41979054 total
[0m
j1: (groupid=0, jobs=4): err= 0: pid=3981: Sun Jan 14 23:16:59 2024
  read: IOPS=176k, BW=687MiB/s (721MB/s)(40.3GiB/60001msec)
    slat (usec): min=4, max=7885, avg=20.17, stdev=66.98
    clat (usec): min=190, max=13498, avg=2886.87, stdev=1216.51
     lat (usec): min=200, max=13503, avg=2907.39, stdev=1225.22
    clat percentiles (usec):
     |  1.00th=[ 1729],  5.00th=[ 1876], 10.00th=[ 1942], 20.00th=[ 2040],
     | 30.00th=[ 2114], 40.00th=[ 2212], 50.00th=[ 2311], 60.00th=[ 2474],
     | 70.00th=[ 3097], 80.00th=[ 4015], 90.00th=[ 4555], 95.00th=[ 5211],
     | 99.00th=[ 6718], 99.50th=[ 7439], 99.90th=[10421], 99.95th=[11076],
     | 99.99th=[12125]
   bw (  KiB/s): min=264480, max=1031241, per=99.88%, avg=703088.45, stdev=50331.16, samples=476
   iops        : min=66120, max=257810, avg=175771.90, stdev=12582.79, samples=476
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=14.98%, 4=64.99%, 10=19.89%, 20=0.15%
  cpu          : usr=12.76%, sys=36.42%, ctx=964482, majf=0, minf=1262
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=10559438,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=687MiB/s (721MB/s), 687MiB/s-687MiB/s (721MB/s-721MB/s), io=40.3GiB (43.3GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=176k, BW=687MiB/s (721MB/s)(40.3GiB/60001msec)
umount: /mnt/fsbench: not mounted.
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:06:47.lsvd-multi.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:06:47.lsvd-multi.triple-hdd.txt
Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=175k, BW=685MiB/s (718MB/s)(40.1GiB/60002msec)
Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=64.9k, BW=253MiB/s (266MB/s)(14.8GiB/60001msec)
Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=114k, BW=446MiB/s (468MB/s)(26.2GiB/60001msec)
Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=154k, BW=602MiB/s (631MB/s)(35.2GiB/60001msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=176k, BW=687MiB/s (721MB/s)(40.3GiB/60001msec)
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.4
[0m[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.3
[0m+ exit
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 44621933 total
[0m[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.2
[0m[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark.multi.1
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0mflush thread (7f5cbb7fe640) exiting
flush thread (7f5cf17f2640) exiting
flush thread (7f5d0eff5640) exiting
flush thread (7f5d207e8640) exiting
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
++ date +%FT%T
+ cur_time=2024-01-14T23:17:16
+ default_cache_size=128849018880
+ cache_size=128849018880
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=120
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:17:16.rbd-multi.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=rbd-benchmark
+ imgsize=10G
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ rbd -p rssd2 rm rbd-benchmark.multi.1
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 rm rbd-benchmark.multi.2
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 rm rbd-benchmark.multi.3
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 rm rbd-benchmark.multi.4
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.1
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.2
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.3
+ wait
+ rbd -p rssd2 create --size 10G --thick-provision rbd-benchmark.multi.4
Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 9% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 10% complete...Thick provisioning: 10% complete...Thick provisioning: 12% complete...Thick provisioning: 11% complete...Thick provisioning: 11% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 14% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 15% complete...Thick provisioning: 14% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 16% complete...Thick provisioning: 18% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 23% complete...Thick provisioning: 22% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 24% complete...Thick provisioning: 23% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 25% complete...Thick provisioning: 24% complete...Thick provisioning: 23% complete...Thick provisioning: 26% complete...Thick provisioning: 25% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 27% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 25% complete...Thick provisioning: 28% complete...Thick provisioning: 27% complete...Thick provisioning: 26% complete...Thick provisioning: 26% complete...Thick provisioning: 29% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 27% complete...Thick provisioning: 30% complete...Thick provisioning: 29% complete...Thick provisioning: 28% complete...Thick provisioning: 31% complete...Thick provisioning: 28% complete...Thick provisioning: 30% complete...Thick provisioning: 29% complete...Thick provisioning: 32% complete...Thick provisioning: 29% complete...Thick provisioning: 31% complete...Thick provisioning: 33% complete...Thick provisioning: 30% complete...Thick provisioning: 30% complete...Thick provisioning: 32% complete...Thick provisioning: 34% complete...Thick provisioning: 31% complete...Thick provisioning: 35% complete...Thick provisioning: 33% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 36% complete...Thick provisioning: 34% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 35% complete...Thick provisioning: 37% complete...Thick provisioning: 34% complete...Thick provisioning: 33% complete...Thick provisioning: 38% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 34% complete...Thick provisioning: 39% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 40% complete...Thick provisioning: 35% complete...Thick provisioning: 38% complete...Thick provisioning: 37% complete...Thick provisioning: 41% complete...Thick provisioning: 36% complete...Thick provisioning: 39% complete...Thick provisioning: 38% complete...Thick provisioning: 42% complete...Thick provisioning: 37% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 43% complete...Thick provisioning: 38% complete...Thick provisioning: 44% complete...Thick provisioning: 41% complete...Thick provisioning: 40% complete...Thick provisioning: 39% complete...Thick provisioning: 45% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 40% complete...Thick provisioning: 46% complete...Thick provisioning: 43% complete...Thick provisioning: 42% complete...Thick provisioning: 41% complete...Thick provisioning: 47% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 42% complete...Thick provisioning: 48% complete...Thick provisioning: 45% complete...Thick provisioning: 44% complete...Thick provisioning: 43% complete...Thick provisioning: 49% complete...Thick provisioning: 46% complete...Thick provisioning: 45% complete...Thick provisioning: 50% complete...Thick provisioning: 44% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 51% complete...Thick provisioning: 45% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 52% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 46% complete...Thick provisioning: 53% complete...Thick provisioning: 47% complete...Thick provisioning: 50% complete...Thick provisioning: 49% complete...Thick provisioning: 54% complete...Thick provisioning: 51% complete...Thick provisioning: 48% complete...Thick provisioning: 50% complete...Thick provisioning: 55% complete...Thick provisioning: 52% complete...Thick provisioning: 51% complete...Thick provisioning: 49% complete...Thick provisioning: 56% complete...Thick provisioning: 53% complete...Thick provisioning: 52% complete...Thick provisioning: 50% complete...Thick provisioning: 57% complete...Thick provisioning: 54% complete...Thick provisioning: 53% complete...Thick provisioning: 51% complete...Thick provisioning: 58% complete...Thick provisioning: 55% complete...Thick provisioning: 52% complete...Thick provisioning: 54% complete...Thick provisioning: 59% complete...Thick provisioning: 56% complete...Thick provisioning: 53% complete...Thick provisioning: 60% complete...Thick provisioning: 55% complete...Thick provisioning: 57% complete...Thick provisioning: 61% complete...Thick provisioning: 54% complete...Thick provisioning: 58% complete...Thick provisioning: 56% complete...Thick provisioning: 62% complete...Thick provisioning: 55% complete...Thick provisioning: 59% complete...Thick provisioning: 63% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 64% complete...Thick provisioning: 60% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 65% complete...Thick provisioning: 61% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 66% complete...Thick provisioning: 62% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 67% complete...Thick provisioning: 63% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 68% complete...Thick provisioning: 64% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 69% complete...Thick provisioning: 65% complete...Thick provisioning: 62% complete...Thick provisioning: 70% complete...Thick provisioning: 63% complete...Thick provisioning: 66% complete...Thick provisioning: 63% complete...Thick provisioning: 71% complete...Thick provisioning: 64% complete...Thick provisioning: 67% complete...Thick provisioning: 64% complete...Thick provisioning: 72% complete...Thick provisioning: 65% complete...Thick provisioning: 68% complete...Thick provisioning: 65% complete...Thick provisioning: 73% complete...Thick provisioning: 66% complete...Thick provisioning: 69% complete...Thick provisioning: 74% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 70% complete...Thick provisioning: 75% complete...Thick provisioning: 68% complete...Thick provisioning: 71% complete...Thick provisioning: 67% complete...Thick provisioning: 76% complete...Thick provisioning: 69% complete...Thick provisioning: 72% complete...Thick provisioning: 68% complete...Thick provisioning: 77% complete...Thick provisioning: 73% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 78% complete...Thick provisioning: 70% complete...Thick provisioning: 74% complete...Thick provisioning: 71% complete...Thick provisioning: 79% complete...Thick provisioning: 71% complete...Thick provisioning: 75% complete...Thick provisioning: 72% complete...Thick provisioning: 80% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 76% complete...Thick provisioning: 81% complete...Thick provisioning: 74% complete...Thick provisioning: 73% complete...Thick provisioning: 77% complete...Thick provisioning: 82% complete...Thick provisioning: 75% complete...Thick provisioning: 74% complete...Thick provisioning: 78% complete...Thick provisioning: 76% complete...Thick provisioning: 83% complete...Thick provisioning: 75% complete...Thick provisioning: 79% complete...Thick provisioning: 84% complete...Thick provisioning: 77% complete...Thick provisioning: 80% complete...Thick provisioning: 76% complete...Thick provisioning: 85% complete...Thick provisioning: 78% complete...Thick provisioning: 77% complete...Thick provisioning: 81% complete...Thick provisioning: 86% complete...Thick provisioning: 79% complete...Thick provisioning: 82% complete...Thick provisioning: 78% complete...Thick provisioning: 80% complete...Thick provisioning: 87% complete...Thick provisioning: 83% complete...Thick provisioning: 79% complete...Thick provisioning: 88% complete...Thick provisioning: 81% complete...Thick provisioning: 84% complete...Thick provisioning: 80% complete...Thick provisioning: 89% complete...Thick provisioning: 82% complete...Thick provisioning: 85% complete...Thick provisioning: 81% complete...Thick provisioning: 90% complete...Thick provisioning: 83% complete...Thick provisioning: 86% complete...Thick provisioning: 91% complete...Thick provisioning: 82% complete...Thick provisioning: 84% complete...Thick provisioning: 87% complete...Thick provisioning: 83% complete...Thick provisioning: 92% complete...Thick provisioning: 85% complete...Thick provisioning: 84% complete...Thick provisioning: 88% complete...Thick provisioning: 93% complete...Thick provisioning: 86% complete...Thick provisioning: 85% complete...Thick provisioning: 94% complete...Thick provisioning: 89% complete...Thick provisioning: 95% complete...Thick provisioning: 86% complete...Thick provisioning: 87% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 88% complete...Thick provisioning: 96% complete...Thick provisioning: 87% complete...Thick provisioning: 92% complete...Thick provisioning: 97% complete...Thick provisioning: 89% complete...Thick provisioning: 88% complete...Thick provisioning: 93% complete...Thick provisioning: 98% complete...Thick provisioning: 90% complete...Thick provisioning: 89% complete...Thick provisioning: 94% complete...Thick provisioning: 99% complete...Thick provisioning: 91% complete...Thick provisioning: 90% complete...Thick provisioning: 95% complete...Thick provisioning: 100% complete...Thick provisioning: 92% complete...Thick provisioning: 91% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 96% complete...Thick provisioning: 93% complete...Thick provisioning: 92% complete...Thick provisioning: 97% complete...Thick provisioning: 94% complete...Thick provisioning: 93% complete...Thick provisioning: 98% complete...Thick provisioning: 95% complete...Thick provisioning: 94% complete...Thick provisioning: 99% complete...Thick provisioning: 96% complete...Thick provisioning: 95% complete...Thick provisioning: 100% complete...Thick provisioning: 97% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 96% complete...Thick provisioning: 98% complete...Thick provisioning: 97% complete...Thick provisioning: 99% complete...Thick provisioning: 98% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done.
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ fstrim /mnt/nvme
+ launch_gw_background
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ sleep 5
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-14 23:18:27.128730] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-14 23:18:27.128855] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid807308 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-14 23:18:27.231151] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-14 23:18:27.373105] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-14 23:18:27.373154] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-14 23:18:27.373237] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-14 23:18:27.373243] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-14 23:18:32.790542] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-14 23:18:33.669560] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 rbd-benchmark.multi.1
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.1
+ local bdev=bdev_rbd-benchmark.multi.1
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.1 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.1
[2024-01-14 23:18:34.094508] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.1 rbd disk to lun
bdev_rbd-benchmark.multi.1
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.1
+ add_rbd_img rssd2 rbd-benchmark.multi.2
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.2
+ local bdev=bdev_rbd-benchmark.multi.2
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.2 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.2
[2024-01-14 23:18:34.857976] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.2 rbd disk to lun
bdev_rbd-benchmark.multi.2
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.2
+ add_rbd_img rssd2 rbd-benchmark.multi.3
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.3
+ local bdev=bdev_rbd-benchmark.multi.3
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.3 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.3
[2024-01-14 23:18:35.579037] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.3 rbd disk to lun
bdev_rbd-benchmark.multi.3
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.3
+ add_rbd_img rssd2 rbd-benchmark.multi.4
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=rbd-benchmark.multi.4
+ local bdev=bdev_rbd-benchmark.multi.4
+ scripts/rpc.py bdev_rbd_create rssd2 rbd-benchmark.multi.4 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.4
[2024-01-14 23:18:36.311236] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.4 rbd disk to lun
bdev_rbd-benchmark.multi.4
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.4
+ trap 'cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:17:16.rbd-multi.rssd2.txt multi-client/client-bench-multi.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:17:16.rbd-multi.rssd2.txt
+ local benchscript=multi-client/client-bench-multi.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:17:16.rbd-multi.rssd2.txt
===Starting client benchmark

NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
device: nvme1
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n2          SPDK00000000000001   SPDK_Controller1                         2          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n3          SPDK00000000000001   SPDK_Controller1                         3          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n4          SPDK00000000000001   SPDK_Controller1                         4          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
Using device /dev/nvme1n1
/dev/nvme1n2
/dev/nvme1n3
/dev/nvme1n4


===Fio: workload=randread, time=60, iodepth=256, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=4244: Sun Jan 14 23:19:43 2024
  read: IOPS=64.5k, BW=252MiB/s (264MB/s)(14.8GiB/60004msec)
    slat (usec): min=4, max=5467, avg=59.31, stdev=75.48
    clat (usec): min=2839, max=85327, avg=15815.36, stdev=3261.54
     lat (usec): min=2946, max=85343, avg=15875.05, stdev=3270.28
    clat percentiles (usec):
     |  1.00th=[ 8848],  5.00th=[11338], 10.00th=[12649], 20.00th=[13698],
     | 30.00th=[14353], 40.00th=[14877], 50.00th=[15401], 60.00th=[16057],
     | 70.00th=[16712], 80.00th=[17695], 90.00th=[19268], 95.00th=[21103],
     | 99.00th=[26084], 99.50th=[29230], 99.90th=[41681], 99.95th=[47449],
     | 99.99th=[60556]
   bw (  KiB/s): min=172128, max=355200, per=100.00%, avg=258301.91, stdev=8084.55, samples=476
   iops        : min=43032, max=88800, avg=64574.98, stdev=2021.15, samples=476
  lat (msec)   : 4=0.01%, 10=1.95%, 20=90.65%, 50=7.36%, 100=0.04%
  cpu          : usr=6.58%, sys=17.54%, ctx=2584127, majf=0, minf=1450
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3869293,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=252MiB/s (264MB/s), 252MiB/s-252MiB/s (264MB/s-264MB/s), io=14.8GiB (15.8GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=64.5k, BW=252MiB/s (264MB/s)(14.8GiB/60004msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process

j1: (groupid=0, jobs=1): err= 0: pid=4297: Sun Jan 14 23:20:46 2024
  read: IOPS=65.6k, BW=256MiB/s (269MB/s)(15.0GiB/60002msec)
    slat (usec): min=4, max=2091, avg=12.67, stdev=18.12
    clat (usec): min=460, max=29876, avg=1935.31, stdev=891.46
     lat (usec): min=467, max=29917, avg=1948.27, stdev=891.92
    clat percentiles (usec):
     |  1.00th=[  979],  5.00th=[ 1139], 10.00th=[ 1237], 20.00th=[ 1369],
     | 30.00th=[ 1483], 40.00th=[ 1598], 50.00th=[ 1713], 60.00th=[ 1844],
     | 70.00th=[ 2024], 80.00th=[ 2278], 90.00th=[ 2802], 95.00th=[ 3556],
     | 99.00th=[ 5407], 99.50th=[ 6194], 99.90th=[ 9503], 99.95th=[11469],
     | 99.99th=[15926]
   bw (  KiB/s): min=214456, max=298424, per=100.00%, avg=262558.72, stdev=20851.00, samples=119
   iops        : min=53614, max=74606, avg=65639.68, stdev=5212.71, samples=119
  lat (usec)   : 500=0.01%, 750=0.04%, 1000=1.19%
  lat (msec)   : 2=67.58%, 4=27.76%, 10=3.35%, 20=0.08%, 50=0.01%
  cpu          : usr=19.97%, sys=57.89%, ctx=289431, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3938215,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=256MiB/s (269MB/s), 256MiB/s-256MiB/s (269MB/s-269MB/s), io=15.0GiB (16.1GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=65.6k, BW=256MiB/s (269MB/s)(15.0GiB/60002msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes

j1: (groupid=0, jobs=2): err= 0: pid=4344: Sun Jan 14 23:21:48 2024
  read: IOPS=68.4k, BW=267MiB/s (280MB/s)(15.6GiB/60005msec)
    slat (usec): min=4, max=2721, avg=26.30, stdev=40.31
    clat (usec): min=460, max=39449, avg=3715.84, stdev=1983.78
     lat (usec): min=476, max=39497, avg=3742.50, stdev=1988.02
    clat percentiles (usec):
     |  1.00th=[  988],  5.00th=[ 1270], 10.00th=[ 1549], 20.00th=[ 2507],
     | 30.00th=[ 2966], 40.00th=[ 3195], 50.00th=[ 3425], 60.00th=[ 3687],
     | 70.00th=[ 4047], 80.00th=[ 4555], 90.00th=[ 5669], 95.00th=[ 7046],
     | 99.00th=[11338], 99.50th=[13435], 99.90th=[19268], 99.95th=[21627],
     | 99.99th=[26346]
   bw (  KiB/s): min=149648, max=398016, per=100.00%, avg=273593.01, stdev=29345.24, samples=238
   iops        : min=37412, max=99504, avg=68398.25, stdev=7336.31, samples=238
  lat (usec)   : 500=0.01%, 750=0.03%, 1000=1.05%
  lat (msec)   : 2=14.72%, 4=53.32%, 10=29.23%, 20=1.57%, 50=0.08%
  cpu          : usr=12.44%, sys=35.47%, ctx=1184735, majf=0, minf=430
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4102088,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=267MiB/s (280MB/s), 267MiB/s-267MiB/s (280MB/s-280MB/s), io=15.6GiB (16.8GB), run=60005-60005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=68.4k, BW=267MiB/s (280MB/s)(15.6GiB/60005msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes

j1: (groupid=0, jobs=3): err= 0: pid=4392: Sun Jan 14 23:22:51 2024
  read: IOPS=69.2k, BW=270MiB/s (283MB/s)(15.8GiB/60003msec)
    slat (usec): min=4, max=7034, avg=40.77, stdev=59.66
    clat (usec): min=448, max=54395, avg=5506.40, stdev=3156.63
     lat (usec): min=489, max=54418, avg=5547.50, stdev=3162.99
    clat percentiles (usec):
     |  1.00th=[ 1029],  5.00th=[ 1418], 10.00th=[ 1958], 20.00th=[ 3490],
     | 30.00th=[ 3916], 40.00th=[ 4359], 50.00th=[ 5014], 60.00th=[ 5669],
     | 70.00th=[ 6325], 80.00th=[ 7111], 90.00th=[ 8979], 95.00th=[11207],
     | 99.00th=[17171], 99.50th=[20055], 99.90th=[26870], 99.95th=[30278],
     | 99.99th=[37487]
   bw (  KiB/s): min=157145, max=462475, per=100.00%, avg=277197.19, stdev=22597.46, samples=357
   iops        : min=39285, max=115618, avg=69298.92, stdev=5649.37, samples=357
  lat (usec)   : 500=0.01%, 750=0.04%, 1000=0.78%
  lat (msec)   : 2=9.47%, 4=22.15%, 10=60.36%, 20=6.71%, 50=0.49%
  lat (msec)   : 100=0.01%
  cpu          : usr=8.44%, sys=22.68%, ctx=2046138, majf=0, minf=577
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4151938,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=270MiB/s (283MB/s), 270MiB/s-270MiB/s (283MB/s-283MB/s), io=15.8GiB (17.0GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=69.2k, BW=270MiB/s (283MB/s)(15.8GiB/60003msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=4441: Sun Jan 14 23:23:53 2024
  read: IOPS=65.4k, BW=255MiB/s (268MB/s)(15.0GiB/60004msec)
    slat (usec): min=4, max=9762, avg=58.46, stdev=80.57
    clat (usec): min=613, max=50485, avg=7770.07, stdev=2004.42
     lat (usec): min=627, max=50552, avg=7828.87, stdev=2011.87
    clat percentiles (usec):
     |  1.00th=[ 4113],  5.00th=[ 5211], 10.00th=[ 5735], 20.00th=[ 6390],
     | 30.00th=[ 6849], 40.00th=[ 7242], 50.00th=[ 7504], 60.00th=[ 7898],
     | 70.00th=[ 8291], 80.00th=[ 8848], 90.00th=[ 9896], 95.00th=[11207],
     | 99.00th=[15139], 99.50th=[16909], 99.90th=[20841], 99.95th=[23200],
     | 99.99th=[29492]
   bw (  KiB/s): min=174432, max=367640, per=100.00%, avg=261748.57, stdev=8797.81, samples=476
   iops        : min=43608, max=91910, avg=65437.13, stdev=2199.45, samples=476
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.05%, 4=0.76%, 10=89.84%, 20=9.21%, 50=0.14%
  lat (msec)   : 100=0.01%
  cpu          : usr=6.33%, sys=17.55%, ctx=2429365, majf=0, minf=564
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3923077,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=255MiB/s (268MB/s), 255MiB/s-255MiB/s (268MB/s-268MB/s), io=15.0GiB (16.1GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=65.4k, BW=255MiB/s (268MB/s)(15.0GiB/60004msec)
umount: /mnt/fsbench: not mounted.
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:17:16.rbd-multi.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:17:16.rbd-multi.rssd2.txt
Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=64.5k, BW=252MiB/s (264MB/s)(14.8GiB/60004msec)
Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=65.6k, BW=256MiB/s (269MB/s)(15.0GiB/60002msec)
Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=68.4k, BW=267MiB/s (280MB/s)(15.6GiB/60005msec)
Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=69.2k, BW=270MiB/s (283MB/s)(15.8GiB/60003msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=65.4k, BW=255MiB/s (268MB/s)(15.0GiB/60004msec)
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
++ date +%FT%T
+ cur_time=2024-01-14T23:23:56
+ default_cache_size=128849018880
+ cache_size=128849018880
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=120
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:23:56.rbd-multi.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=rbd-benchmark
+ imgsize=10G
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ rbd -p triple-hdd rm rbd-benchmark.multi.1
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd rm rbd-benchmark.multi.2
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd rm rbd-benchmark.multi.3
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd rm rbd-benchmark.multi.4
Removing image: 1% complete...Removing image: 2% complete...Removing image: 3% complete...Removing image: 4% complete...Removing image: 5% complete...Removing image: 6% complete...Removing image: 7% complete...Removing image: 8% complete...Removing image: 9% complete...Removing image: 10% complete...Removing image: 11% complete...Removing image: 12% complete...Removing image: 13% complete...Removing image: 14% complete...Removing image: 15% complete...Removing image: 16% complete...Removing image: 17% complete...Removing image: 18% complete...Removing image: 19% complete...Removing image: 20% complete...Removing image: 21% complete...Removing image: 22% complete...Removing image: 23% complete...Removing image: 24% complete...Removing image: 25% complete...Removing image: 26% complete...Removing image: 27% complete...Removing image: 28% complete...Removing image: 29% complete...Removing image: 30% complete...Removing image: 31% complete...Removing image: 32% complete...Removing image: 33% complete...Removing image: 34% complete...Removing image: 35% complete...Removing image: 36% complete...Removing image: 37% complete...Removing image: 38% complete...Removing image: 39% complete...Removing image: 40% complete...Removing image: 41% complete...Removing image: 42% complete...Removing image: 43% complete...Removing image: 44% complete...Removing image: 45% complete...Removing image: 46% complete...Removing image: 47% complete...Removing image: 48% complete...Removing image: 49% complete...Removing image: 50% complete...Removing image: 51% complete...Removing image: 52% complete...Removing image: 53% complete...Removing image: 54% complete...Removing image: 55% complete...Removing image: 56% complete...Removing image: 57% complete...Removing image: 58% complete...Removing image: 59% complete...Removing image: 60% complete...Removing image: 61% complete...Removing image: 62% complete...Removing image: 63% complete...Removing image: 64% complete...Removing image: 65% complete...Removing image: 66% complete...Removing image: 67% complete...Removing image: 68% complete...Removing image: 69% complete...Removing image: 70% complete...Removing image: 71% complete...Removing image: 72% complete...Removing image: 73% complete...Removing image: 74% complete...Removing image: 75% complete...Removing image: 76% complete...Removing image: 77% complete...Removing image: 78% complete...Removing image: 79% complete...Removing image: 80% complete...Removing image: 81% complete...Removing image: 82% complete...Removing image: 83% complete...Removing image: 84% complete...Removing image: 85% complete...Removing image: 86% complete...Removing image: 87% complete...Removing image: 88% complete...Removing image: 89% complete...Removing image: 90% complete...Removing image: 91% complete...Removing image: 92% complete...Removing image: 93% complete...Removing image: 94% complete...Removing image: 95% complete...Removing image: 96% complete...Removing image: 97% complete...Removing image: 98% complete...Removing image: 99% complete...Removing image: 100% complete...done.
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.1
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.2
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.3
+ wait
+ rbd -p triple-hdd create --size 10G --thick-provision rbd-benchmark.multi.4
Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 3% complete...Thick provisioning: 3% complete...Thick provisioning: 5% complete...Thick provisioning: 5% complete...Thick provisioning: 4% complete...Thick provisioning: 4% complete...Thick provisioning: 6% complete...Thick provisioning: 6% complete...Thick provisioning: 5% complete...Thick provisioning: 7% complete...Thick provisioning: 7% complete...Thick provisioning: 5% complete...Thick provisioning: 8% complete...Thick provisioning: 6% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 6% complete...Thick provisioning: 9% complete...Thick provisioning: 7% complete...Thick provisioning: 10% complete...Thick provisioning: 7% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 8% complete...Thick provisioning: 8% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 9% complete...Thick provisioning: 12% complete...Thick provisioning: 9% complete...Thick provisioning: 13% complete...Thick provisioning: 10% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 14% complete...Thick provisioning: 10% complete...Thick provisioning: 15% complete...Thick provisioning: 11% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 12% complete...Thick provisioning: 11% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 17% complete...Thick provisioning: 13% complete...Thick provisioning: 12% complete...Thick provisioning: 18% complete...Thick provisioning: 18% complete...Thick provisioning: 14% complete...Thick provisioning: 13% complete...Thick provisioning: 19% complete...Thick provisioning: 19% complete...Thick provisioning: 15% complete...Thick provisioning: 14% complete...Thick provisioning: 20% complete...Thick provisioning: 20% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 21% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 22% complete...Thick provisioning: 17% complete...Thick provisioning: 16% complete...Thick provisioning: 23% complete...Thick provisioning: 23% complete...Thick provisioning: 18% complete...Thick provisioning: 17% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 19% complete...Thick provisioning: 25% complete...Thick provisioning: 25% complete...Thick provisioning: 18% complete...Thick provisioning: 26% complete...Thick provisioning: 20% complete...Thick provisioning: 26% complete...Thick provisioning: 19% complete...Thick provisioning: 27% complete...Thick provisioning: 27% complete...Thick provisioning: 21% complete...Thick provisioning: 20% complete...Thick provisioning: 28% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 22% complete...Thick provisioning: 21% complete...Thick provisioning: 30% complete...Thick provisioning: 29% complete...Thick provisioning: 22% complete...Thick provisioning: 31% complete...Thick provisioning: 23% complete...Thick provisioning: 32% complete...Thick provisioning: 30% complete...Thick provisioning: 23% complete...Thick provisioning: 33% complete...Thick provisioning: 24% complete...Thick provisioning: 24% complete...Thick provisioning: 34% complete...Thick provisioning: 31% complete...Thick provisioning: 25% complete...Thick provisioning: 25% complete...Thick provisioning: 35% complete...Thick provisioning: 32% complete...Thick provisioning: 36% complete...Thick provisioning: 26% complete...Thick provisioning: 33% complete...Thick provisioning: 26% complete...Thick provisioning: 34% complete...Thick provisioning: 37% complete...Thick provisioning: 27% complete...Thick provisioning: 27% complete...Thick provisioning: 35% complete...Thick provisioning: 38% complete...Thick provisioning: 28% complete...Thick provisioning: 36% complete...Thick provisioning: 28% complete...Thick provisioning: 39% complete...Thick provisioning: 29% complete...Thick provisioning: 37% complete...Thick provisioning: 40% complete...Thick provisioning: 29% complete...Thick provisioning: 38% complete...Thick provisioning: 41% complete...Thick provisioning: 30% complete...Thick provisioning: 30% complete...Thick provisioning: 39% complete...Thick provisioning: 42% complete...Thick provisioning: 31% complete...Thick provisioning: 40% complete...Thick provisioning: 43% complete...Thick provisioning: 31% complete...Thick provisioning: 41% complete...Thick provisioning: 44% complete...Thick provisioning: 32% complete...Thick provisioning: 32% complete...Thick provisioning: 45% complete...Thick provisioning: 42% complete...Thick provisioning: 33% complete...Thick provisioning: 46% complete...Thick provisioning: 33% complete...Thick provisioning: 43% complete...Thick provisioning: 34% complete...Thick provisioning: 47% complete...Thick provisioning: 44% complete...Thick provisioning: 48% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 45% complete...Thick provisioning: 49% complete...Thick provisioning: 36% complete...Thick provisioning: 46% complete...Thick provisioning: 35% complete...Thick provisioning: 50% complete...Thick provisioning: 47% complete...Thick provisioning: 51% complete...Thick provisioning: 37% complete...Thick provisioning: 48% complete...Thick provisioning: 36% complete...Thick provisioning: 52% complete...Thick provisioning: 49% complete...Thick provisioning: 38% complete...Thick provisioning: 53% complete...Thick provisioning: 37% complete...Thick provisioning: 50% complete...Thick provisioning: 39% complete...Thick provisioning: 54% complete...Thick provisioning: 38% complete...Thick provisioning: 55% complete...Thick provisioning: 51% complete...Thick provisioning: 40% complete...Thick provisioning: 52% complete...Thick provisioning: 56% complete...Thick provisioning: 39% complete...Thick provisioning: 57% complete...Thick provisioning: 53% complete...Thick provisioning: 41% complete...Thick provisioning: 58% complete...Thick provisioning: 40% complete...Thick provisioning: 54% complete...Thick provisioning: 59% complete...Thick provisioning: 42% complete...Thick provisioning: 55% complete...Thick provisioning: 41% complete...Thick provisioning: 60% complete...Thick provisioning: 56% complete...Thick provisioning: 43% complete...Thick provisioning: 61% complete...Thick provisioning: 42% complete...Thick provisioning: 57% complete...Thick provisioning: 44% complete...Thick provisioning: 62% complete...Thick provisioning: 58% complete...Thick provisioning: 63% complete...Thick provisioning: 43% complete...Thick provisioning: 59% complete...Thick provisioning: 45% complete...Thick provisioning: 64% complete...Thick provisioning: 60% complete...Thick provisioning: 44% complete...Thick provisioning: 46% complete...Thick provisioning: 65% complete...Thick provisioning: 61% complete...Thick provisioning: 45% complete...Thick provisioning: 66% complete...Thick provisioning: 47% complete...Thick provisioning: 62% complete...Thick provisioning: 67% complete...Thick provisioning: 48% complete...Thick provisioning: 46% complete...Thick provisioning: 63% complete...Thick provisioning: 68% complete...Thick provisioning: 49% complete...Thick provisioning: 47% complete...Thick provisioning: 64% complete...Thick provisioning: 69% complete...Thick provisioning: 50% complete...Thick provisioning: 65% complete...Thick provisioning: 48% complete...Thick provisioning: 70% complete...Thick provisioning: 66% complete...Thick provisioning: 51% complete...Thick provisioning: 71% complete...Thick provisioning: 49% complete...Thick provisioning: 67% complete...Thick provisioning: 52% complete...Thick provisioning: 72% complete...Thick provisioning: 68% complete...Thick provisioning: 53% complete...Thick provisioning: 73% complete...Thick provisioning: 50% complete...Thick provisioning: 69% complete...Thick provisioning: 54% complete...Thick provisioning: 74% complete...Thick provisioning: 70% complete...Thick provisioning: 75% complete...Thick provisioning: 51% complete...Thick provisioning: 55% complete...Thick provisioning: 71% complete...Thick provisioning: 76% complete...Thick provisioning: 52% complete...Thick provisioning: 56% complete...Thick provisioning: 72% complete...Thick provisioning: 77% complete...Thick provisioning: 73% complete...Thick provisioning: 57% complete...Thick provisioning: 53% complete...Thick provisioning: 78% complete...Thick provisioning: 74% complete...Thick provisioning: 58% complete...Thick provisioning: 54% complete...Thick provisioning: 79% complete...Thick provisioning: 75% complete...Thick provisioning: 59% complete...Thick provisioning: 80% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 81% complete...Thick provisioning: 55% complete...Thick provisioning: 60% complete...Thick provisioning: 82% complete...Thick provisioning: 78% complete...Thick provisioning: 61% complete...Thick provisioning: 79% complete...Thick provisioning: 56% complete...Thick provisioning: 83% complete...Thick provisioning: 62% complete...Thick provisioning: 80% complete...Thick provisioning: 84% complete...Thick provisioning: 57% complete...Thick provisioning: 63% complete...Thick provisioning: 85% complete...Thick provisioning: 81% complete...Thick provisioning: 86% complete...Thick provisioning: 64% complete...Thick provisioning: 82% complete...Thick provisioning: 58% complete...Thick provisioning: 87% complete...Thick provisioning: 83% complete...Thick provisioning: 65% complete...Thick provisioning: 88% complete...Thick provisioning: 59% complete...Thick provisioning: 84% complete...Thick provisioning: 66% complete...Thick provisioning: 89% complete...Thick provisioning: 60% complete...Thick provisioning: 85% complete...Thick provisioning: 90% complete...Thick provisioning: 67% complete...Thick provisioning: 86% complete...Thick provisioning: 91% complete...Thick provisioning: 61% complete...Thick provisioning: 92% complete...Thick provisioning: 68% complete...Thick provisioning: 62% complete...Thick provisioning: 87% complete...Thick provisioning: 93% complete...Thick provisioning: 88% complete...Thick provisioning: 69% complete...Thick provisioning: 63% complete...Thick provisioning: 94% complete...Thick provisioning: 89% complete...Thick provisioning: 70% complete...Thick provisioning: 64% complete...Thick provisioning: 95% complete...Thick provisioning: 90% complete...Thick provisioning: 71% complete...Thick provisioning: 65% complete...Thick provisioning: 96% complete...Thick provisioning: 91% complete...Thick provisioning: 72% complete...Thick provisioning: 66% complete...Thick provisioning: 92% complete...Thick provisioning: 97% complete...Thick provisioning: 93% complete...Thick provisioning: 98% complete...Thick provisioning: 73% complete...Thick provisioning: 67% complete...Thick provisioning: 99% complete...Thick provisioning: 94% complete...Thick provisioning: 74% complete...Thick provisioning: 68% complete...Thick provisioning: 100% complete...Thick provisioning: 95% complete...Thick provisioning: 75% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 69% complete...Thick provisioning: 96% complete...Thick provisioning: 76% complete...Thick provisioning: 97% complete...Thick provisioning: 70% complete...Thick provisioning: 98% complete...Thick provisioning: 77% complete...Thick provisioning: 71% complete...Thick provisioning: 99% complete...Thick provisioning: 78% complete...Thick provisioning: 72% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 79% complete...Thick provisioning: 73% complete...Thick provisioning: 80% complete...Thick provisioning: 74% complete...Thick provisioning: 81% complete...Thick provisioning: 75% complete...Thick provisioning: 82% complete...Thick provisioning: 76% complete...Thick provisioning: 83% complete...Thick provisioning: 77% complete...Thick provisioning: 84% complete...Thick provisioning: 78% complete...Thick provisioning: 85% complete...Thick provisioning: 79% complete...Thick provisioning: 86% complete...Thick provisioning: 80% complete...Thick provisioning: 87% complete...Thick provisioning: 81% complete...Thick provisioning: 88% complete...Thick provisioning: 89% complete...Thick provisioning: 82% complete...Thick provisioning: 90% complete...Thick provisioning: 83% complete...Thick provisioning: 91% complete...Thick provisioning: 84% complete...Thick provisioning: 92% complete...Thick provisioning: 85% complete...Thick provisioning: 93% complete...Thick provisioning: 86% complete...Thick provisioning: 94% complete...Thick provisioning: 87% complete...Thick provisioning: 95% complete...Thick provisioning: 88% complete...Thick provisioning: 96% complete...Thick provisioning: 89% complete...Thick provisioning: 97% complete...Thick provisioning: 90% complete...Thick provisioning: 98% complete...Thick provisioning: 91% complete...Thick provisioning: 99% complete...Thick provisioning: 92% complete...Thick provisioning: 100% complete...Thick provisioning: 93% complete...Thick provisioning: 100% complete...done.
Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done.
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ fstrim /mnt/nvme
+ launch_gw_background
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ sleep 5
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-14 23:27:17.522870] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-14 23:27:17.522979] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid807551 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-14 23:27:17.616347] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-14 23:27:17.739551] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-14 23:27:17.739625] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-14 23:27:17.739700] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-14 23:27:17.739705] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-14 23:27:23.313914] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-14 23:27:24.044671] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd rbd-benchmark.multi.1
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.1
+ local bdev=bdev_rbd-benchmark.multi.1
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.1 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.1
[2024-01-14 23:27:24.435972] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.1 rbd disk to lun
bdev_rbd-benchmark.multi.1
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.1
+ add_rbd_img triple-hdd rbd-benchmark.multi.2
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.2
+ local bdev=bdev_rbd-benchmark.multi.2
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.2 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.2
[2024-01-14 23:27:25.202861] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.2 rbd disk to lun
bdev_rbd-benchmark.multi.2
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.2
+ add_rbd_img triple-hdd rbd-benchmark.multi.3
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.3
+ local bdev=bdev_rbd-benchmark.multi.3
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.3 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.3
[2024-01-14 23:27:25.953720] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.3 rbd disk to lun
bdev_rbd-benchmark.multi.3
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.3
+ add_rbd_img triple-hdd rbd-benchmark.multi.4
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=rbd-benchmark.multi.4
+ local bdev=bdev_rbd-benchmark.multi.4
+ scripts/rpc.py bdev_rbd_create triple-hdd rbd-benchmark.multi.4 4096 -c rbd_cluster -b bdev_rbd-benchmark.multi.4
[2024-01-14 23:27:26.736145] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_rbd-benchmark.multi.4 rbd disk to lun
bdev_rbd-benchmark.multi.4
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_rbd-benchmark.multi.4
+ trap 'cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:23:56.rbd-multi.triple-hdd.txt multi-client/client-bench-multi.bash read_entire_img=0
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:23:56.rbd-multi.triple-hdd.txt
+ local benchscript=multi-client/client-bench-multi.bash
+ local additional_args=read_entire_img=0
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=0'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:23:56.rbd-multi.triple-hdd.txt
===Starting client benchmark

NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
device: nvme1
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n2          SPDK00000000000001   SPDK_Controller1                         2          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n3          SPDK00000000000001   SPDK_Controller1                         3          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
/dev/nvme1n4          SPDK00000000000001   SPDK_Controller1                         4          10.74  GB /  10.74  GB      4 KiB +  0 B   23.09   
Using device /dev/nvme1n1
/dev/nvme1n2
/dev/nvme1n3
/dev/nvme1n4


===Fio: workload=randread, time=60, iodepth=256, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=4704: Sun Jan 14 23:28:35 2024
  read: IOPS=5136, BW=20.1MiB/s (21.0MB/s)(1223MiB/60941msec)
    slat (usec): min=4, max=377903, avg=761.19, stdev=1603.10
    clat (msec): min=32, max=2099, avg=198.42, stdev=135.86
     lat (msec): min=32, max=2099, avg=199.18, stdev=135.98
    clat percentiles (msec):
     |  1.00th=[   58],  5.00th=[   91], 10.00th=[  112], 20.00th=[  134],
     | 30.00th=[  150], 40.00th=[  163], 50.00th=[  174], 60.00th=[  186],
     | 70.00th=[  201], 80.00th=[  224], 90.00th=[  279], 95.00th=[  376],
     | 99.00th=[  944], 99.50th=[ 1133], 99.90th=[ 1485], 99.95th=[ 1687],
     | 99.99th=[ 1871]
   bw (  KiB/s): min= 5768, max=39072, per=100.00%, avg=20799.40, stdev=1491.38, samples=480
   iops        : min= 1442, max= 9768, avg=5199.87, stdev=372.85, samples=480
  lat (msec)   : 50=0.34%, 100=6.37%, 250=79.61%, 500=11.07%, 750=1.34%
  lat (msec)   : 1000=0.44%, 2000=0.83%, >=2000=0.01%
  cpu          : usr=1.23%, sys=3.63%, ctx=246615, majf=0, minf=1083
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=313012,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=20.1MiB/s (21.0MB/s), 20.1MiB/s-20.1MiB/s (21.0MB/s-21.0MB/s), io=1223MiB (1282MB), run=60941-60941msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=5136, BW=20.1MiB/s (21.0MB/s)(1223MiB/60941msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=1 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process

j1: (groupid=0, jobs=1): err= 0: pid=4753: Sun Jan 14 23:29:38 2024
  read: IOPS=6002, BW=23.4MiB/s (24.6MB/s)(1416MiB/60394msec)
    slat (usec): min=4, max=2266, avg=141.78, stdev=216.20
    clat (usec): min=386, max=974698, avg=21175.99, stdev=57920.31
     lat (usec): min=419, max=975243, avg=21318.31, stdev=57922.83
    clat percentiles (usec):
     |  1.00th=[   529],  5.00th=[   685], 10.00th=[   955], 20.00th=[  3130],
     | 30.00th=[  5014], 40.00th=[  6652], 50.00th=[  8225], 60.00th=[  9765],
     | 70.00th=[ 12387], 80.00th=[ 17695], 90.00th=[ 31589], 95.00th=[ 63701],
     | 99.00th=[379585], 99.50th=[442500], 99.90th=[530580], 99.95th=[566232],
     | 99.99th=[666895]
   bw (  KiB/s): min=18976, max=73664, per=100.00%, avg=24161.20, stdev=6687.20, samples=120
   iops        : min= 4744, max=18416, avg=6040.30, stdev=1671.80, samples=120
  lat (usec)   : 500=0.49%, 750=5.77%, 1000=4.57%
  lat (msec)   : 2=4.40%, 4=9.34%, 10=37.16%, 20=21.21%, 50=10.90%
  lat (msec)   : 100=2.55%, 250=1.72%, 500=1.71%, 750=0.16%, 1000=0.01%
  cpu          : usr=3.95%, sys=10.36%, ctx=257068, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=362545,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=23.4MiB/s (24.6MB/s), 23.4MiB/s-23.4MiB/s (24.6MB/s-24.6MB/s), io=1416MiB (1485MB), run=60394-60394msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=6002, BW=23.4MiB/s (24.6MB/s)(1416MiB/60394msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=2 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 2 processes

j1: (groupid=0, jobs=2): err= 0: pid=4800: Sun Jan 14 23:30:40 2024
  read: IOPS=5767, BW=22.5MiB/s (23.6MB/s)(1362MiB/60461msec)
    slat (usec): min=4, max=137934, avg=314.62, stdev=540.72
    clat (usec): min=375, max=1093.4k, avg=44057.45, stdev=91868.29
     lat (usec): min=434, max=1094.8k, avg=44372.73, stdev=91883.59
    clat percentiles (usec):
     |  1.00th=[    685],  5.00th=[   3261], 10.00th=[   5800],
     | 20.00th=[  10028], 30.00th=[  16450], 40.00th=[  22938],
     | 50.00th=[  27395], 60.00th=[  31065], 70.00th=[  34866],
     | 80.00th=[  40633], 90.00th=[  54264], 95.00th=[ 104334],
     | 99.00th=[ 526386], 99.50th=[ 599786], 99.90th=[ 952108],
     | 99.95th=[ 985662], 99.99th=[1035994]
   bw (  KiB/s): min=11320, max=40792, per=100.00%, avg=23229.00, stdev=2998.69, samples=240
   iops        : min= 2830, max=10198, avg=5807.25, stdev=749.67, samples=240
  lat (usec)   : 500=0.07%, 750=1.26%, 1000=0.98%
  lat (msec)   : 2=0.87%, 4=3.18%, 10=13.59%, 20=15.27%, 50=53.02%
  lat (msec)   : 100=6.65%, 250=1.66%, 500=2.28%, 750=0.86%, 1000=0.29%
  lat (msec)   : 2000=0.04%
  cpu          : usr=2.27%, sys=6.63%, ctx=269980, majf=0, minf=285
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=348689,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=22.5MiB/s (23.6MB/s), 22.5MiB/s-22.5MiB/s (23.6MB/s-23.6MB/s), io=1362MiB (1428MB), run=60461-60461msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=5767, BW=22.5MiB/s (23.6MB/s)(1362MiB/60461msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=3 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 3 processes

j1: (groupid=0, jobs=3): err= 0: pid=4847: Sun Jan 14 23:31:43 2024
  read: IOPS=6039, BW=23.6MiB/s (24.7MB/s)(1420MiB/60186msec)
    slat (usec): min=4, max=467376, avg=465.74, stdev=1181.65
    clat (usec): min=413, max=1821.5k, avg=63100.28, stdev=109303.08
     lat (usec): min=444, max=1822.2k, avg=63566.73, stdev=109336.39
    clat percentiles (usec):
     |  1.00th=[    709],  5.00th=[   4146], 10.00th=[   7832],
     | 20.00th=[  19268], 30.00th=[  32375], 40.00th=[  41157],
     | 50.00th=[  47449], 60.00th=[  52691], 70.00th=[  58459],
     | 80.00th=[  67634], 90.00th=[  89654], 95.00th=[ 156238],
     | 99.00th=[ 541066], 99.50th=[ 935330], 99.90th=[1300235],
     | 99.95th=[1333789], 99.99th=[1417675]
   bw (  KiB/s): min= 7320, max=44464, per=100.00%, avg=24208.40, stdev=2176.63, samples=360
   iops        : min= 1830, max=11116, avg=6052.10, stdev=544.16, samples=360
  lat (usec)   : 500=0.03%, 750=1.24%, 1000=1.00%
  lat (msec)   : 2=0.56%, 4=1.99%, 10=8.47%, 20=7.18%, 50=34.68%
  lat (msec)   : 100=36.54%, 250=5.40%, 500=1.72%, 750=0.53%, 1000=0.18%
  lat (msec)   : 2000=0.47%
  cpu          : usr=1.65%, sys=5.01%, ctx=274241, majf=0, minf=428
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=363507,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=23.6MiB/s (24.7MB/s), 23.6MiB/s-23.6MiB/s (24.7MB/s-24.7MB/s), io=1420MiB (1489MB), run=60186-60186msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=6039, BW=23.6MiB/s (24.7MB/s)(1420MiB/60186msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki, disks=4 ===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j2: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j3: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
j4: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 4 processes

j1: (groupid=0, jobs=4): err= 0: pid=4895: Sun Jan 14 23:32:46 2024
  read: IOPS=6101, BW=23.8MiB/s (25.0MB/s)(1436MiB/60234msec)
    slat (usec): min=4, max=193719, avg=634.65, stdev=1095.58
    clat (usec): min=395, max=1231.2k, avg=83258.94, stdev=89931.60
     lat (usec): min=401, max=1232.4k, avg=83894.36, stdev=89992.45
    clat percentiles (usec):
     |  1.00th=[    816],  5.00th=[   9110], 10.00th=[  23987],
     | 20.00th=[  41157], 30.00th=[  52691], 40.00th=[  62129],
     | 50.00th=[  69731], 60.00th=[  76022], 70.00th=[  84411],
     | 80.00th=[  94897], 90.00th=[ 119014], 95.00th=[ 179307],
     | 99.00th=[ 557843], 99.50th=[ 641729], 99.90th=[ 876610],
     | 99.95th=[1010828], 99.99th=[1061159]
   bw (  KiB/s): min=11240, max=45040, per=100.00%, avg=24469.53, stdev=1530.64, samples=480
   iops        : min= 2810, max=11260, avg=6117.27, stdev=382.67, samples=480
  lat (usec)   : 500=0.01%, 750=0.76%, 1000=0.60%
  lat (msec)   : 2=0.21%, 4=0.80%, 10=3.15%, 20=3.09%, 50=18.66%
  lat (msec)   : 100=56.10%, 250=13.00%, 500=2.12%, 750=1.35%, 1000=0.10%
  lat (msec)   : 2000=0.06%
  cpu          : usr=1.36%, sys=3.99%, ctx=285093, majf=0, minf=571
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=367515,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=23.8MiB/s (25.0MB/s), 23.8MiB/s-23.8MiB/s (25.0MB/s-25.0MB/s), io=1436MiB (1505MB), run=60234-60234msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n2: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n3: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
  nvme1n4: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=6101, BW=23.8MiB/s (25.0MB/s)(1436MiB/60234msec)
umount: /mnt/fsbench: not mounted.
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:23:56.rbd-multi.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:23:56.rbd-multi.triple-hdd.txt
Fio (disks=4, iodepth=256; bs=4ki) randread:  read: IOPS=5136, BW=20.1MiB/s (21.0MB/s)(1223MiB/60941msec)
Fio (disks=1, iodepth=128; bs=4ki) randread:  read: IOPS=6002, BW=23.4MiB/s (24.6MB/s)(1416MiB/60394msec)
Fio (disks=2, iodepth=128; bs=4ki) randread:  read: IOPS=5767, BW=22.5MiB/s (23.6MB/s)(1362MiB/60461msec)
Fio (disks=3, iodepth=128; bs=4ki) randread:  read: IOPS=6039, BW=23.6MiB/s (24.7MB/s)(1420MiB/60186msec)
Fio (disks=4, iodepth=128; bs=4ki) randread:  read: IOPS=6101, BW=23.8MiB/s (25.0MB/s)(1436MiB/60234msec)
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-14T23:32:54
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:32:54.lsvd-nvme.240.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-14T23:32:54
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thick rssd2 lsvd-benchmark 80g
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark
Removing all objects from pool rssd2 with prefix lsvd-benchmark
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 25181/68127 objects
+ ./thick-image --size=80g rssd2/lsvd-benchmark
Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 45% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 80% complete...Thick provisioning: 81% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 87% complete...Thick provisioning: 88% complete...Thick provisioning: 89% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 92% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p rssd2 stat lsvd-benchmark
rssd2/lsvd-benchmark mtime 2024-01-14T23:36:59.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=257698037760
+ LSVD_CACHE_SIZE=257698037760
+ rm -rf /mnt/nvme-remote//lsvd-write/0da4e91c-73ac-485f-9f25-8fecabf45688.wcache /mnt/nvme-remote//lsvd-write/2da42638-ced5-4036-a27e-8e73f1e81a9c.wcache /mnt/nvme-remote//lsvd-write/7ec5c67a-54f7-4fae-a685-6205f8856fe5.wcache /mnt/nvme-remote//lsvd-write/abb33856-e24c-415d-9661-d13abc846e28.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-14 23:37:00.340996] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-14 23:37:00.341169] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid807851 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-14 23:37:00.441710] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-14 23:37:00.586510] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-14 23:37:00.586575] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-14 23:37:00.586640] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-14 23:37:00.586643] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-14 23:37:06.208799] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-14 23:37:06.967641] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 245760 MiB in 16 shards, 15360 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//b94ed7f9-8247-43ed-a572-c72bb2218976.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-14 23:37:20.409163] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 1/2 hits, 0 MiB read, 1.778 read amp, 2 total
[0m+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:32:54.lsvd-nvme.240.rssd2.txt client-bench.bash read_entire_img=1
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:32:54.lsvd-nvme.240.rssd2.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=1
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=1'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:32:54.lsvd-nvme.240.rssd2.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 1 '!=' read_entire_img=1 ']'
+ eval read_entire_img=1
++ read_entire_img=1
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=1


===Reading entire image to warm cache===

+ [[ 1 -eq 1 ]]
+ printf '\n\n===Reading entire image to warm cache===\n\n'
+ dd if=/dev/nvme1n1 of=/dev/null bs=1048576 count=81910
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 10126/30748 hits, 1279 MiB read, 1.007 read amp, 30748 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 26744/81394 hits, 3389 MiB read, 1.008 read amp, 81394 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 43598/132760 hits, 5529 MiB read, 1.008 read amp, 132760 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 184732 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 238114 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 291190 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 345040 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 398290 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 452140 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 505252 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 558694 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 611848 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 665992 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52506/160000 hits, 6666 MiB read, 1.008 read amp, 718858 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 772090 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 825435 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 878689 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 931582 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 984664 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1036498 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1089724 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1143106 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1196842 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1249948 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1301704 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 1355500 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1409002 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1462042 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1515628 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1569004 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1622362 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1675018 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1727422 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1778974 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1833094 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1886674 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1939633 total
[0m81910+0 records in
81910+0 records out
85888860160 bytes (86 GB, 80 GiB) copied, 749.879 s, 115 MB/s
+ set +x


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 2148002 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 3437463 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 4721723 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5155: Sun Jan 14 23:50:57 2024
  read: IOPS=60.7k, BW=237MiB/s (249MB/s)(13.9GiB/60001msec)
    slat (usec): min=4, max=1724, avg=13.79, stdev=53.07
    clat (usec): min=116, max=5253, avg=2091.97, stdev=385.54
     lat (usec): min=127, max=5266, avg=2106.09, stdev=381.96
    clat percentiles (usec):
     |  1.00th=[ 1139],  5.00th=[ 1336], 10.00th=[ 1516], 20.00th=[ 1811],
     | 30.00th=[ 1942], 40.00th=[ 2057], 50.00th=[ 2147], 60.00th=[ 2245],
     | 70.00th=[ 2311], 80.00th=[ 2376], 90.00th=[ 2507], 95.00th=[ 2606],
     | 99.00th=[ 2966], 99.50th=[ 3097], 99.90th=[ 3294], 99.95th=[ 3392],
     | 99.99th=[ 3884]
   bw (  KiB/s): min=227056, max=265368, per=100.00%, avg=242971.04, stdev=6068.91, samples=119
   iops        : min=56764, max=66342, avg=60742.76, stdev=1517.21, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.05%
  lat (msec)   : 2=35.21%, 4=64.72%, 10=0.01%
  cpu          : usr=15.98%, sys=53.05%, ctx=77844, majf=0, minf=140
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3643175,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=237MiB/s (249MB/s), 237MiB/s-237MiB/s (249MB/s-249MB/s), io=13.9GiB (14.9GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=60.7k, BW=237MiB/s (249MB/s)(13.9GiB/60001msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 5882655 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 7385171 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 9023566 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5203: Sun Jan 14 23:52:00 2024
  read: IOPS=75.2k, BW=294MiB/s (308MB/s)(17.2GiB/60001msec)
    slat (usec): min=3, max=1883, avg=10.82, stdev=25.67
    clat (usec): min=137, max=6731, avg=1688.36, stdev=393.62
     lat (usec): min=205, max=6736, avg=1699.53, stdev=394.86
    clat percentiles (usec):
     |  1.00th=[ 1074],  5.00th=[ 1287], 10.00th=[ 1369], 20.00th=[ 1434],
     | 30.00th=[ 1483], 40.00th=[ 1516], 50.00th=[ 1582], 60.00th=[ 1663],
     | 70.00th=[ 1778], 80.00th=[ 1942], 90.00th=[ 2147], 95.00th=[ 2311],
     | 99.00th=[ 2900], 99.50th=[ 3949], 99.90th=[ 5080], 99.95th=[ 5211],
     | 99.99th=[ 5473]
   bw (  KiB/s): min=184368, max=340256, per=99.96%, avg=300844.64, stdev=36841.00, samples=119
   iops        : min=46092, max=85062, avg=75211.09, stdev=9210.26, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.31%
  lat (msec)   : 2=82.43%, 4=16.79%, 10=0.47%
  cpu          : usr=16.89%, sys=71.21%, ctx=62311, majf=0, minf=140
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4514425,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=294MiB/s (308MB/s), 294MiB/s-294MiB/s (308MB/s-308MB/s), io=17.2GiB (18.5GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=75.2k, BW=294MiB/s (308MB/s)(17.2GiB/60001msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 10634013 total
[0m

===Fio: workload=randread, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 10768156 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 589 MiB read, 0.000 read amp, 10908085 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 11053357 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5249: Sun Jan 14 23:53:02 2024
  read: IOPS=6782, BW=26.5MiB/s (27.8MB/s)(1590MiB/60001msec)
    slat (usec): min=6, max=442, avg=11.41, stdev= 6.17
    clat (nsec): min=1150, max=4271.3k, avg=132899.07, stdev=40307.94
     lat (usec): min=96, max=4279, avg=144.59, stdev=41.68
    clat percentiles (usec):
     |  1.00th=[  100],  5.00th=[  104], 10.00th=[  108], 20.00th=[  113],
     | 30.00th=[  118], 40.00th=[  122], 50.00th=[  126], 60.00th=[  130],
     | 70.00th=[  135], 80.00th=[  141], 90.00th=[  157], 95.00th=[  192],
     | 99.00th=[  318], 99.50th=[  392], 99.90th=[  553], 99.95th=[  594],
     | 99.99th=[  693]
   bw (  KiB/s): min=23728, max=31040, per=100.00%, avg=27141.38, stdev=1662.90, samples=119
   iops        : min= 5932, max= 7760, avg=6785.34, stdev=415.73, samples=119
  lat (usec)   : 2=0.01%, 50=0.01%, 100=1.23%, 250=97.18%, 500=1.37%
  lat (usec)   : 750=0.21%, 1000=0.01%
  lat (msec)   : 2=0.01%, 10=0.01%
  cpu          : usr=4.15%, sys=12.16%, ctx=406980, majf=0, minf=12
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=406973,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=26.5MiB/s (27.8MB/s), 26.5MiB/s-26.5MiB/s (27.8MB/s-27.8MB/s), io=1590MiB (1667MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randread:  read: IOPS=6782, BW=26.5MiB/s (27.8MB/s)(1590MiB/60001msec)


===Fio: workload=randread, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 11433027 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 11893620 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 12356446 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5296: Sun Jan 14 23:54:04 2024
  read: IOPS=21.8k, BW=85.1MiB/s (89.2MB/s)(5104MiB/60001msec)
    slat (usec): min=4, max=451, avg=10.27, stdev= 5.44
    clat (usec): min=46, max=2517, avg=170.75, stdev=45.72
     lat (usec): min=94, max=2531, avg=181.30, stdev=45.74
    clat percentiles (usec):
     |  1.00th=[  105],  5.00th=[  117], 10.00th=[  125], 20.00th=[  137],
     | 30.00th=[  147], 40.00th=[  157], 50.00th=[  165], 60.00th=[  176],
     | 70.00th=[  186], 80.00th=[  198], 90.00th=[  221], 95.00th=[  243],
     | 99.00th=[  306], 99.50th=[  379], 99.90th=[  594], 99.95th=[  627],
     | 99.99th=[  693]
   bw (  KiB/s): min=81064, max=93528, per=100.00%, avg=87142.39, stdev=2679.62, samples=119
   iops        : min=20266, max=23382, avg=21785.60, stdev=669.89, samples=119
  lat (usec)   : 50=0.01%, 100=0.24%, 250=95.95%, 500=3.54%, 750=0.27%
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%
  cpu          : usr=11.09%, sys=28.79%, ctx=773161, majf=0, minf=14
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=1306723,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
   READ: bw=85.1MiB/s (89.2MB/s), 85.1MiB/s-85.1MiB/s (89.2MB/s-89.2MB/s), io=5104MiB (5352MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randread:  read: IOPS=21.8k, BW=85.1MiB/s (89.2MB/s)(5104MiB/60001msec)


===Fio: workload=randread, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 12908112 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 13591299 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 14260349 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5342: Sun Jan 14 23:55:07 2024
  read: IOPS=31.8k, BW=124MiB/s (130MB/s)(7455MiB/60001msec)
    slat (usec): min=4, max=455, avg=10.70, stdev= 5.34
    clat (usec): min=93, max=2825, avg=238.14, stdev=51.06
     lat (usec): min=100, max=2835, avg=249.16, stdev=50.68
    clat percentiles (usec):
     |  1.00th=[  143],  5.00th=[  165], 10.00th=[  184], 20.00th=[  202],
     | 30.00th=[  217], 40.00th=[  227], 50.00th=[  235], 60.00th=[  245],
     | 70.00th=[  255], 80.00th=[  269], 90.00th=[  293], 95.00th=[  310],
     | 99.00th=[  392], 99.50th=[  482], 99.90th=[  668], 99.95th=[  693],
     | 99.99th=[  750]
   bw (  KiB/s): min=120144, max=136384, per=100.00%, avg=127265.14, stdev=3878.55, samples=119
   iops        : min=30036, max=34096, avg=31816.27, stdev=969.60, samples=119
  lat (usec)   : 100=0.01%, 250=65.64%, 500=33.90%, 750=0.45%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%
  cpu          : usr=15.00%, sys=37.65%, ctx=620273, majf=0, minf=18
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=1908423,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
   READ: bw=124MiB/s (130MB/s), 124MiB/s-124MiB/s (130MB/s-130MB/s), io=7455MiB (7817MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randread:  read: IOPS=31.8k, BW=124MiB/s (130MB/s)(7455MiB/60001msec)


===Fio: workload=randread, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 15083297 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 16198177 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 17280890 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5392: Sun Jan 14 23:56:09 2024
  read: IOPS=51.5k, BW=201MiB/s (211MB/s)(11.8GiB/60001msec)
    slat (usec): min=4, max=455, avg= 9.81, stdev= 5.78
    clat (usec): min=103, max=4387, avg=298.44, stdev=60.83
     lat (usec): min=112, max=4396, avg=308.55, stdev=60.61
    clat percentiles (usec):
     |  1.00th=[  190],  5.00th=[  217], 10.00th=[  233], 20.00th=[  253],
     | 30.00th=[  269], 40.00th=[  281], 50.00th=[  293], 60.00th=[  306],
     | 70.00th=[  318], 80.00th=[  334], 90.00th=[  367], 95.00th=[  396],
     | 99.00th=[  486], 99.50th=[  611], 99.90th=[  742], 99.95th=[  775],
     | 99.99th=[  848]
   bw (  KiB/s): min=186512, max=222352, per=100.00%, avg=206213.78, stdev=5811.62, samples=119
   iops        : min=46628, max=55588, avg=51553.39, stdev=1452.83, samples=119
  lat (usec)   : 250=17.78%, 500=81.35%, 750=0.78%, 1000=0.08%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=0.01%
  cpu          : usr=19.63%, sys=50.88%, ctx=379125, majf=0, minf=28
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=3090145,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=201MiB/s (211MB/s), 201MiB/s-201MiB/s (211MB/s-211MB/s), io=11.8GiB (12.7GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randread:  read: IOPS=51.5k, BW=201MiB/s (211MB/s)(11.8GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 18369438 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 19694190 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 21137842 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5439: Sun Jan 14 23:57:12 2024
  read: IOPS=65.5k, BW=256MiB/s (268MB/s)(15.0GiB/60001msec)
    slat (usec): min=4, max=476, avg= 8.71, stdev= 6.18
    clat (usec): min=203, max=3606, avg=477.88, stdev=95.76
     lat (usec): min=209, max=3611, avg=486.86, stdev=95.22
    clat percentiles (usec):
     |  1.00th=[  310],  5.00th=[  355], 10.00th=[  371], 20.00th=[  396],
     | 30.00th=[  420], 40.00th=[  441], 50.00th=[  465], 60.00th=[  490],
     | 70.00th=[  519], 80.00th=[  553], 90.00th=[  594], 95.00th=[  644],
     | 99.00th=[  766], 99.50th=[  865], 99.90th=[ 1012], 99.95th=[ 1057],
     | 99.99th=[ 1156]
   bw (  KiB/s): min=225416, max=281512, per=100.00%, avg=261978.02, stdev=14354.21, samples=119
   iops        : min=56354, max=70378, avg=65494.49, stdev=3588.55, samples=119
  lat (usec)   : 250=0.05%, 500=63.57%, 750=35.20%, 1000=1.07%
  lat (msec)   : 2=0.11%, 4=0.01%
  cpu          : usr=19.80%, sys=56.59%, ctx=220191, majf=0, minf=40
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=3928817,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=256MiB/s (268MB/s), 256MiB/s-256MiB/s (268MB/s-268MB/s), io=15.0GiB (16.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randread:  read: IOPS=65.5k, BW=256MiB/s (268MB/s)(15.0GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 22358145 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 23799314 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 25310011 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5485: Sun Jan 14 23:58:14 2024
  read: IOPS=67.5k, BW=263MiB/s (276MB/s)(15.4GiB/60001msec)
    slat (usec): min=4, max=469, avg= 7.83, stdev= 6.28
    clat (usec): min=212, max=4513, avg=938.83, stdev=190.32
     lat (usec): min=226, max=4520, avg=946.93, stdev=189.48
    clat percentiles (usec):
     |  1.00th=[  529],  5.00th=[  627], 10.00th=[  693], 20.00th=[  783],
     | 30.00th=[  848], 40.00th=[  889], 50.00th=[  938], 60.00th=[  979],
     | 70.00th=[ 1029], 80.00th=[ 1090], 90.00th=[ 1172], 95.00th=[ 1254],
     | 99.00th=[ 1418], 99.50th=[ 1500], 99.90th=[ 1680], 99.95th=[ 1762],
     | 99.99th=[ 1942]
   bw (  KiB/s): min=229280, max=292664, per=100.00%, avg=269905.82, stdev=18043.30, samples=119
   iops        : min=57320, max=73166, avg=67476.45, stdev=4510.86, samples=119
  lat (usec)   : 250=0.01%, 500=0.43%, 750=15.79%, 1000=48.16%
  lat (msec)   : 2=35.61%, 4=0.01%, 10=0.01%
  cpu          : usr=18.39%, sys=53.79%, ctx=162861, majf=0, minf=74
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=4047410,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=263MiB/s (276MB/s), 263MiB/s-263MiB/s (276MB/s-276MB/s), io=15.4GiB (16.6GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randread:  read: IOPS=67.5k, BW=263MiB/s (276MB/s)(15.4GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 26538289 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 27969066 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 29348843 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5532: Sun Jan 14 23:59:16 2024
  read: IOPS=66.0k, BW=258MiB/s (270MB/s)(15.1GiB/60001msec)
    slat (usec): min=4, max=1489, avg=12.45, stdev=36.46
    clat (usec): min=145, max=6363, avg=1923.94, stdev=299.41
     lat (usec): min=162, max=6370, avg=1936.72, stdev=297.93
    clat percentiles (usec):
     |  1.00th=[ 1139],  5.00th=[ 1385], 10.00th=[ 1532], 20.00th=[ 1713],
     | 30.00th=[ 1795], 40.00th=[ 1876], 50.00th=[ 1942], 60.00th=[ 2008],
     | 70.00th=[ 2073], 80.00th=[ 2147], 90.00th=[ 2278], 95.00th=[ 2376],
     | 99.00th=[ 2704], 99.50th=[ 2835], 99.90th=[ 3097], 99.95th=[ 3163],
     | 99.99th=[ 3359]
   bw (  KiB/s): min=232440, max=296072, per=100.00%, avg=264124.30, stdev=14480.79, samples=119
   iops        : min=58110, max=74018, avg=66031.06, stdev=3620.21, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.08%
  lat (msec)   : 2=59.03%, 4=40.89%, 10=0.01%
  cpu          : usr=16.71%, sys=60.39%, ctx=105448, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3961425,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=258MiB/s (270MB/s), 258MiB/s-258MiB/s (270MB/s-270MB/s), io=15.1GiB (16.2GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=66.0k, BW=258MiB/s (270MB/s)(15.1GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 30571056 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 32042317 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 33492648 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5581: Mon Jan 15 00:00:19 2024
  read: IOPS=68.6k, BW=268MiB/s (281MB/s)(15.7GiB/60001msec)
    slat (usec): min=4, max=3535, avg=12.00, stdev=37.11
    clat (usec): min=793, max=10402, avg=3715.49, stdev=414.88
     lat (usec): min=801, max=10412, avg=3727.79, stdev=414.94
    clat percentiles (usec):
     |  1.00th=[ 2933],  5.00th=[ 3163], 10.00th=[ 3261], 20.00th=[ 3392],
     | 30.00th=[ 3458], 40.00th=[ 3556], 50.00th=[ 3621], 60.00th=[ 3752],
     | 70.00th=[ 3884], 80.00th=[ 4080], 90.00th=[ 4293], 95.00th=[ 4490],
     | 99.00th=[ 4817], 99.50th=[ 4948], 99.90th=[ 5604], 99.95th=[ 6194],
     | 99.99th=[ 7177]
   bw (  KiB/s): min=242504, max=295064, per=100.00%, avg=274835.21, stdev=12907.00, samples=119
   iops        : min=60626, max=73766, avg=68708.82, stdev=3226.77, samples=119
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.01%, 4=76.27%, 10=23.73%, 20=0.01%
  cpu          : usr=17.41%, sys=59.03%, ctx=99040, majf=0, minf=265
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4118212,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=268MiB/s (281MB/s), 268MiB/s-268MiB/s (281MB/s-281MB/s), io=15.7GiB (16.9GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randread:  read: IOPS=68.6k, BW=268MiB/s (281MB/s)(15.7GiB/60001msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 34827445 total
[0m

===Fio: workload=randread, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 36144882 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 37542236 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 38937551 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5704: Mon Jan 15 00:01:21 2024
  read: IOPS=65.2k, BW=255MiB/s (267MB/s)(14.9GiB/60001msec)
    slat (usec): min=4, max=1668, avg=12.58, stdev=33.20
    clat (usec): min=667, max=15628, avg=7837.71, stdev=759.95
     lat (usec): min=674, max=15646, avg=7850.63, stdev=761.13
    clat percentiles (usec):
     |  1.00th=[ 6587],  5.00th=[ 6849], 10.00th=[ 6980], 20.00th=[ 7242],
     | 30.00th=[ 7504], 40.00th=[ 7701], 50.00th=[ 7832], 60.00th=[ 7963],
     | 70.00th=[ 8094], 80.00th=[ 8291], 90.00th=[ 8586], 95.00th=[ 8848],
     | 99.00th=[11207], 99.50th=[11994], 99.90th=[12911], 99.95th=[13304],
     | 99.99th=[13566]
   bw (  KiB/s): min=204344, max=293824, per=100.00%, avg=260931.50, stdev=16697.26, samples=119
   iops        : min=51086, max=73456, avg=65232.89, stdev=4174.33, samples=119
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=98.40%, 20=1.60%
  cpu          : usr=16.96%, sys=62.30%, ctx=109817, majf=0, minf=521
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3911741,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
   READ: bw=255MiB/s (267MB/s), 255MiB/s-255MiB/s (267MB/s-267MB/s), io=14.9GiB (16.0GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randread:  read: IOPS=65.2k, BW=255MiB/s (267MB/s)(14.9GiB/60001msec)


===Fio: workload=read, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 40422639 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 42082670 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 43738143 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5751: Mon Jan 15 00:02:23 2024
  read: IOPS=78.6k, BW=307MiB/s (322MB/s)(18.0GiB/60001msec)
    slat (usec): min=3, max=454, avg= 7.94, stdev= 5.78
    clat (usec): min=149, max=5891, avg=397.58, stdev=71.11
     lat (usec): min=161, max=5897, avg=405.78, stdev=70.70
    clat percentiles (usec):
     |  1.00th=[  285],  5.00th=[  310], 10.00th=[  322], 20.00th=[  343],
     | 30.00th=[  359], 40.00th=[  375], 50.00th=[  388], 60.00th=[  404],
     | 70.00th=[  424], 80.00th=[  445], 90.00th=[  482], 95.00th=[  515],
     | 99.00th=[  611], 99.50th=[  734], 99.90th=[  857], 99.95th=[  889],
     | 99.99th=[  971]
   bw (  KiB/s): min=295704, max=328984, per=100.00%, avg=314518.25, stdev=5273.36, samples=119
   iops        : min=73926, max=82246, avg=78629.68, stdev=1318.34, samples=119
  lat (usec)   : 250=0.12%, 500=93.39%, 750=6.05%, 1000=0.44%
  lat (msec)   : 2=0.01%, 10=0.01%
  cpu          : usr=19.41%, sys=61.78%, ctx=223188, majf=0, minf=45
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=4715458,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=307MiB/s (322MB/s), 307MiB/s-307MiB/s (322MB/s-322MB/s), io=18.0GiB (19.3GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) read:  read: IOPS=78.6k, BW=307MiB/s (322MB/s)(18.0GiB/60001msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 45279909 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 46982440 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 48726412 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5797: Mon Jan 15 00:03:26 2024
  read: IOPS=81.8k, BW=319MiB/s (335MB/s)(18.7GiB/60001msec)
    slat (usec): min=3, max=1218, avg=10.28, stdev=25.87
    clat (usec): min=320, max=4661, avg=1553.66, stdev=246.67
     lat (usec): min=326, max=4673, avg=1564.20, stdev=246.14
    clat percentiles (usec):
     |  1.00th=[  988],  5.00th=[ 1123], 10.00th=[ 1237], 20.00th=[ 1385],
     | 30.00th=[ 1450], 40.00th=[ 1500], 50.00th=[ 1549], 60.00th=[ 1598],
     | 70.00th=[ 1663], 80.00th=[ 1745], 90.00th=[ 1844], 95.00th=[ 1975],
     | 99.00th=[ 2212], 99.50th=[ 2311], 99.90th=[ 2507], 99.95th=[ 2606],
     | 99.99th=[ 2933]
   bw (  KiB/s): min=271560, max=353264, per=100.00%, avg=327101.39, stdev=20167.05, samples=119
   iops        : min=67892, max=88316, avg=81775.34, stdev=5041.74, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=1.16%
  lat (msec)   : 2=94.54%, 4=4.30%, 10=0.01%
  cpu          : usr=17.84%, sys=59.41%, ctx=143945, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4905585,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=319MiB/s (335MB/s), 319MiB/s-319MiB/s (335MB/s-335MB/s), io=18.7GiB (20.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=81.8k, BW=319MiB/s (335MB/s)(18.7GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 50029977 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 51414689 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 52823943 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5843: Mon Jan 15 00:04:28 2024
  read: IOPS=65.6k, BW=256MiB/s (269MB/s)(15.0GiB/60001msec)
    slat (usec): min=4, max=1866, avg=12.51, stdev=40.41
    clat (usec): min=123, max=7384, avg=1935.19, stdev=318.67
     lat (usec): min=147, max=7390, avg=1948.03, stdev=316.69
    clat percentiles (usec):
     |  1.00th=[ 1172],  5.00th=[ 1418], 10.00th=[ 1532], 20.00th=[ 1713],
     | 30.00th=[ 1778], 40.00th=[ 1844], 50.00th=[ 1909], 60.00th=[ 1991],
     | 70.00th=[ 2089], 80.00th=[ 2212], 90.00th=[ 2343], 95.00th=[ 2442],
     | 99.00th=[ 2737], 99.50th=[ 2835], 99.90th=[ 3097], 99.95th=[ 3195],
     | 99.99th=[ 3425]
   bw (  KiB/s): min=223352, max=284896, per=100.00%, avg=262622.72, stdev=15970.92, samples=119
   iops        : min=55838, max=71224, avg=65655.70, stdev=3992.71, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.03%
  lat (msec)   : 2=60.20%, 4=39.76%, 10=0.01%
  cpu          : usr=16.81%, sys=59.25%, ctx=91484, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3938425,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=256MiB/s (269MB/s), 256MiB/s-256MiB/s (269MB/s-269MB/s), io=15.0GiB (16.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=65.6k, BW=256MiB/s (269MB/s)(15.0GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 53998757 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1110 MiB read, 0.000 read amp, 55193677 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1109 MiB read, 0.000 read amp, 56364803 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5890: Mon Jan 15 00:05:30 2024
  read: IOPS=53.1k, BW=415MiB/s (435MB/s)(24.3GiB/60002msec)
    slat (usec): min=4, max=2106, avg=15.91, stdev=75.39
    clat (usec): min=248, max=6216, avg=2392.30, stdev=539.90
     lat (usec): min=267, max=6376, avg=2408.53, stdev=536.82
    clat percentiles (usec):
     |  1.00th=[ 1237],  5.00th=[ 1418], 10.00th=[ 1598], 20.00th=[ 1876],
     | 30.00th=[ 2089], 40.00th=[ 2278], 50.00th=[ 2507], 60.00th=[ 2638],
     | 70.00th=[ 2769], 80.00th=[ 2868], 90.00th=[ 2999], 95.00th=[ 3130],
     | 99.00th=[ 3425], 99.50th=[ 3589], 99.90th=[ 3949], 99.95th=[ 4113],
     | 99.99th=[ 4555]
   bw (  KiB/s): min=380416, max=513472, per=100.00%, avg=424834.15, stdev=32986.06, samples=119
   iops        : min=47552, max=64184, avg=53104.27, stdev=4123.32, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=25.52%, 4=74.38%, 10=0.08%
  cpu          : usr=15.53%, sys=48.95%, ctx=67362, majf=0, minf=264
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3185795,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=415MiB/s (435MB/s), 415MiB/s-415MiB/s (435MB/s-435MB/s), io=24.3GiB (26.1GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randread:  read: IOPS=53.1k, BW=415MiB/s (435MB/s)(24.3GiB/60002msec)


===Fio: workload=randread, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1998 MiB read, 0.000 read amp, 57427296 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1998 MiB read, 0.000 read amp, 58626360 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1998 MiB read, 0.000 read amp, 59819354 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5940: Mon Jan 15 00:06:33 2024
  read: IOPS=47.9k, BW=748MiB/s (785MB/s)(43.9GiB/60003msec)
    slat (usec): min=4, max=1969, avg=18.07, stdev=34.90
    clat (usec): min=1533, max=5879, avg=2651.46, stdev=277.15
     lat (usec): min=1543, max=6083, avg=2669.86, stdev=278.11
    clat percentiles (usec):
     |  1.00th=[ 2180],  5.00th=[ 2343], 10.00th=[ 2409], 20.00th=[ 2474],
     | 30.00th=[ 2507], 40.00th=[ 2573], 50.00th=[ 2606], 60.00th=[ 2671],
     | 70.00th=[ 2737], 80.00th=[ 2802], 90.00th=[ 2933], 95.00th=[ 3064],
     | 99.00th=[ 3884], 99.50th=[ 4113], 99.90th=[ 4555], 99.95th=[ 4686],
     | 99.99th=[ 4948]
   bw (  KiB/s): min=669349, max=799296, per=100.00%, avg=766770.60, stdev=20334.03, samples=119
   iops        : min=41834, max=49956, avg=47923.14, stdev=1270.93, samples=119
  lat (msec)   : 2=0.44%, 4=98.85%, 10=0.70%
  cpu          : usr=16.14%, sys=52.17%, ctx=493302, majf=0, minf=523
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=2874344,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=748MiB/s (785MB/s), 748MiB/s-748MiB/s (785MB/s-785MB/s), io=43.9GiB (47.1GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randread:  read: IOPS=47.9k, BW=748MiB/s (785MB/s)(43.9GiB/60003msec)


===Fio: workload=randread, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3329 MiB read, 0.000 read amp, 60807073 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3335 MiB read, 0.000 read amp, 61717091 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3332 MiB read, 0.000 read amp, 62641020 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=5991: Mon Jan 15 00:07:35 2024
  read: IOPS=30.6k, BW=957MiB/s (1003MB/s)(56.1GiB/60005msec)
    slat (usec): min=5, max=2389, avg=29.43, stdev=29.86
    clat (usec): min=355, max=8893, avg=4149.16, stdev=334.67
     lat (usec): min=406, max=9042, avg=4178.94, stdev=336.73
    clat percentiles (usec):
     |  1.00th=[ 3752],  5.00th=[ 3818], 10.00th=[ 3884], 20.00th=[ 3949],
     | 30.00th=[ 3982], 40.00th=[ 4047], 50.00th=[ 4080], 60.00th=[ 4146],
     | 70.00th=[ 4228], 80.00th=[ 4293], 90.00th=[ 4424], 95.00th=[ 4686],
     | 99.00th=[ 5538], 99.50th=[ 5866], 99.90th=[ 7111], 99.95th=[ 7832],
     | 99.99th=[ 8717]
   bw (  KiB/s): min=894080, max=1013184, per=100.00%, avg=979973.27, stdev=25342.50, samples=119
   iops        : min=27940, max=31662, avg=30624.19, stdev=791.99, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.03%, 4=31.55%, 10=68.41%
  cpu          : usr=12.20%, sys=36.06%, ctx=1048569, majf=0, minf=1035
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1836775,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=957MiB/s (1003MB/s), 957MiB/s-957MiB/s (1003MB/s-1003MB/s), io=56.1GiB (60.2GB), run=60005-60005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randread:  read: IOPS=30.6k, BW=957MiB/s (1003MB/s)(56.1GiB/60005msec)


===Fio: workload=randread, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4368 MiB read, 0.000 read amp, 63424540 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4998 MiB read, 0.000 read amp, 64133398 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5001 MiB read, 0.000 read amp, 64832664 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6037: Mon Jan 15 00:08:37 2024
  read: IOPS=17.7k, BW=1109MiB/s (1162MB/s)(65.0GiB/60007msec)
    slat (usec): min=6, max=1747, avg=53.76, stdev=27.43
    clat (usec): min=398, max=13537, avg=7159.96, stdev=602.92
     lat (usec): min=504, max=13610, avg=7214.06, stdev=607.05
    clat percentiles (usec):
     |  1.00th=[ 6652],  5.00th=[ 6718], 10.00th=[ 6718], 20.00th=[ 6783],
     | 30.00th=[ 6849], 40.00th=[ 6915], 50.00th=[ 6980], 60.00th=[ 7111],
     | 70.00th=[ 7242], 80.00th=[ 7439], 90.00th=[ 7701], 95.00th=[ 8094],
     | 99.00th=[ 9634], 99.50th=[10290], 99.90th=[11731], 99.95th=[11731],
     | 99.99th=[11994]
   bw (  MiB/s): min= 1000, max= 1165, per=100.00%, avg=1108.96, stdev=44.12, samples=119
   iops        : min=16002, max=18644, avg=17743.34, stdev=705.85, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.03%, 4=0.07%, 10=99.19%, 20=0.69%
  cpu          : usr=7.54%, sys=22.47%, ctx=1062775, majf=0, minf=2058
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1064389,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=1109MiB/s (1162MB/s), 1109MiB/s-1109MiB/s (1162MB/s-1162MB/s), io=65.0GiB (69.8GB), run=60007-60007msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randread:  read: IOPS=17.7k, BW=1109MiB/s (1162MB/s)(65.0GiB/60007msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3867 MiB read, 0.000 read amp, 65494936 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 67147635 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 68908614 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6083: Mon Jan 15 00:09:40 2024
  read: IOPS=79.0k, BW=309MiB/s (324MB/s)(18.1GiB/60001msec)
    slat (usec): min=3, max=3995, avg=10.59, stdev=27.13
    clat (usec): min=282, max=5667, avg=1607.08, stdev=266.84
     lat (usec): min=293, max=5691, avg=1617.95, stdev=266.50
    clat percentiles (usec):
     |  1.00th=[ 1012],  5.00th=[ 1156], 10.00th=[ 1270], 20.00th=[ 1401],
     | 30.00th=[ 1483], 40.00th=[ 1532], 50.00th=[ 1598], 60.00th=[ 1663],
     | 70.00th=[ 1729], 80.00th=[ 1811], 90.00th=[ 1942], 95.00th=[ 2073],
     | 99.00th=[ 2278], 99.50th=[ 2442], 99.90th=[ 2737], 99.95th=[ 2769],
     | 99.99th=[ 3097]
   bw (  KiB/s): min=259440, max=358704, per=99.98%, avg=316115.83, stdev=24645.79, samples=119
   iops        : min=64858, max=89676, avg=79029.01, stdev=6161.55, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.82%
  lat (msec)   : 2=91.79%, 4=7.39%, 10=0.01%
  cpu          : usr=17.82%, sys=59.66%, ctx=141899, majf=0, minf=137
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4742628,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=309MiB/s (324MB/s), 309MiB/s-309MiB/s (324MB/s-324MB/s), io=18.1GiB (19.4GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=79.0k, BW=309MiB/s (324MB/s)(18.1GiB/60001msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 70492826 total
[0m

===Fio: workload=read, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 71843993 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 73220255 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 74561316 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6132: Mon Jan 15 00:10:42 2024
  read: IOPS=62.2k, BW=486MiB/s (509MB/s)(28.5GiB/60002msec)
    slat (usec): min=4, max=2959, avg=13.69, stdev=56.85
    clat (usec): min=184, max=5439, avg=2043.41, stdev=478.97
     lat (usec): min=194, max=5445, avg=2057.40, stdev=477.84
    clat percentiles (usec):
     |  1.00th=[ 1156],  5.00th=[ 1270], 10.00th=[ 1401], 20.00th=[ 1614],
     | 30.00th=[ 1745], 40.00th=[ 1876], 50.00th=[ 2008], 60.00th=[ 2180],
     | 70.00th=[ 2343], 80.00th=[ 2507], 90.00th=[ 2671], 95.00th=[ 2802],
     | 99.00th=[ 3097], 99.50th=[ 3228], 99.90th=[ 3523], 99.95th=[ 3654],
     | 99.99th=[ 4015]
   bw (  KiB/s): min=400032, max=625728, per=99.95%, avg=497059.76, stdev=60617.60, samples=119
   iops        : min=50004, max=78216, avg=62132.49, stdev=7577.20, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=49.05%, 4=50.91%, 10=0.01%
  cpu          : usr=15.04%, sys=52.95%, ctx=88154, majf=0, minf=266
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3729889,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=486MiB/s (509MB/s), 486MiB/s-486MiB/s (509MB/s-509MB/s), io=28.5GiB (30.6GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) read:  read: IOPS=62.2k, BW=486MiB/s (509MB/s)(28.5GiB/60002msec)


===Fio: workload=read, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 75752087 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 77034648 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 78342713 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6179: Mon Jan 15 00:11:44 2024
  read: IOPS=52.4k, BW=818MiB/s (858MB/s)(48.0GiB/60003msec)
    slat (usec): min=4, max=1800, avg=16.74, stdev=26.70
    clat (usec): min=470, max=7136, avg=2425.15, stdev=234.97
     lat (usec): min=486, max=7143, avg=2442.21, stdev=236.16
    clat percentiles (usec):
     |  1.00th=[ 2040],  5.00th=[ 2114], 10.00th=[ 2180], 20.00th=[ 2245],
     | 30.00th=[ 2311], 40.00th=[ 2343], 50.00th=[ 2409], 60.00th=[ 2442],
     | 70.00th=[ 2507], 80.00th=[ 2573], 90.00th=[ 2704], 95.00th=[ 2835],
     | 99.00th=[ 3195], 99.50th=[ 3556], 99.90th=[ 4015], 99.95th=[ 4178],
     | 99.99th=[ 4490]
   bw (  KiB/s): min=714016, max=930304, per=100.00%, avg=838483.36, stdev=36253.61, samples=119
   iops        : min=44626, max=58144, avg=52405.29, stdev=2265.91, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.51%, 4=99.39%, 10=0.10%
  cpu          : usr=15.79%, sys=54.80%, ctx=584901, majf=0, minf=522
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3142642,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=818MiB/s (858MB/s), 818MiB/s-818MiB/s (858MB/s-858MB/s), io=48.0GiB (51.5GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) read:  read: IOPS=52.4k, BW=818MiB/s (858MB/s)(48.0GiB/60003msec)


===Fio: workload=read, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3333 MiB read, 0.000 read amp, 79301610 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3333 MiB read, 0.000 read amp, 80292715 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3333 MiB read, 0.000 read amp, 81292053 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6227: Mon Jan 15 00:12:47 2024
  read: IOPS=33.5k, BW=1047MiB/s (1098MB/s)(61.4GiB/60004msec)
    slat (usec): min=5, max=2531, avg=27.61, stdev=27.53
    clat (usec): min=290, max=9082, avg=3790.47, stdev=335.39
     lat (usec): min=321, max=9108, avg=3818.36, stdev=337.44
    clat percentiles (usec):
     |  1.00th=[ 3392],  5.00th=[ 3425], 10.00th=[ 3490], 20.00th=[ 3556],
     | 30.00th=[ 3654], 40.00th=[ 3687], 50.00th=[ 3752], 60.00th=[ 3785],
     | 70.00th=[ 3851], 80.00th=[ 3949], 90.00th=[ 4113], 95.00th=[ 4293],
     | 99.00th=[ 5211], 99.50th=[ 5669], 99.90th=[ 6390], 99.95th=[ 6849],
     | 99.99th=[ 7635]
   bw (  MiB/s): min=  898, max= 1145, per=99.98%, avg=1046.92, stdev=42.74, samples=119
   iops        : min=28756, max=36642, avg=33501.43, stdev=1367.52, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.03%, 4=84.38%, 10=15.57%
  cpu          : usr=10.32%, sys=36.31%, ctx=1145108, majf=0, minf=1034
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=2010532,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=1047MiB/s (1098MB/s), 1047MiB/s-1047MiB/s (1098MB/s-1098MB/s), io=61.4GiB (65.9GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) read:  read: IOPS=33.5k, BW=1047MiB/s (1098MB/s)(61.4GiB/60004msec)


===Fio: workload=read, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5000 MiB read, 0.000 read amp, 82031607 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5000 MiB read, 0.000 read amp, 82749363 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5000 MiB read, 0.000 read amp, 83477043 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6274: Mon Jan 15 00:13:49 2024
  read: IOPS=18.1k, BW=1128MiB/s (1183MB/s)(66.1GiB/60007msec)
    slat (usec): min=6, max=4097, avg=53.13, stdev=24.67
    clat (usec): min=362, max=16834, avg=7034.35, stdev=596.07
     lat (usec): min=448, max=16939, avg=7087.82, stdev=600.19
    clat percentiles (usec):
     |  1.00th=[ 6652],  5.00th=[ 6718], 10.00th=[ 6783], 20.00th=[ 6783],
     | 30.00th=[ 6849], 40.00th=[ 6849], 50.00th=[ 6915], 60.00th=[ 6915],
     | 70.00th=[ 6980], 80.00th=[ 7111], 90.00th=[ 7373], 95.00th=[ 7701],
     | 99.00th=[ 9896], 99.50th=[11207], 99.90th=[11863], 99.95th=[11994],
     | 99.99th=[13960]
   bw (  MiB/s): min=  992, max= 1170, per=100.00%, avg=1128.64, stdev=32.51, samples=119
   iops        : min=15872, max=18722, avg=18058.17, stdev=520.13, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=0.04%, 4=0.08%, 10=98.90%, 20=0.96%
  cpu          : usr=6.95%, sys=22.47%, ctx=1081639, majf=0, minf=2059
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1083387,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=1128MiB/s (1183MB/s), 1128MiB/s-1128MiB/s (1183MB/s-1183MB/s), io=66.1GiB (71.0GB), run=60007-60007msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) read:  read: IOPS=18.1k, BW=1128MiB/s (1183MB/s)(66.1GiB/60007msec)


===Fio: workload=randwrite, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4999 MiB read, 0.000 read amp, 83799893 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6324: Mon Jan 15 00:14:52 2024
  write: IOPS=5752, BW=22.5MiB/s (23.6MB/s)(1348MiB/60001msec); 0 zone resets
    slat (usec): min=7, max=469, avg=14.58, stdev= 8.53
    clat (nsec): min=1066, max=3809.9k, avg=155803.79, stdev=66532.22
     lat (usec): min=110, max=3818, avg=170.67, stdev=67.80
    clat percentiles (usec):
     |  1.00th=[  118],  5.00th=[  123], 10.00th=[  127], 20.00th=[  133],
     | 30.00th=[  137], 40.00th=[  139], 50.00th=[  143], 60.00th=[  145],
     | 70.00th=[  149], 80.00th=[  155], 90.00th=[  169], 95.00th=[  273],
     | 99.00th=[  437], 99.50th=[  490], 99.90th=[  644], 99.95th=[  979],
     | 99.99th=[ 1876]
   bw (  KiB/s): min=20912, max=25680, per=99.99%, avg=23008.81, stdev=1061.08, samples=119
   iops        : min= 5228, max= 6420, avg=5752.20, stdev=265.27, samples=119
  lat (usec)   : 2=0.01%, 20=0.01%, 100=0.01%, 250=94.49%, 500=5.07%
  lat (usec)   : 750=0.38%, 1000=0.02%
  lat (msec)   : 2=0.05%, 4=0.01%
  cpu          : usr=4.26%, sys=12.46%, ctx=345165, majf=0, minf=12
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,345164,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=22.5MiB/s (23.6MB/s), 22.5MiB/s-22.5MiB/s (23.6MB/s-23.6MB/s), io=1348MiB (1414MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5752, BW=22.5MiB/s (23.6MB/s)(1348MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4996 MiB read, 0.000 read amp, 83800029 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6378: Mon Jan 15 00:15:54 2024
  write: IOPS=17.7k, BW=69.1MiB/s (72.5MB/s)(4149MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=468, avg= 9.44, stdev= 5.91
    clat (usec): min=29, max=5142, avg=213.83, stdev=96.76
     lat (usec): min=110, max=5147, avg=223.50, stdev=96.47
    clat percentiles (usec):
     |  1.00th=[  130],  5.00th=[  143], 10.00th=[  153], 20.00th=[  167],
     | 30.00th=[  176], 40.00th=[  184], 50.00th=[  192], 60.00th=[  202],
     | 70.00th=[  215], 80.00th=[  233], 90.00th=[  302], 95.00th=[  363],
     | 99.00th=[  502], 99.50th=[  611], 99.90th=[ 1516], 99.95th=[ 1680],
     | 99.99th=[ 2008]
   bw (  KiB/s): min=66152, max=75680, per=100.00%, avg=70824.67, stdev=2504.02, samples=119
   iops        : min=16538, max=18920, avg=17706.18, stdev=626.01, samples=119
  lat (usec)   : 50=0.01%, 100=0.01%, 250=83.81%, 500=15.16%, 750=0.73%
  lat (usec)   : 1000=0.06%
  lat (msec)   : 2=0.23%, 4=0.01%, 10=0.01%
  cpu          : usr=8.65%, sys=21.63%, ctx=421097, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1062101,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
  WRITE: bw=69.1MiB/s (72.5MB/s), 69.1MiB/s-69.1MiB/s (72.5MB/s-72.5MB/s), io=4149MiB (4350MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=17.7k, BW=69.1MiB/s (72.5MB/s)(4149MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4994 MiB read, 0.000 read amp, 83800181 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6435: Mon Jan 15 00:16:56 2024
  write: IOPS=27.7k, BW=108MiB/s (113MB/s)(6490MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=559, avg= 9.68, stdev= 6.08
    clat (usec): min=71, max=3874, avg=276.70, stdev=112.29
     lat (usec): min=103, max=3880, avg=286.65, stdev=112.21
    clat percentiles (usec):
     |  1.00th=[  139],  5.00th=[  169], 10.00th=[  186], 20.00th=[  206],
     | 30.00th=[  223], 40.00th=[  239], 50.00th=[  255], 60.00th=[  273],
     | 70.00th=[  293], 80.00th=[  330], 90.00th=[  396], 95.00th=[  445],
     | 99.00th=[  594], 99.50th=[  865], 99.90th=[ 1418], 99.95th=[ 1582],
     | 99.99th=[ 2245]
   bw (  KiB/s): min=99288, max=119968, per=100.00%, avg=110894.99, stdev=4385.44, samples=119
   iops        : min=24822, max=29992, avg=27723.75, stdev=1096.36, samples=119
  lat (usec)   : 100=0.01%, 250=47.30%, 500=50.71%, 750=1.41%, 1000=0.13%
  lat (msec)   : 2=0.43%, 4=0.02%
  cpu          : usr=12.53%, sys=31.91%, ctx=472997, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1661567,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
  WRITE: bw=108MiB/s (113MB/s), 108MiB/s-108MiB/s (113MB/s-113MB/s), io=6490MiB (6806MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=27.7k, BW=108MiB/s (113MB/s)(6490MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4991 MiB read, 0.000 read amp, 83800341 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6489: Mon Jan 15 00:17:59 2024
  write: IOPS=39.4k, BW=154MiB/s (161MB/s)(9228MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=476, avg= 7.65, stdev= 5.25
    clat (usec): min=99, max=5296, avg=396.78, stdev=169.73
     lat (usec): min=114, max=5300, avg=404.64, stdev=169.55
    clat percentiles (usec):
     |  1.00th=[  155],  5.00th=[  212], 10.00th=[  235], 20.00th=[  277],
     | 30.00th=[  314], 40.00th=[  347], 50.00th=[  375], 60.00th=[  408],
     | 70.00th=[  445], 80.00th=[  494], 90.00th=[  553], 95.00th=[  611],
     | 99.00th=[ 1020], 99.50th=[ 1418], 99.90th=[ 1975], 99.95th=[ 2343],
     | 99.99th=[ 2769]
   bw (  KiB/s): min=145784, max=170240, per=100.00%, avg=157564.91, stdev=4990.16, samples=119
   iops        : min=36446, max=42560, avg=39391.18, stdev=1247.59, samples=119
  lat (usec)   : 100=0.01%, 250=13.69%, 500=67.68%, 750=17.11%, 1000=0.51%
  lat (msec)   : 2=0.92%, 4=0.09%, 10=0.01%
  cpu          : usr=12.32%, sys=32.75%, ctx=302630, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2362469,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
  WRITE: bw=154MiB/s (161MB/s), 154MiB/s-154MiB/s (161MB/s-161MB/s), io=9228MiB (9677MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=39.4k, BW=154MiB/s (161MB/s)(9228MiB/60001msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4989 MiB read, 0.000 read amp, 83800426 total
[0m

===Fio: workload=randwrite, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4987 MiB read, 0.000 read amp, 83800511 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6546: Mon Jan 15 00:19:01 2024
  write: IOPS=54.9k, BW=215MiB/s (225MB/s)(12.6GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=478, avg= 8.45, stdev= 5.85
    clat (usec): min=176, max=12544, avg=572.20, stdev=244.05
     lat (usec): min=189, max=12549, avg=580.89, stdev=244.02
    clat percentiles (usec):
     |  1.00th=[  310],  5.00th=[  371], 10.00th=[  404], 20.00th=[  445],
     | 30.00th=[  474], 40.00th=[  502], 50.00th=[  529], 60.00th=[  562],
     | 70.00th=[  594], 80.00th=[  644], 90.00th=[  725], 95.00th=[  824],
     | 99.00th=[ 1729], 99.50th=[ 2089], 99.90th=[ 2769], 99.95th=[ 3130],
     | 99.99th=[ 4113]
   bw (  KiB/s): min=60192, max=248632, per=100.00%, avg=219805.65, stdev=24829.66, samples=119
   iops        : min=15048, max=62158, avg=54951.45, stdev=6207.41, samples=119
  lat (usec)   : 250=0.10%, 500=39.45%, 750=52.14%, 1000=5.55%
  lat (msec)   : 2=2.17%, 4=0.56%, 10=0.01%, 20=0.01%
  cpu          : usr=18.18%, sys=45.75%, ctx=227136, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,3295053,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=215MiB/s (225MB/s), 215MiB/s-215MiB/s (225MB/s-225MB/s), io=12.6GiB (13.5GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=54.9k, BW=215MiB/s (225MB/s)(12.6GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4984 MiB read, 0.000 read amp, 83800699 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6599: Mon Jan 15 00:20:03 2024
  write: IOPS=57.8k, BW=226MiB/s (237MB/s)(13.2GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=471, avg= 8.25, stdev= 6.20
    clat (usec): min=163, max=18883, avg=1097.80, stdev=628.20
     lat (usec): min=172, max=18893, avg=1106.27, stdev=628.65
    clat percentiles (usec):
     |  1.00th=[  515],  5.00th=[  627], 10.00th=[  717], 20.00th=[  799],
     | 30.00th=[  857], 40.00th=[  914], 50.00th=[  971], 60.00th=[ 1037],
     | 70.00th=[ 1106], 80.00th=[ 1188], 90.00th=[ 1352], 95.00th=[ 2024],
     | 99.00th=[ 3982], 99.50th=[ 4424], 99.90th=[ 6259], 99.95th=[ 6587],
     | 99.99th=[10814]
   bw (  KiB/s): min=42936, max=261344, per=100.00%, avg=231270.66, stdev=51955.12, samples=119
   iops        : min=10734, max=65336, avg=57817.68, stdev=12988.79, samples=119
  lat (usec)   : 250=0.01%, 500=0.50%, 750=13.03%, 1000=41.14%
  lat (msec)   : 2=40.26%, 4=4.11%, 10=0.96%, 20=0.01%
  cpu          : usr=17.55%, sys=47.75%, ctx=232280, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,3465694,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=226MiB/s (237MB/s), 226MiB/s-226MiB/s (237MB/s-237MB/s), io=13.2GiB (14.2GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=57.8k, BW=226MiB/s (237MB/s)(13.2GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4980 MiB read, 0.000 read amp, 83800884 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6653: Mon Jan 15 00:21:06 2024
  write: IOPS=60.0k, BW=234MiB/s (246MB/s)(13.7GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=17595, avg=14.19, stdev=46.86
    clat (usec): min=150, max=19996, avg=2118.05, stdev=626.07
     lat (usec): min=265, max=20013, avg=2132.51, stdev=628.38
    clat percentiles (usec):
     |  1.00th=[ 1254],  5.00th=[ 1516], 10.00th=[ 1614], 20.00th=[ 1762],
     | 30.00th=[ 1860], 40.00th=[ 1926], 50.00th=[ 2008], 60.00th=[ 2089],
     | 70.00th=[ 2180], 80.00th=[ 2343], 90.00th=[ 2704], 95.00th=[ 3097],
     | 99.00th=[ 4359], 99.50th=[ 4948], 99.90th=[ 5997], 99.95th=[ 8848],
     | 99.99th=[17433]
   bw (  KiB/s): min=195344, max=265792, per=100.00%, avg=240199.39, stdev=16406.97, samples=119
   iops        : min=48834, max=66448, avg=60049.87, stdev=4101.80, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.05%
  lat (msec)   : 2=49.05%, 4=49.31%, 10=1.54%, 20=0.05%
  cpu          : usr=17.76%, sys=53.70%, ctx=152281, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3598252,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=234MiB/s (246MB/s), 234MiB/s-234MiB/s (246MB/s-246MB/s), io=13.7GiB (14.7GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=60.0k, BW=234MiB/s (246MB/s)(13.7GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4976 MiB read, 0.000 read amp, 83801070 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6706: Mon Jan 15 00:22:08 2024
  write: IOPS=60.1k, BW=235MiB/s (246MB/s)(13.7GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=18383, avg=14.14, stdev=50.27
    clat (usec): min=520, max=22593, avg=4245.67, stdev=887.90
     lat (usec): min=645, max=22605, avg=4260.10, stdev=889.73
    clat percentiles (usec):
     |  1.00th=[ 3130],  5.00th=[ 3392], 10.00th=[ 3556], 20.00th=[ 3687],
     | 30.00th=[ 3818], 40.00th=[ 3949], 50.00th=[ 4080], 60.00th=[ 4228],
     | 70.00th=[ 4424], 80.00th=[ 4686], 90.00th=[ 5080], 95.00th=[ 5473],
     | 99.00th=[ 6783], 99.50th=[ 7832], 99.90th=[16319], 99.95th=[18744],
     | 99.99th=[21365]
   bw (  KiB/s): min=189112, max=268288, per=99.97%, avg=240174.39, stdev=18063.25, samples=119
   iops        : min=47278, max=67072, avg=60043.61, stdev=4515.81, samples=119
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=43.32%, 10=56.44%, 20=0.22%, 50=0.02%
  cpu          : usr=18.32%, sys=52.19%, ctx=153350, majf=0, minf=8
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3603850,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
  WRITE: bw=235MiB/s (246MB/s), 235MiB/s-235MiB/s (246MB/s-246MB/s), io=13.7GiB (14.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=60.1k, BW=235MiB/s (246MB/s)(13.7GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4972 MiB read, 0.000 read amp, 83801256 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6760: Mon Jan 15 00:23:10 2024
  write: IOPS=64.0k, BW=250MiB/s (262MB/s)(14.6GiB/60002msec); 0 zone resets
    slat (usec): min=4, max=180795, avg=13.42, stdev=103.82
    clat (usec): min=371, max=189465, avg=7988.30, stdev=2404.03
     lat (usec): min=1607, max=189474, avg=8001.96, stdev=2406.42
    clat percentiles (msec):
     |  1.00th=[    7],  5.00th=[    7], 10.00th=[    8], 20.00th=[    8],
     | 30.00th=[    8], 40.00th=[    8], 50.00th=[    8], 60.00th=[    8],
     | 70.00th=[    9], 80.00th=[    9], 90.00th=[   10], 95.00th=[   10],
     | 99.00th=[   12], 99.50th=[   13], 99.90th=[   23], 99.95th=[   24],
     | 99.99th=[  188]
   bw (  KiB/s): min=156200, max=269616, per=100.00%, avg=255930.35, stdev=16091.33, samples=119
   iops        : min=39050, max=67404, avg=63982.57, stdev=4022.87, samples=119
  lat (usec)   : 500=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=95.82%, 20=4.00%, 50=0.16%
  lat (msec)   : 250=0.01%
  cpu          : usr=17.93%, sys=52.26%, ctx=156628, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3838036,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
  WRITE: bw=250MiB/s (262MB/s), 250MiB/s-250MiB/s (262MB/s-262MB/s), io=14.6GiB (15.7GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=64.0k, BW=250MiB/s (262MB/s)(14.6GiB/60002msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4969 MiB read, 0.000 read amp, 83801444 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6813: Mon Jan 15 00:24:13 2024
  write: IOPS=46.8k, BW=183MiB/s (191MB/s)(10.7GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=472, avg= 8.34, stdev= 5.75
    clat (usec): min=194, max=343029, avg=674.37, stdev=4341.80
     lat (usec): min=203, max=343039, avg=682.96, stdev=4341.76
    clat percentiles (usec):
     |  1.00th=[   306],  5.00th=[   371], 10.00th=[   408], 20.00th=[   453],
     | 30.00th=[   490], 40.00th=[   523], 50.00th=[   553], 60.00th=[   586],
     | 70.00th=[   627], 80.00th=[   676], 90.00th=[   758], 95.00th=[   832],
     | 99.00th=[  1106], 99.50th=[  1303], 99.90th=[  4883], 99.95th=[109577],
     | 99.99th=[227541]
   bw (  KiB/s): min=59168, max=260792, per=100.00%, avg=187310.92, stdev=43481.38, samples=119
   iops        : min=14792, max=65198, avg=46827.82, stdev=10870.35, samples=119
  lat (usec)   : 250=0.21%, 500=33.54%, 750=55.78%, 1000=8.79%
  lat (msec)   : 2=1.46%, 4=0.11%, 10=0.04%, 20=0.01%, 50=0.01%
  lat (msec)   : 100=0.01%, 250=0.04%, 500=0.01%
  cpu          : usr=14.28%, sys=38.65%, ctx=200183, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2805100,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=183MiB/s (191MB/s), 183MiB/s-183MiB/s (191MB/s-191MB/s), io=10.7GiB (11.5GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) write:  write: IOPS=46.8k, BW=183MiB/s (191MB/s)(10.7GiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4966 MiB read, 0.000 read amp, 83801580 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6869: Mon Jan 15 00:25:15 2024
  write: IOPS=46.3k, BW=181MiB/s (189MB/s)(10.6GiB/60024msec); 0 zone resets
    slat (usec): min=4, max=428678, avg=19.58, stdev=1024.11
    clat (usec): min=181, max=530609, avg=2746.09, stdev=11775.86
     lat (usec): min=347, max=530628, avg=2765.92, stdev=11822.43
    clat percentiles (usec):
     |  1.00th=[  1090],  5.00th=[  1500], 10.00th=[  1647], 20.00th=[  1844],
     | 30.00th=[  2008], 40.00th=[  2147], 50.00th=[  2245], 60.00th=[  2343],
     | 70.00th=[  2442], 80.00th=[  2573], 90.00th=[  2802], 95.00th=[  3064],
     | 99.00th=[  4047], 99.50th=[  5669], 99.90th=[210764], 99.95th=[320865],
     | 99.99th=[404751]
   bw (  KiB/s): min=21584, max=279224, per=100.00%, avg=186133.11, stdev=60137.88, samples=119
   iops        : min= 5396, max=69808, avg=46533.33, stdev=15034.53, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.30%
  lat (msec)   : 2=28.76%, 4=69.91%, 10=0.72%, 20=0.09%, 50=0.01%
  lat (msec)   : 100=0.01%, 250=0.10%, 500=0.08%, 750=0.01%
  cpu          : usr=13.31%, sys=38.51%, ctx=149817, majf=0, minf=14
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2776228,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=181MiB/s (189MB/s), 181MiB/s-181MiB/s (189MB/s-189MB/s), io=10.6GiB (11.4GB), run=60024-60024msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=46.3k, BW=181MiB/s (189MB/s)(10.6GiB/60024msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4964 MiB read, 0.000 read amp, 83801716 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6927: Mon Jan 15 00:26:17 2024
  write: IOPS=50.8k, BW=198MiB/s (208MB/s)(11.6GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=288394, avg=17.35, stdev=634.94
    clat (usec): min=454, max=343021, avg=2502.81, stdev=7373.23
     lat (usec): min=459, max=343032, avg=2520.41, stdev=7401.66
    clat percentiles (usec):
     |  1.00th=[  1287],  5.00th=[  1582], 10.00th=[  1696], 20.00th=[  1827],
     | 30.00th=[  1926], 40.00th=[  2024], 50.00th=[  2114], 60.00th=[  2212],
     | 70.00th=[  2343], 80.00th=[  2474], 90.00th=[  2704], 95.00th=[  3032],
     | 99.00th=[  4359], 99.50th=[  5604], 99.90th=[149947], 99.95th=[191890],
     | 99.99th=[287310]
   bw (  KiB/s): min=52376, max=272992, per=100.00%, avg=203025.41, stdev=45095.43, samples=119
   iops        : min=13094, max=68248, avg=50756.37, stdev=11273.85, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.04%
  lat (msec)   : 2=38.23%, 4=60.37%, 10=1.08%, 20=0.06%, 50=0.04%
  lat (msec)   : 100=0.02%, 250=0.15%, 500=0.02%
  cpu          : usr=14.95%, sys=43.17%, ctx=150812, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3045061,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=198MiB/s (208MB/s), 198MiB/s-198MiB/s (208MB/s-208MB/s), io=11.6GiB (12.5GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=50.8k, BW=198MiB/s (208MB/s)(11.6GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4961 MiB read, 0.000 read amp, 83801902 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=6981: Mon Jan 15 00:27:20 2024
  write: IOPS=32.4k, BW=253MiB/s (266MB/s)(14.8GiB/60003msec); 0 zone resets
    slat (usec): min=4, max=382317, avg=28.45, stdev=1193.58
    clat (usec): min=289, max=385539, avg=3916.43, stdev=13634.83
     lat (usec): min=306, max=385576, avg=3945.14, stdev=13688.69
    clat percentiles (usec):
     |  1.00th=[  1450],  5.00th=[  1958], 10.00th=[  2147], 20.00th=[  2409],
     | 30.00th=[  2573], 40.00th=[  2704], 50.00th=[  2868], 60.00th=[  2999],
     | 70.00th=[  3195], 80.00th=[  3458], 90.00th=[  3916], 95.00th=[  4424],
     | 99.00th=[  7767], 99.50th=[ 42730], 99.90th=[256902], 99.95th=[278922],
     | 99.99th=[299893]
   bw (  KiB/s): min=82352, max=405536, per=100.00%, avg=260670.25, stdev=85223.40, samples=119
   iops        : min=10294, max=50692, avg=32583.80, stdev=10652.96, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=5.91%, 4=85.06%, 10=8.16%, 20=0.25%, 50=0.10%
  lat (msec)   : 100=0.07%, 250=0.32%, 500=0.10%
  cpu          : usr=10.27%, sys=31.33%, ctx=121903, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1945884,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=253MiB/s (266MB/s), 253MiB/s-253MiB/s (266MB/s-266MB/s), io=14.8GiB (15.9GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=32.4k, BW=253MiB/s (266MB/s)(14.8GiB/60003msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4959 MiB read, 0.000 read amp, 83801994 total
[0m

===Fio: workload=randwrite, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randwrite, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4957 MiB read, 0.000 read amp, 83802086 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7034: Mon Jan 15 00:28:22 2024
  write: IOPS=19.9k, BW=311MiB/s (326MB/s)(18.2GiB/60107msec); 0 zone resets
    slat (usec): min=5, max=579954, avg=47.37, stdev=1369.41
    clat (usec): min=483, max=593540, avg=6382.59, stdev=15966.09
     lat (usec): min=518, max=593548, avg=6430.32, stdev=16030.26
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    6], 50.00th=[    6], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    6], 90.00th=[    7], 95.00th=[    8],
     | 99.00th=[   20], 99.50th=[   27], 99.90th=[  296], 99.95th=[  439],
     | 99.99th=[  592]
   bw (  KiB/s): min=12064, max=399392, per=100.00%, avg=321597.31, stdev=86687.00, samples=119
   iops        : min=  754, max=24962, avg=20099.82, stdev=5417.94, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.05%, 4=0.11%, 10=97.49%, 20=1.40%, 50=0.61%
  lat (msec)   : 100=0.10%, 250=0.10%, 500=0.12%, 750=0.03%
  cpu          : usr=9.60%, sys=25.45%, ctx=311273, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1196068,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=311MiB/s (326MB/s), 311MiB/s-311MiB/s (326MB/s-326MB/s), io=18.2GiB (19.6GB), run=60107-60107msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=19.9k, BW=311MiB/s (326MB/s)(18.2GiB/60107msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randwrite, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4953 MiB read, 0.000 read amp, 83802266 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7087: Mon Jan 15 00:29:25 2024
  write: IOPS=10.2k, BW=318MiB/s (334MB/s)(18.7GiB/60009msec); 0 zone resets
    slat (usec): min=5, max=653136, avg=94.26, stdev=2064.62
    clat (usec): min=873, max=666923, avg=12464.98, stdev=24812.24
     lat (usec): min=941, max=667507, avg=12559.76, stdev=24919.58
    clat percentiles (msec):
     |  1.00th=[    9],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   10], 60.00th=[   11],
     | 70.00th=[   11], 80.00th=[   12], 90.00th=[   13], 95.00th=[   20],
     | 99.00th=[   48], 99.50th=[   64], 99.90th=[  592], 99.95th=[  625],
     | 99.99th=[  667]
   bw (  KiB/s): min=20352, max=440768, per=99.91%, avg=325742.79, stdev=112176.66, samples=119
   iops        : min=  636, max=13774, avg=10179.46, stdev=3505.52, samples=119
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.02%, 4=0.03%, 10=53.01%, 20=42.12%, 50=4.08%
  lat (msec)   : 100=0.40%, 250=0.18%, 500=0.02%, 750=0.15%
  cpu          : usr=7.19%, sys=16.75%, ctx=315378, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,611420,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=318MiB/s (334MB/s), 318MiB/s-318MiB/s (334MB/s-334MB/s), io=18.7GiB (20.0GB), run=60009-60009msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=10.2k, BW=318MiB/s (334MB/s)(18.7GiB/60009msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randwrite, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4950 MiB read, 0.000 read amp, 83802436 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7140: Mon Jan 15 00:30:27 2024
  write: IOPS=5511, BW=344MiB/s (361MB/s)(20.2GiB/60170msec); 0 zone resets
    slat (usec): min=7, max=564745, avg=175.41, stdev=2747.82
    clat (usec): min=736, max=686855, avg=23042.71, stdev=33301.91
     lat (usec): min=766, max=687055, avg=23218.85, stdev=33449.18
    clat percentiles (msec):
     |  1.00th=[   17],  5.00th=[   17], 10.00th=[   18], 20.00th=[   18],
     | 30.00th=[   19], 40.00th=[   20], 50.00th=[   20], 60.00th=[   21],
     | 70.00th=[   21], 80.00th=[   22], 90.00th=[   24], 95.00th=[   29],
     | 99.00th=[   97], 99.50th=[  153], 99.90th=[  584], 99.95th=[  592],
     | 99.99th=[  684]
   bw (  KiB/s): min=28160, max=471680, per=100.00%, avg=356580.03, stdev=100751.52, samples=119
   iops        : min=  440, max= 7370, avg=5571.56, stdev=1574.24, samples=119
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.02%, 4=0.03%, 10=0.05%, 20=59.45%, 50=39.08%
  lat (msec)   : 100=0.43%, 250=0.58%, 500=0.10%, 750=0.26%
  cpu          : usr=6.45%, sys=11.46%, ctx=286069, majf=0, minf=14
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,331636,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=344MiB/s (361MB/s), 344MiB/s-344MiB/s (361MB/s-361MB/s), io=20.2GiB (21.7GB), run=60170-60170msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=5511, BW=344MiB/s (361MB/s)(20.2GiB/60170msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4947 MiB read, 0.000 read amp, 83802604 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7196: Mon Jan 15 00:31:30 2024
  write: IOPS=61.2k, BW=239MiB/s (251MB/s)(14.0GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=323832, avg=14.26, stdev=451.61
    clat (usec): min=141, max=326413, avg=2074.17, stdev=5201.35
     lat (usec): min=248, max=326422, avg=2088.70, stdev=5221.97
    clat percentiles (usec):
     |  1.00th=[  1074],  5.00th=[  1369], 10.00th=[  1483], 20.00th=[  1614],
     | 30.00th=[  1729], 40.00th=[  1811], 50.00th=[  1893], 60.00th=[  1991],
     | 70.00th=[  2114], 80.00th=[  2245], 90.00th=[  2474], 95.00th=[  2737],
     | 99.00th=[  3949], 99.50th=[  4817], 99.90th=[ 11994], 99.95th=[ 21627],
     | 99.99th=[308282]
   bw (  KiB/s): min=85504, max=286016, per=100.00%, avg=245371.90, stdev=44098.58, samples=119
   iops        : min=21376, max=71506, avg=61343.04, stdev=11024.66, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.38%
  lat (msec)   : 2=60.99%, 4=37.65%, 10=0.86%, 20=0.06%, 50=0.01%
  lat (msec)   : 250=0.02%, 500=0.02%
  cpu          : usr=17.09%, sys=51.15%, ctx=148501, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3674349,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=239MiB/s (251MB/s), 239MiB/s-239MiB/s (251MB/s-251MB/s), io=14.0GiB (15.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=61.2k, BW=239MiB/s (251MB/s)(14.0GiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4945 MiB read, 0.000 read amp, 83802740 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7249: Mon Jan 15 00:32:32 2024
  write: IOPS=41.6k, BW=325MiB/s (341MB/s)(19.0GiB/60003msec); 0 zone resets
    slat (usec): min=4, max=442585, avg=21.93, stdev=801.46
    clat (usec): min=261, max=445807, avg=3053.30, stdev=9249.93
     lat (usec): min=307, max=445818, avg=3075.51, stdev=9286.66
    clat percentiles (usec):
     |  1.00th=[  1729],  5.00th=[  2073], 10.00th=[  2245], 20.00th=[  2409],
     | 30.00th=[  2507], 40.00th=[  2573], 50.00th=[  2638], 60.00th=[  2737],
     | 70.00th=[  2835], 80.00th=[  2966], 90.00th=[  3261], 95.00th=[  3720],
     | 99.00th=[  6456], 99.50th=[  8455], 99.90th=[ 65799], 99.95th=[299893],
     | 99.99th=[434111]
   bw (  KiB/s): min=85200, max=403072, per=99.95%, avg=332610.69, stdev=71076.69, samples=119
   iops        : min=10650, max=50384, avg=41576.34, stdev=8884.59, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=3.69%, 4=92.52%, 10=3.39%, 20=0.23%, 50=0.04%
  lat (msec)   : 100=0.02%, 250=0.03%, 500=0.06%
  cpu          : usr=12.02%, sys=38.98%, ctx=183183, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2496018,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=325MiB/s (341MB/s), 325MiB/s-325MiB/s (341MB/s-341MB/s), io=19.0GiB (20.4GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) write:  write: IOPS=41.6k, BW=325MiB/s (341MB/s)(19.0GiB/60003msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4943 MiB read, 0.000 read amp, 83802876 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7302: Mon Jan 15 00:33:34 2024
  write: IOPS=21.5k, BW=335MiB/s (352MB/s)(19.7GiB/60005msec); 0 zone resets
    slat (usec): min=4, max=411973, avg=43.98, stdev=986.55
    clat (usec): min=489, max=418793, avg=5917.10, stdev=11363.02
     lat (usec): min=495, max=418831, avg=5961.43, stdev=11410.62
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    5], 50.00th=[    5], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    6], 90.00th=[    7], 95.00th=[    7],
     | 99.00th=[   24], 99.50th=[   27], 99.90th=[  232], 99.95th=[  313],
     | 99.99th=[  380]
   bw (  KiB/s): min=62432, max=415840, per=99.94%, avg=343217.75, stdev=86509.16, samples=119
   iops        : min= 3902, max=25990, avg=21451.13, stdev=5406.83, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.07%, 4=0.20%, 10=96.87%, 20=1.51%, 50=1.15%
  lat (msec)   : 100=0.04%, 250=0.07%, 500=0.09%
  cpu          : usr=9.19%, sys=26.92%, ctx=342786, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1287989,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=335MiB/s (352MB/s), 335MiB/s-335MiB/s (352MB/s-352MB/s), io=19.7GiB (21.1GB), run=60005-60005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) write:  write: IOPS=21.5k, BW=335MiB/s (352MB/s)(19.7GiB/60005msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=write, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4940 MiB read, 0.000 read amp, 83803012 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7355: Mon Jan 15 00:34:37 2024
  write: IOPS=10.9k, BW=340MiB/s (356MB/s)(19.9GiB/60011msec); 0 zone resets
    slat (usec): min=5, max=445795, avg=88.19, stdev=1700.86
    clat (usec): min=473, max=458439, avg=11675.75, stdev=20742.19
     lat (usec): min=497, max=458508, avg=11764.47, stdev=20829.69
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   10], 60.00th=[   11],
     | 70.00th=[   11], 80.00th=[   11], 90.00th=[   12], 95.00th=[   15],
     | 99.00th=[   46], 99.50th=[   55], 99.90th=[  414], 99.95th=[  435],
     | 99.99th=[  456]
   bw (  KiB/s): min=56704, max=456576, per=99.92%, avg=347819.56, stdev=100266.27, samples=119
   iops        : min= 1772, max=14268, avg=10869.36, stdev=3133.32, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=0.10%, 4=0.20%, 10=58.18%, 20=38.59%, 50=2.16%
  lat (msec)   : 100=0.45%, 250=0.03%, 500=0.27%
  cpu          : usr=6.92%, sys=17.60%, ctx=336574, majf=0, minf=14
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,652795,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=340MiB/s (356MB/s), 340MiB/s-340MiB/s (356MB/s-356MB/s), io=19.9GiB (21.4GB), run=60011-60011msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) write:  write: IOPS=10.9k, BW=340MiB/s (356MB/s)(19.9GiB/60011msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=write, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4938 MiB read, 0.000 read amp, 83803146 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=7409: Mon Jan 15 00:35:39 2024
  write: IOPS=5674, BW=355MiB/s (372MB/s)(20.8GiB/60021msec); 0 zone resets
    slat (usec): min=6, max=448513, avg=170.74, stdev=2500.21
    clat (usec): min=615, max=481036, avg=22379.95, stdev=29324.87
     lat (usec): min=762, max=481288, avg=22551.44, stdev=29444.31
    clat percentiles (msec):
     |  1.00th=[   16],  5.00th=[   17], 10.00th=[   18], 20.00th=[   18],
     | 30.00th=[   19], 40.00th=[   19], 50.00th=[   20], 60.00th=[   20],
     | 70.00th=[   21], 80.00th=[   22], 90.00th=[   24], 95.00th=[   28],
     | 99.00th=[   66], 99.50th=[  347], 99.90th=[  464], 99.95th=[  468],
     | 99.99th=[  481]
   bw (  KiB/s): min=44800, max=462720, per=100.00%, avg=363622.18, stdev=98008.82, samples=119
   iops        : min=  700, max= 7230, avg=5681.60, stdev=1531.39, samples=119
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.05%, 4=0.09%, 10=0.26%, 20=66.40%, 50=31.92%
  lat (msec)   : 100=0.64%, 250=0.13%, 500=0.51%
  cpu          : usr=6.61%, sys=11.80%, ctx=298258, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,340619,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=355MiB/s (372MB/s), 355MiB/s-355MiB/s (372MB/s-372MB/s), io=20.8GiB (22.3GB), run=60021-60021msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) write:  write: IOPS=5674, BW=355MiB/s (372MB/s)(20.8GiB/60021msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4937 MiB read, 0.000 read amp, 83803213 total
[0m100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.354017 s, 296 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 96d67769-a6e9-4eb5-a95d-5a35e39f21e6
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.017: File-server Version 3.0 personality successfully loaded
0.017: Populating and pre-allocating filesets
0.288: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.288: Removing bigfileset tree (if exists)
0.295: Pre-allocating directories in bigfileset tree
0.691: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4935 MiB read, 0.000 read amp, 83803305 total
[0m35.641: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
35.641: Population and pre-allocation of filesets completed
35.642: Starting 1 filereader instances
36.652: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4935 MiB read, 0.000 read amp, 83803313 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4934 MiB read, 0.000 read amp, 83803372 total
[0m156.663: Run took 120 seconds...
156.667: Per-Operation Breakdown
statfile1            741540ops     6179ops/s   0.0mb/s    0.011ms/op [0.003ms - 6.158ms]
deletefile1          741540ops     6179ops/s   0.0mb/s    0.185ms/op [0.029ms - 282.951ms]
closefile3           741544ops     6179ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.020ms]
readfile1            741545ops     6179ops/s 808.9mb/s    0.101ms/op [0.004ms - 283.354ms]
openfile2            741545ops     6179ops/s   0.0mb/s    0.067ms/op [0.005ms - 31.154ms]
closefile2           741545ops     6179ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.176ms]
appendfilerand1      741545ops     6179ops/s  48.3mb/s    0.468ms/op [0.001ms - 282.680ms]
openfile1            741545ops     6179ops/s   0.0mb/s    0.072ms/op [0.007ms - 31.266ms]
closefile1           741545ops     6179ops/s   0.0mb/s    0.006ms/op [0.001ms - 30.188ms]
wrtfile1             741548ops     6179ops/s 773.1mb/s    6.244ms/op [0.013ms - 283.726ms]
createfile1          741590ops     6179ops/s   0.0mb/s    0.138ms/op [0.024ms - 281.706ms]
156.667: IO Summary: 8157032 ops 67969.442 ops/s 6179/12358 rd/wr 1630.2mb/s 0.664ms/op
156.667: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:156.667: IO Summary: 8157032 ops 67969.442 ops/s 6179/12358 rd/wr 1630.2mb/s 0.664ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.017: OLTP Version 3.0  personality successfully loaded
0.017: Populating and pre-allocating filesets
0.017: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.017: Removing logfile tree (if exists)
0.024: Pre-allocating directories in logfile tree
0.025: Pre-allocating files in logfile tree
0.141: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.141: Removing datafiles tree (if exists)
0.145: Pre-allocating directories in datafiles tree
0.145: Pre-allocating files in datafiles tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4934 MiB read, 0.000 read amp, 83803374 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4933 MiB read, 0.000 read amp, 83803377 total
[0m37.925: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
37.925: Population and pre-allocation of filesets completed
37.925: Starting 200 shadow instances
38.046: Starting 10 dbwr instances
38.057: Starting 1 lgwr instances
39.060: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4933 MiB read, 0.000 read amp, 83803380 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4933 MiB read, 0.000 read amp, 83803382 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4933 MiB read, 0.000 read amp, 83803384 total
[0m159.110: Run took 120 seconds...
159.131: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     2703000ops    22516ops/s   0.0mb/s    7.326ms/op [0.019ms - 495.331ms]
shadow-post-lg       2703052ops    22516ops/s   0.0mb/s    0.074ms/op [0.002ms - 60.030ms]
shadowhog            2703057ops    22516ops/s   0.0mb/s    0.333ms/op [0.092ms - 100.777ms]
shadowread           2728780ops    22731ops/s  44.0mb/s    1.034ms/op [0.001ms - 267.792ms]
dbwr-aiowait         27020ops      225ops/s   0.0mb/s    2.891ms/op [0.005ms - 48.462ms]
dbwr-block           27023ops      225ops/s   0.0mb/s   17.775ms/op [0.003ms - 381.786ms]
dbwr-hog             27030ops      225ops/s   0.0mb/s    0.012ms/op [0.004ms - 16.151ms]
dbwrite-a            2704280ops    22527ops/s  44.0mb/s    0.004ms/op [0.000ms - 78.156ms]
lg-block             844ops        7ops/s   0.0mb/s  142.075ms/op [33.869ms - 841.880ms]
lg-aiowait           845ops        7ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.025ms]
lg-write             846ops        7ops/s   1.8mb/s    0.009ms/op [0.001ms - 0.100ms]
159.131: IO Summary: 5461771 ops 45496.618 ops/s 22731/22534 rd/wr  89.7mb/s 0.533ms/op
159.131: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:159.131: IO Summary: 5461771 ops 45496.618 ops/s 22731/22534 rd/wr  89.7mb/s 0.533ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.013: Varmail Version 3.0 personality successfully loaded
0.013: Populating and pre-allocating filesets
0.947: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
0.947: Removing bigfileset tree (if exists)
0.954: Pre-allocating directories in bigfileset tree
0.955: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4933 MiB read, 0.000 read amp, 83803386 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4933 MiB read, 0.000 read amp, 83803406 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159998/160000 hits, 4933 MiB read, 0.000 read amp, 83803407 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159996/160000 hits, 4933 MiB read, 0.000 read amp, 83803412 total
[0m73.049: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
73.049: Population and pre-allocation of filesets completed
73.049: Starting 1 filereader instances
74.053: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159996/160000 hits, 4933 MiB read, 0.000 read amp, 83803415 total
[0m194.062: Run took 120 seconds...
194.063: Per-Operation Breakdown
closefile4           218246ops     1819ops/s   0.0mb/s    0.003ms/op [0.001ms - 0.416ms]
readfile4            218248ops     1819ops/s  48.4mb/s    0.720ms/op [0.008ms - 295.287ms]
openfile4            218248ops     1819ops/s   0.0mb/s    0.021ms/op [0.005ms - 2.055ms]
closefile3           218248ops     1819ops/s   0.0mb/s    0.006ms/op [0.001ms - 6.032ms]
fsyncfile3           218249ops     1819ops/s   0.0mb/s    2.681ms/op [0.586ms - 287.439ms]
appendfilerand3      218250ops     1819ops/s  14.2mb/s    0.071ms/op [0.007ms - 11.799ms]
readfile3            218256ops     1819ops/s  48.4mb/s    0.766ms/op [0.008ms - 295.282ms]
openfile3            218256ops     1819ops/s   0.0mb/s    0.020ms/op [0.005ms - 2.083ms]
closefile2           218256ops     1819ops/s   0.0mb/s    0.006ms/op [0.001ms - 6.696ms]
fsyncfile2           218256ops     1819ops/s   0.0mb/s    2.790ms/op [0.865ms - 295.692ms]
appendfilerand2      218259ops     1819ops/s  14.2mb/s    0.257ms/op [0.001ms - 286.218ms]
createfile2          218261ops     1819ops/s   0.0mb/s    0.692ms/op [0.030ms - 290.422ms]
deletefile1          218262ops     1819ops/s   0.0mb/s    0.628ms/op [0.043ms - 286.538ms]
194.063: IO Summary: 2837295 ops 23642.399 ops/s 3637/3637 rd/wr 125.2mb/s 0.666ms/op
194.063: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:194.063: IO Summary: 2837295 ops 23642.399 ops/s 3637/3637 rd/wr 125.2mb/s 0.666ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:32:54.lsvd-nvme.240.rssd2.txt
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-14T23:32:54.lsvd-nvme.240.rssd2.txt
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=60.7k, BW=237MiB/s (249MB/s)(13.9GiB/60001msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=75.2k, BW=294MiB/s (308MB/s)(17.2GiB/60001msec)
Fio (iodepth=1; bs=4ki) randread:  read: IOPS=6782, BW=26.5MiB/s (27.8MB/s)(1590MiB/60001msec)
Fio (iodepth=4; bs=4ki) randread:  read: IOPS=21.8k, BW=85.1MiB/s (89.2MB/s)(5104MiB/60001msec)
Fio (iodepth=8; bs=4ki) randread:  read: IOPS=31.8k, BW=124MiB/s (130MB/s)(7455MiB/60001msec)
Fio (iodepth=16; bs=4ki) randread:  read: IOPS=51.5k, BW=201MiB/s (211MB/s)(11.8GiB/60001msec)
Fio (iodepth=32; bs=4ki) randread:  read: IOPS=65.5k, BW=256MiB/s (268MB/s)(15.0GiB/60001msec)
Fio (iodepth=64; bs=4ki) randread:  read: IOPS=67.5k, BW=263MiB/s (276MB/s)(15.4GiB/60001msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=66.0k, BW=258MiB/s (270MB/s)(15.1GiB/60001msec)
Fio (iodepth=256; bs=4ki) randread:  read: IOPS=68.6k, BW=268MiB/s (281MB/s)(15.7GiB/60001msec)
Fio (iodepth=512; bs=4ki) randread:  read: IOPS=65.2k, BW=255MiB/s (267MB/s)(14.9GiB/60001msec)
Fio (iodepth=32; bs=4ki) read:  read: IOPS=78.6k, BW=307MiB/s (322MB/s)(18.0GiB/60001msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=81.8k, BW=319MiB/s (335MB/s)(18.7GiB/60001msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=65.6k, BW=256MiB/s (269MB/s)(15.0GiB/60001msec)
Fio (iodepth=128; bs=8ki) randread:  read: IOPS=53.1k, BW=415MiB/s (435MB/s)(24.3GiB/60002msec)
Fio (iodepth=128; bs=16ki) randread:  read: IOPS=47.9k, BW=748MiB/s (785MB/s)(43.9GiB/60003msec)
Fio (iodepth=128; bs=32ki) randread:  read: IOPS=30.6k, BW=957MiB/s (1003MB/s)(56.1GiB/60005msec)
Fio (iodepth=128; bs=64ki) randread:  read: IOPS=17.7k, BW=1109MiB/s (1162MB/s)(65.0GiB/60007msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=79.0k, BW=309MiB/s (324MB/s)(18.1GiB/60001msec)
Fio (iodepth=128; bs=8ki) read:  read: IOPS=62.2k, BW=486MiB/s (509MB/s)(28.5GiB/60002msec)
Fio (iodepth=128; bs=16ki) read:  read: IOPS=52.4k, BW=818MiB/s (858MB/s)(48.0GiB/60003msec)
Fio (iodepth=128; bs=32ki) read:  read: IOPS=33.5k, BW=1047MiB/s (1098MB/s)(61.4GiB/60004msec)
Fio (iodepth=128; bs=64ki) read:  read: IOPS=18.1k, BW=1128MiB/s (1183MB/s)(66.1GiB/60007msec)
Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5752, BW=22.5MiB/s (23.6MB/s)(1348MiB/60001msec); 0 zone resets
Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=17.7k, BW=69.1MiB/s (72.5MB/s)(4149MiB/60001msec); 0 zone resets
Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=27.7k, BW=108MiB/s (113MB/s)(6490MiB/60001msec); 0 zone resets
Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=39.4k, BW=154MiB/s (161MB/s)(9228MiB/60001msec); 0 zone resets
Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=54.9k, BW=215MiB/s (225MB/s)(12.6GiB/60001msec); 0 zone resets
Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=57.8k, BW=226MiB/s (237MB/s)(13.2GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=60.0k, BW=234MiB/s (246MB/s)(13.7GiB/60001msec); 0 zone resets
Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=60.1k, BW=235MiB/s (246MB/s)(13.7GiB/60001msec); 0 zone resets
Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=64.0k, BW=250MiB/s (262MB/s)(14.6GiB/60002msec); 0 zone resets
Fio (iodepth=32; bs=4ki) write:  write: IOPS=46.8k, BW=183MiB/s (191MB/s)(10.7GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=46.3k, BW=181MiB/s (189MB/s)(10.6GiB/60024msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=50.8k, BW=198MiB/s (208MB/s)(11.6GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=32.4k, BW=253MiB/s (266MB/s)(14.8GiB/60003msec); 0 zone resets
Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=19.9k, BW=311MiB/s (326MB/s)(18.2GiB/60107msec); 0 zone resets
Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=10.2k, BW=318MiB/s (334MB/s)(18.7GiB/60009msec); 0 zone resets
Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=5511, BW=344MiB/s (361MB/s)(20.2GiB/60170msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=61.2k, BW=239MiB/s (251MB/s)(14.0GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=8ki) write:  write: IOPS=41.6k, BW=325MiB/s (341MB/s)(19.0GiB/60003msec); 0 zone resets
Fio (iodepth=128; bs=16ki) write:  write: IOPS=21.5k, BW=335MiB/s (352MB/s)(19.7GiB/60005msec); 0 zone resets
Fio (iodepth=128; bs=32ki) write:  write: IOPS=10.9k, BW=340MiB/s (356MB/s)(19.9GiB/60011msec); 0 zone resets
Fio (iodepth=128; bs=64ki) write:  write: IOPS=5674, BW=355MiB/s (372MB/s)(20.8GiB/60021msec); 0 zone resets
Filebench /tmp/filebench/fileserver.f:156.667: IO Summary: 8157032 ops 67969.442 ops/s 6179/12358 rd/wr 1630.2mb/s 0.664ms/op
Filebench /tmp/filebench/oltp.f:159.131: IO Summary: 5461771 ops 45496.618 ops/s 22731/22534 rd/wr  89.7mb/s 0.533ms/op
Filebench /tmp/filebench/varmail.f:194.063: IO Summary: 2837295 ops 23642.399 ops/s 3637/3637 rd/wr 125.2mb/s 0.666ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7fd8a0ff1640) exiting
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-15T00:44:49
+ default_cache_size=21474836480
+ cache_size=257698037760
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=240
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T00:44:49.lsvd-nvme.240.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-15T00:44:49
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thick triple-hdd lsvd-benchmark 80g
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark
Removing all objects from pool triple-hdd with prefix lsvd-benchmark
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 15375/62003 objects
+ ./thick-image --size=80g triple-hdd/lsvd-benchmark
Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 45% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 80% complete...Thick provisioning: 81% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 87% complete...Thick provisioning: 88% complete...Thick provisioning: 89% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 92% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p triple-hdd stat lsvd-benchmark
triple-hdd/lsvd-benchmark mtime 2024-01-15T00:56:26.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/ 257698037760
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/
+ local cache_size=257698037760
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=257698037760
+ LSVD_CACHE_SIZE=257698037760
+ rm -rf /mnt/nvme-remote//lsvd-write/b94ed7f9-8247-43ed-a572-c72bb2218976.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-15 00:56:26.769892] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-15 00:56:26.770015] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid859617 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-15 00:56:26.835313] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-15 00:56:26.940725] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-15 00:56:26.940796] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-15 00:56:26.940867] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
[2024-01-15 00:56:26.940863] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-15 00:56:32.403713] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-15 00:56:33.246697] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 245760 MiB in 16 shards, 15360 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//af309aba-4712-4d06-8879-5cbe1576fb36.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-15 00:57:10.389903] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 1/2 hits, 0 MiB read, 1.778 read amp, 2 total
[0m+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T00:44:49.lsvd-nvme.240.triple-hdd.txt client-bench.bash read_entire_img=1
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T00:44:49.lsvd-nvme.240.triple-hdd.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=1
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=1'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T00:44:49.lsvd-nvme.240.triple-hdd.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 1 '!=' read_entire_img=1 ']'
+ eval read_entire_img=1
++ read_entire_img=1
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=1


===Reading entire image to warm cache===

+ [[ 1 -eq 1 ]]
+ printf '\n\n===Reading entire image to warm cache===\n\n'
+ dd if=/dev/nvme1n1 of=/dev/null bs=1048576 count=81910
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 7407/22462 hits, 934 MiB read, 1.007 read amp, 22462 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 19326/58786 hits, 2447 MiB read, 1.008 read amp, 58786 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 31495/95872 hits, 3992 MiB read, 1.008 read amp, 95872 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 41884/127534 hits, 5312 MiB read, 1.008 read amp, 127534 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 163012 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 199240 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 235282 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 271804 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 308110 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 344668 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 381940 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 419020 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 455824 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 492046 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 528868 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52506/160000 hits, 6666 MiB read, 1.008 read amp, 566254 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 603076 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 640036 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 676540 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 711988 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 749314 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 785902 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 822580 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 859108 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 895378 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 931612 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 967732 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1003474 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1040164 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1076584 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1112308 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1149514 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1177042 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1213372 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1247140 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1284400 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1321096 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1358368 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1394632 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1431652 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1468246 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1504486 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1540882 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1576882 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1613266 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1649206 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1685794 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52498/160000 hits, 6666 MiB read, 1.008 read amp, 1721962 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1758988 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1796026 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1833004 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1869250 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1904926 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1941850 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1965916 total
[0m81910+0 records in
81910+0 records out
85888860160 bytes (86 GB, 80 GiB) copied, 1093.92 s, 78.5 MB/s
+ set +x


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 2991458 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 4123559 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 5251180 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11003: Mon Jan 15 01:16:32 2024
  read: IOPS=53.1k, BW=208MiB/s (218MB/s)(12.2GiB/60001msec)
    slat (usec): min=4, max=7735, avg=16.00, stdev=71.18
    clat (usec): min=107, max=10402, avg=2391.00, stdev=511.97
     lat (usec): min=134, max=10408, avg=2407.32, stdev=506.75
    clat percentiles (usec):
     |  1.00th=[ 1123],  5.00th=[ 1336], 10.00th=[ 1565], 20.00th=[ 2008],
     | 30.00th=[ 2245], 40.00th=[ 2409], 50.00th=[ 2507], 60.00th=[ 2606],
     | 70.00th=[ 2671], 80.00th=[ 2769], 90.00th=[ 2900], 95.00th=[ 3032],
     | 99.00th=[ 3359], 99.50th=[ 3556], 99.90th=[ 4621], 99.95th=[ 5014],
     | 99.99th=[ 5473]
   bw (  KiB/s): min=189160, max=221880, per=100.00%, avg=212607.06, stdev=3939.57, samples=119
   iops        : min=47290, max=55470, avg=53151.76, stdev=984.90, samples=119
  lat (usec)   : 250=0.01%, 500=0.02%, 750=0.01%, 1000=0.05%
  lat (msec)   : 2=19.62%, 4=80.12%, 10=0.17%, 20=0.01%
  cpu          : usr=15.20%, sys=46.69%, ctx=68124, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3187519,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=208MiB/s (218MB/s), 208MiB/s-208MiB/s (218MB/s-218MB/s), io=12.2GiB (13.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=53.1k, BW=208MiB/s (218MB/s)(12.2GiB/60001msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 6474027 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 7868199 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 9409918 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11050: Mon Jan 15 01:17:34 2024
  read: IOPS=69.4k, BW=271MiB/s (284MB/s)(15.9GiB/60001msec)
    slat (usec): min=3, max=2985, avg=12.13, stdev=44.96
    clat (usec): min=189, max=5353, avg=1830.09, stdev=404.48
     lat (usec): min=240, max=5379, avg=1842.54, stdev=403.00
    clat percentiles (usec):
     |  1.00th=[ 1037],  5.00th=[ 1172], 10.00th=[ 1303], 20.00th=[ 1483],
     | 30.00th=[ 1598], 40.00th=[ 1696], 50.00th=[ 1827], 60.00th=[ 1942],
     | 70.00th=[ 2057], 80.00th=[ 2180], 90.00th=[ 2343], 95.00th=[ 2474],
     | 99.00th=[ 2802], 99.50th=[ 2966], 99.90th=[ 3523], 99.95th=[ 4178],
     | 99.99th=[ 4686]
   bw (  KiB/s): min=228488, max=346928, per=99.90%, avg=277366.18, stdev=30387.18, samples=119
   iops        : min=57122, max=86732, avg=69341.56, stdev=7596.78, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.58%
  lat (msec)   : 2=63.91%, 4=35.44%, 10=0.07%
  cpu          : usr=15.09%, sys=56.77%, ctx=89647, majf=0, minf=137
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4164605,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=271MiB/s (284MB/s), 271MiB/s-271MiB/s (284MB/s-284MB/s), io=15.9GiB (17.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=69.4k, BW=271MiB/s (284MB/s)(15.9GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 9875134 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 10023199 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 10168565 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11099: Mon Jan 15 01:18:37 2024
  read: IOPS=6893, BW=26.9MiB/s (28.2MB/s)(1616MiB/60001msec)
    slat (usec): min=7, max=454, avg=12.09, stdev= 5.59
    clat (usec): min=27, max=10689, avg=129.52, stdev=42.33
     lat (usec): min=101, max=10704, avg=141.98, stdev=43.42
    clat percentiles (usec):
     |  1.00th=[  100],  5.00th=[  103], 10.00th=[  106], 20.00th=[  112],
     | 30.00th=[  117], 40.00th=[  121], 50.00th=[  124], 60.00th=[  126],
     | 70.00th=[  130], 80.00th=[  135], 90.00th=[  147], 95.00th=[  188],
     | 99.00th=[  297], 99.50th=[  379], 99.90th=[  545], 99.95th=[  594],
     | 99.99th=[  685]
   bw (  KiB/s): min=25488, max=30656, per=100.00%, avg=27594.71, stdev=1215.83, samples=119
   iops        : min= 6372, max= 7664, avg=6898.67, stdev=303.96, samples=119
  lat (usec)   : 50=0.01%, 100=1.39%, 250=97.15%, 500=1.24%, 750=0.21%
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 20=0.01%
  cpu          : usr=5.32%, sys=12.51%, ctx=413622, majf=0, minf=12
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=413614,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=26.9MiB/s (28.2MB/s), 26.9MiB/s-26.9MiB/s (28.2MB/s-28.2MB/s), io=1616MiB (1694MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randread:  read: IOPS=6893, BW=26.9MiB/s (28.2MB/s)(1616MiB/60001msec)


===Fio: workload=randread, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 10475310 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 10939305 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 11400655 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11145: Mon Jan 15 01:19:39 2024
  read: IOPS=21.7k, BW=84.9MiB/s (89.0MB/s)(5091MiB/60001msec)
    slat (usec): min=4, max=464, avg=10.52, stdev= 5.57
    clat (usec): min=48, max=1963, avg=170.63, stdev=45.92
     lat (usec): min=89, max=1982, avg=181.50, stdev=45.96
    clat percentiles (usec):
     |  1.00th=[  105],  5.00th=[  117], 10.00th=[  124], 20.00th=[  137],
     | 30.00th=[  147], 40.00th=[  157], 50.00th=[  167], 60.00th=[  176],
     | 70.00th=[  186], 80.00th=[  198], 90.00th=[  221], 95.00th=[  241],
     | 99.00th=[  302], 99.50th=[  392], 99.90th=[  603], 99.95th=[  627],
     | 99.99th=[  685]
   bw (  KiB/s): min=78720, max=95560, per=100.00%, avg=87011.63, stdev=2672.62, samples=119
   iops        : min=19680, max=23890, avg=21752.91, stdev=668.16, samples=119
  lat (usec)   : 50=0.01%, 100=0.30%, 250=96.05%, 500=3.36%, 750=0.28%
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.01%
  cpu          : usr=12.54%, sys=28.99%, ctx=766472, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=1303401,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
   READ: bw=84.9MiB/s (89.0MB/s), 84.9MiB/s-84.9MiB/s (89.0MB/s-89.0MB/s), io=5091MiB (5339MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randread:  read: IOPS=21.7k, BW=84.9MiB/s (89.0MB/s)(5091MiB/60001msec)


===Fio: workload=randread, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 11899270 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 12581014 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 13263249 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11191: Mon Jan 15 01:20:41 2024
  read: IOPS=32.0k, BW=125MiB/s (131MB/s)(7491MiB/60001msec)
    slat (usec): min=4, max=489, avg=10.90, stdev= 6.04
    clat (usec): min=53, max=2932, avg=236.57, stdev=51.85
     lat (usec): min=101, max=2941, avg=247.83, stdev=51.61
    clat percentiles (usec):
     |  1.00th=[  141],  5.00th=[  163], 10.00th=[  182], 20.00th=[  200],
     | 30.00th=[  215], 40.00th=[  225], 50.00th=[  235], 60.00th=[  243],
     | 70.00th=[  253], 80.00th=[  269], 90.00th=[  289], 95.00th=[  310],
     | 99.00th=[  400], 99.50th=[  490], 99.90th=[  668], 99.95th=[  693],
     | 99.99th=[  750]
   bw (  KiB/s): min=122072, max=141328, per=100.00%, avg=127998.92, stdev=3380.67, samples=119
   iops        : min=30518, max=35332, avg=31999.78, stdev=845.14, samples=119
  lat (usec)   : 100=0.01%, 250=66.96%, 500=32.56%, 750=0.46%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%
  cpu          : usr=15.72%, sys=38.46%, ctx=609294, majf=0, minf=17
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=1917794,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
   READ: bw=125MiB/s (131MB/s), 125MiB/s-125MiB/s (131MB/s-131MB/s), io=7491MiB (7855MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randread:  read: IOPS=32.0k, BW=125MiB/s (131MB/s)(7491MiB/60001msec)


===Fio: workload=randread, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 13967724 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 14984685 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 16001237 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11238: Mon Jan 15 01:21:44 2024
  read: IOPS=47.2k, BW=184MiB/s (193MB/s)(10.8GiB/60001msec)
    slat (usec): min=4, max=673, avg=10.04, stdev= 9.57
    clat (usec): min=105, max=2883, avg=326.41, stdev=83.23
     lat (usec): min=119, max=2892, avg=336.79, stdev=83.92
    clat percentiles (usec):
     |  1.00th=[  190],  5.00th=[  219], 10.00th=[  237], 20.00th=[  265],
     | 30.00th=[  285], 40.00th=[  302], 50.00th=[  318], 60.00th=[  334],
     | 70.00th=[  351], 80.00th=[  375], 90.00th=[  416], 95.00th=[  461],
     | 99.00th=[  652], 99.50th=[  725], 99.90th=[  832], 99.95th=[  881],
     | 99.99th=[ 1012]
   bw (  KiB/s): min=140376, max=207424, per=100.00%, avg=188821.58, stdev=11146.28, samples=119
   iops        : min=35094, max=51856, avg=47205.41, stdev=2786.57, samples=119
  lat (usec)   : 250=13.99%, 500=82.79%, 750=2.86%, 1000=0.35%
  lat (msec)   : 2=0.01%, 4=0.01%
  cpu          : usr=17.15%, sys=49.91%, ctx=336673, majf=0, minf=25
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=2831313,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=184MiB/s (193MB/s), 184MiB/s-184MiB/s (193MB/s-193MB/s), io=10.8GiB (11.6GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randread:  read: IOPS=47.2k, BW=184MiB/s (193MB/s)(10.8GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 16915691 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 18296336 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 19671964 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11284: Mon Jan 15 01:22:46 2024
  read: IOPS=64.0k, BW=250MiB/s (262MB/s)(14.6GiB/60001msec)
    slat (usec): min=4, max=591, avg= 8.23, stdev= 6.74
    clat (usec): min=142, max=2618, avg=489.68, stdev=109.88
     lat (usec): min=160, max=2622, avg=498.20, stdev=109.51
    clat percentiles (usec):
     |  1.00th=[  281],  5.00th=[  347], 10.00th=[  371], 20.00th=[  400],
     | 30.00th=[  424], 40.00th=[  453], 50.00th=[  478], 60.00th=[  506],
     | 70.00th=[  537], 80.00th=[  570], 90.00th=[  627], 95.00th=[  676],
     | 99.00th=[  824], 99.50th=[  914], 99.90th=[ 1106], 99.95th=[ 1221],
     | 99.99th=[ 1401]
   bw (  KiB/s): min=221008, max=271144, per=100.00%, avg=256437.18, stdev=8246.55, samples=119
   iops        : min=55252, max=67786, avg=64109.33, stdev=2061.67, samples=119
  lat (usec)   : 250=0.04%, 500=58.47%, 750=39.62%, 1000=1.63%
  lat (msec)   : 2=0.25%, 4=0.01%
  cpu          : usr=18.29%, sys=54.47%, ctx=192810, majf=0, minf=41
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=3839165,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=250MiB/s (262MB/s), 250MiB/s-250MiB/s (262MB/s-262MB/s), io=14.6GiB (15.7GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randread:  read: IOPS=64.0k, BW=250MiB/s (262MB/s)(14.6GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 20812617 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 22148305 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 23584797 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11330: Mon Jan 15 01:23:49 2024
  read: IOPS=64.4k, BW=252MiB/s (264MB/s)(14.7GiB/60001msec)
    slat (usec): min=4, max=892, avg= 8.31, stdev= 9.78
    clat (usec): min=131, max=4747, avg=982.73, stdev=237.32
     lat (usec): min=164, max=4775, avg=991.34, stdev=237.67
    clat percentiles (usec):
     |  1.00th=[  545],  5.00th=[  619], 10.00th=[  701], 20.00th=[  799],
     | 30.00th=[  873], 40.00th=[  922], 50.00th=[  971], 60.00th=[ 1029],
     | 70.00th=[ 1090], 80.00th=[ 1156], 90.00th=[ 1237], 95.00th=[ 1336],
     | 99.00th=[ 1663], 99.50th=[ 2040], 99.90th=[ 2638], 99.95th=[ 2737],
     | 99.99th=[ 2900]
   bw (  KiB/s): min=146464, max=280920, per=100.00%, avg=257804.84, stdev=22216.21, samples=119
   iops        : min=36616, max=70230, avg=64451.38, stdev=5554.19, samples=119
  lat (usec)   : 250=0.01%, 500=0.31%, 750=14.63%, 1000=40.66%
  lat (msec)   : 2=43.88%, 4=0.52%, 10=0.01%
  cpu          : usr=17.48%, sys=55.69%, ctx=135698, majf=0, minf=73
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=3865809,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=252MiB/s (264MB/s), 252MiB/s-252MiB/s (264MB/s-264MB/s), io=14.7GiB (15.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randread:  read: IOPS=64.4k, BW=252MiB/s (264MB/s)(14.7GiB/60001msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 24835618 total
[0m

===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 26171060 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 27517489 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 28885457 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11376: Mon Jan 15 01:24:51 2024
  read: IOPS=64.2k, BW=251MiB/s (263MB/s)(14.7GiB/60001msec)
    slat (usec): min=4, max=2042, avg=12.91, stdev=47.97
    clat (usec): min=128, max=5526, avg=1979.84, stdev=365.22
     lat (usec): min=138, max=5536, avg=1993.07, stdev=362.27
    clat percentiles (usec):
     |  1.00th=[ 1139],  5.00th=[ 1270], 10.00th=[ 1401], 20.00th=[ 1713],
     | 30.00th=[ 1844], 40.00th=[ 1958], 50.00th=[ 2040], 60.00th=[ 2114],
     | 70.00th=[ 2180], 80.00th=[ 2245], 90.00th=[ 2376], 95.00th=[ 2474],
     | 99.00th=[ 2835], 99.50th=[ 2999], 99.90th=[ 3752], 99.95th=[ 3949],
     | 99.99th=[ 4228]
   bw (  KiB/s): min=209560, max=279384, per=100.00%, avg=256811.09, stdev=11040.80, samples=119
   iops        : min=52390, max=69846, avg=64202.77, stdev=2760.20, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.04%
  lat (msec)   : 2=45.32%, 4=54.59%, 10=0.04%
  cpu          : usr=16.68%, sys=53.85%, ctx=75616, majf=0, minf=137
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3849576,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=251MiB/s (263MB/s), 251MiB/s-251MiB/s (263MB/s-263MB/s), io=14.7GiB (15.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=64.2k, BW=251MiB/s (263MB/s)(14.7GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 30133020 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 31541971 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 32934985 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11423: Mon Jan 15 01:25:53 2024
  read: IOPS=66.1k, BW=258MiB/s (271MB/s)(15.1GiB/60001msec)
    slat (usec): min=4, max=1445, avg=12.49, stdev=43.39
    clat (usec): min=769, max=9710, avg=3855.84, stdev=483.16
     lat (usec): min=784, max=9738, avg=3868.65, stdev=482.97
    clat percentiles (usec):
     |  1.00th=[ 2966],  5.00th=[ 3163], 10.00th=[ 3261], 20.00th=[ 3425],
     | 30.00th=[ 3556], 40.00th=[ 3654], 50.00th=[ 3818], 60.00th=[ 3982],
     | 70.00th=[ 4146], 80.00th=[ 4293], 90.00th=[ 4490], 95.00th=[ 4621],
     | 99.00th=[ 5080], 99.50th=[ 5407], 99.90th=[ 6194], 99.95th=[ 6390],
     | 99.99th=[ 8094]
   bw (  KiB/s): min=222840, max=288008, per=100.00%, avg=264712.27, stdev=12907.46, samples=119
   iops        : min=55710, max=72002, avg=66178.08, stdev=3226.87, samples=119
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.01%, 4=61.65%, 10=38.34%
  cpu          : usr=17.03%, sys=55.96%, ctx=86825, majf=0, minf=265
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3968286,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=258MiB/s (271MB/s), 258MiB/s-258MiB/s (271MB/s-271MB/s), io=15.1GiB (16.3GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randread:  read: IOPS=66.1k, BW=258MiB/s (271MB/s)(15.1GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 34132959 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 35532993 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 587 MiB read, 0.000 read amp, 36919429 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11474: Mon Jan 15 01:26:56 2024
  read: IOPS=65.1k, BW=254MiB/s (266MB/s)(14.9GiB/60002msec)
    slat (usec): min=4, max=1405, avg=12.70, stdev=46.64
    clat (usec): min=650, max=22301, avg=7853.51, stdev=950.60
     lat (usec): min=1287, max=22326, avg=7866.52, stdev=952.19
    clat percentiles (usec):
     |  1.00th=[ 6652],  5.00th=[ 6915], 10.00th=[ 7046], 20.00th=[ 7242],
     | 30.00th=[ 7439], 40.00th=[ 7570], 50.00th=[ 7701], 60.00th=[ 7898],
     | 70.00th=[ 8094], 80.00th=[ 8356], 90.00th=[ 8717], 95.00th=[ 8979],
     | 99.00th=[ 9765], 99.50th=[11600], 99.90th=[19268], 99.95th=[20841],
     | 99.99th=[21890]
   bw (  KiB/s): min=162376, max=283952, per=100.00%, avg=260457.77, stdev=16906.53, samples=119
   iops        : min=40594, max=70988, avg=65114.45, stdev=4226.65, samples=119
  lat (usec)   : 750=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=99.11%, 20=0.81%, 50=0.07%
  cpu          : usr=16.48%, sys=55.03%, ctx=84612, majf=0, minf=521
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3903721,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
   READ: bw=254MiB/s (266MB/s), 254MiB/s-254MiB/s (266MB/s-266MB/s), io=14.9GiB (16.0GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randread:  read: IOPS=65.1k, BW=254MiB/s (266MB/s)(14.9GiB/60002msec)


===Fio: workload=read, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 38229391 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 39882010 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 41537402 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11521: Mon Jan 15 01:27:58 2024
  read: IOPS=76.7k, BW=299MiB/s (314MB/s)(17.5GiB/60001msec)
    slat (usec): min=4, max=548, avg= 8.04, stdev= 6.26
    clat (usec): min=172, max=1337, avg=407.63, stdev=78.21
     lat (usec): min=183, max=1343, avg=415.96, stdev=77.96
    clat percentiles (usec):
     |  1.00th=[  281],  5.00th=[  314], 10.00th=[  326], 20.00th=[  347],
     | 30.00th=[  363], 40.00th=[  379], 50.00th=[  396], 60.00th=[  412],
     | 70.00th=[  433], 80.00th=[  461], 90.00th=[  502], 95.00th=[  545],
     | 99.00th=[  676], 99.50th=[  766], 99.90th=[  881], 99.95th=[  930],
     | 99.99th=[ 1012]
   bw (  KiB/s): min=244552, max=325056, per=100.00%, avg=306702.52, stdev=13865.78, samples=119
   iops        : min=61138, max=81264, avg=76675.66, stdev=3466.44, samples=119
  lat (usec)   : 250=0.12%, 500=89.78%, 750=9.52%, 1000=0.56%
  lat (msec)   : 2=0.01%
  cpu          : usr=19.58%, sys=61.67%, ctx=202997, majf=0, minf=42
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=4599400,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=299MiB/s (314MB/s), 299MiB/s-299MiB/s (314MB/s-314MB/s), io=17.5GiB (18.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) read:  read: IOPS=76.7k, BW=299MiB/s (314MB/s)(17.5GiB/60001msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 42979515 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 44724360 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 46527794 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11567: Mon Jan 15 01:29:00 2024
  read: IOPS=80.5k, BW=314MiB/s (330MB/s)(18.4GiB/60001msec)
    slat (usec): min=4, max=8289, avg=10.32, stdev=23.81
    clat (usec): min=263, max=9700, avg=1578.82, stdev=314.75
     lat (usec): min=268, max=9706, avg=1589.42, stdev=315.42
    clat percentiles (usec):
     |  1.00th=[  996],  5.00th=[ 1139], 10.00th=[ 1254], 20.00th=[ 1385],
     | 30.00th=[ 1450], 40.00th=[ 1500], 50.00th=[ 1549], 60.00th=[ 1614],
     | 70.00th=[ 1663], 80.00th=[ 1745], 90.00th=[ 1893], 95.00th=[ 2089],
     | 99.00th=[ 2507], 99.50th=[ 2868], 99.90th=[ 4228], 99.95th=[ 4752],
     | 99.99th=[ 8356]
   bw (  KiB/s): min=160696, max=358944, per=100.00%, avg=321865.14, stdev=33971.80, samples=119
   iops        : min=40174, max=89736, avg=80466.27, stdev=8492.97, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=1.03%
  lat (msec)   : 2=92.10%, 4=6.72%, 10=0.15%
  cpu          : usr=18.03%, sys=63.03%, ctx=135096, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=4827551,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=314MiB/s (330MB/s), 314MiB/s-314MiB/s (330MB/s-330MB/s), io=18.4GiB (19.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=80.5k, BW=314MiB/s (330MB/s)(18.4GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 47814312 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 49147848 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 50532587 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11613: Mon Jan 15 01:30:03 2024
  read: IOPS=63.8k, BW=249MiB/s (261MB/s)(14.6GiB/60001msec)
    slat (usec): min=4, max=1749, avg=12.86, stdev=46.56
    clat (usec): min=57, max=6272, avg=1990.36, stdev=415.56
     lat (usec): min=162, max=6306, avg=2003.55, stdev=413.69
    clat percentiles (usec):
     |  1.00th=[ 1106],  5.00th=[ 1237], 10.00th=[ 1385], 20.00th=[ 1680],
     | 30.00th=[ 1811], 40.00th=[ 1926], 50.00th=[ 2040], 60.00th=[ 2114],
     | 70.00th=[ 2212], 80.00th=[ 2278], 90.00th=[ 2409], 95.00th=[ 2573],
     | 99.00th=[ 3097], 99.50th=[ 3228], 99.90th=[ 4621], 99.95th=[ 5080],
     | 99.99th=[ 5604]
   bw (  KiB/s): min=199216, max=286336, per=100.00%, avg=255404.97, stdev=15656.93, samples=119
   iops        : min=49804, max=71584, avg=63851.28, stdev=3914.22, samples=119
  lat (usec)   : 100=0.01%, 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.10%
  lat (msec)   : 2=46.54%, 4=53.18%, 10=0.16%
  cpu          : usr=16.09%, sys=57.81%, ctx=73721, majf=0, minf=137
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3829279,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=249MiB/s (261MB/s), 249MiB/s-249MiB/s (261MB/s-261MB/s), io=14.6GiB (15.7GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=63.8k, BW=249MiB/s (261MB/s)(14.6GiB/60001msec)


===Fio: workload=randread, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 51705951 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1110 MiB read, 0.000 read amp, 52921092 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1110 MiB read, 0.000 read amp, 54114310 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11660: Mon Jan 15 01:31:05 2024
  read: IOPS=53.6k, BW=419MiB/s (439MB/s)(24.6GiB/60002msec)
    slat (usec): min=4, max=3072, avg=15.59, stdev=70.04
    clat (usec): min=139, max=6378, avg=2368.28, stdev=557.30
     lat (usec): min=160, max=6394, avg=2384.21, stdev=555.31
    clat percentiles (usec):
     |  1.00th=[ 1237],  5.00th=[ 1401], 10.00th=[ 1582], 20.00th=[ 1860],
     | 30.00th=[ 2057], 40.00th=[ 2245], 50.00th=[ 2442], 60.00th=[ 2606],
     | 70.00th=[ 2737], 80.00th=[ 2835], 90.00th=[ 2999], 95.00th=[ 3163],
     | 99.00th=[ 3589], 99.50th=[ 3720], 99.90th=[ 4817], 99.95th=[ 5342],
     | 99.99th=[ 5800]
   bw (  KiB/s): min=341936, max=518128, per=99.98%, avg=428965.51, stdev=34586.47, samples=119
   iops        : min=42742, max=64766, avg=53620.69, stdev=4323.33, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=26.75%, 4=72.99%, 10=0.23%
  cpu          : usr=15.71%, sys=52.09%, ctx=62325, majf=0, minf=265
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3218142,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=419MiB/s (439MB/s), 419MiB/s-419MiB/s (439MB/s-439MB/s), io=24.6GiB (26.4GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randread:  read: IOPS=53.6k, BW=419MiB/s (439MB/s)(24.6GiB/60002msec)


===Fio: workload=randread, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1966 MiB read, 0.000 read amp, 55151638 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1998 MiB read, 0.000 read amp, 56237160 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2001 MiB read, 0.000 read amp, 57388594 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11706: Mon Jan 15 01:32:07 2024
  read: IOPS=45.1k, BW=705MiB/s (740MB/s)(41.3GiB/60003msec)
    slat (usec): min=4, max=2188, avg=18.99, stdev=37.00
    clat (usec): min=367, max=6165, avg=2813.48, stdev=326.19
     lat (usec): min=394, max=6176, avg=2832.87, stdev=327.44
    clat percentiles (usec):
     |  1.00th=[ 2278],  5.00th=[ 2442], 10.00th=[ 2507], 20.00th=[ 2573],
     | 30.00th=[ 2638], 40.00th=[ 2704], 50.00th=[ 2769], 60.00th=[ 2835],
     | 70.00th=[ 2900], 80.00th=[ 2999], 90.00th=[ 3163], 95.00th=[ 3326],
     | 99.00th=[ 4178], 99.50th=[ 4424], 99.90th=[ 4883], 99.95th=[ 5080],
     | 99.99th=[ 5407]
   bw (  KiB/s): min=616992, max=769408, per=100.00%, avg=722371.23, stdev=35691.28, samples=119
   iops        : min=38562, max=48088, avg=45148.20, stdev=2230.70, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.30%, 4=98.23%, 10=1.47%
  cpu          : usr=17.15%, sys=51.81%, ctx=462850, majf=0, minf=520
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=2708873,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=705MiB/s (740MB/s), 705MiB/s-705MiB/s (740MB/s-740MB/s), io=41.3GiB (44.4GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randread:  read: IOPS=45.1k, BW=705MiB/s (740MB/s)(41.3GiB/60003msec)


===Fio: workload=randread, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2177 MiB read, 0.000 read amp, 58405677 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3331 MiB read, 0.000 read amp, 59298874 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3334 MiB read, 0.000 read amp, 60185113 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11753: Mon Jan 15 01:33:10 2024
  read: IOPS=29.8k, BW=931MiB/s (976MB/s)(54.5GiB/60004msec)
    slat (usec): min=5, max=5522, avg=30.02, stdev=31.07
    clat (usec): min=275, max=10947, avg=4264.66, stdev=299.58
     lat (usec): min=287, max=10990, avg=4295.09, stdev=301.38
    clat percentiles (usec):
     |  1.00th=[ 3851],  5.00th=[ 3949], 10.00th=[ 3982], 20.00th=[ 4047],
     | 30.00th=[ 4113], 40.00th=[ 4178], 50.00th=[ 4228], 60.00th=[ 4293],
     | 70.00th=[ 4359], 80.00th=[ 4424], 90.00th=[ 4555], 95.00th=[ 4686],
     | 99.00th=[ 5342], 99.50th=[ 5669], 99.90th=[ 6587], 99.95th=[ 7308],
     | 99.99th=[ 9896]
   bw (  KiB/s): min=848448, max=987840, per=100.00%, avg=953372.50, stdev=24307.71, samples=119
   iops        : min=26514, max=30870, avg=29792.94, stdev=759.59, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.02%, 4=10.61%, 10=89.35%, 20=0.01%
  cpu          : usr=13.00%, sys=35.58%, ctx=1011871, majf=0, minf=1033
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1787011,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=931MiB/s (976MB/s), 931MiB/s-931MiB/s (976MB/s-976MB/s), io=54.5GiB (58.6GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randread:  read: IOPS=29.8k, BW=931MiB/s (976MB/s)(54.5GiB/60004msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3336 MiB read, 0.000 read amp, 61065007 total
[0m

===Fio: workload=randread, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4999 MiB read, 0.000 read amp, 61694645 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4999 MiB read, 0.000 read amp, 62399521 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5001 MiB read, 0.000 read amp, 63089061 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11799: Mon Jan 15 01:34:12 2024
  read: IOPS=17.4k, BW=1087MiB/s (1140MB/s)(63.7GiB/60008msec)
    slat (usec): min=6, max=2219, avg=54.47, stdev=29.37
    clat (usec): min=187, max=15778, avg=7299.61, stdev=669.99
     lat (usec): min=208, max=15870, avg=7354.53, stdev=674.53
    clat percentiles (usec):
     |  1.00th=[ 6783],  5.00th=[ 6849], 10.00th=[ 6915], 20.00th=[ 6980],
     | 30.00th=[ 7046], 40.00th=[ 7111], 50.00th=[ 7177], 60.00th=[ 7242],
     | 70.00th=[ 7308], 80.00th=[ 7439], 90.00th=[ 7701], 95.00th=[ 8848],
     | 99.00th=[ 9896], 99.50th=[10290], 99.90th=[11994], 99.95th=[12125],
     | 99.99th=[14091]
   bw (  MiB/s): min=  831, max= 1136, per=100.00%, avg=1087.71, stdev=53.80, samples=119
   iops        : min=13300, max=18182, avg=17403.33, stdev=860.72, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.02%, 1000=0.02%
  lat (msec)   : 2=0.05%, 4=0.12%, 10=98.92%, 20=0.86%
  cpu          : usr=8.16%, sys=21.99%, ctx=1039349, majf=0, minf=2057
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1043995,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=1087MiB/s (1140MB/s), 1087MiB/s-1087MiB/s (1140MB/s-1140MB/s), io=63.7GiB (68.4GB), run=60008-60008msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randread:  read: IOPS=17.4k, BW=1087MiB/s (1140MB/s)(63.7GiB/60008msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 64593527 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 66473344 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 68143568 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11846: Mon Jan 15 01:35:14 2024
  read: IOPS=84.0k, BW=328MiB/s (344MB/s)(19.2GiB/60002msec)
    slat (usec): min=4, max=4435, avg= 9.88, stdev=23.54
    clat (usec): min=131, max=6369, avg=1513.00, stdev=249.16
     lat (usec): min=140, max=6375, avg=1523.16, stdev=248.79
    clat percentiles (usec):
     |  1.00th=[  979],  5.00th=[ 1090], 10.00th=[ 1172], 20.00th=[ 1303],
     | 30.00th=[ 1401], 40.00th=[ 1467], 50.00th=[ 1516], 60.00th=[ 1582],
     | 70.00th=[ 1631], 80.00th=[ 1713], 90.00th=[ 1811], 95.00th=[ 1926],
     | 99.00th=[ 2147], 99.50th=[ 2212], 99.90th=[ 2474], 99.95th=[ 2606],
     | 99.99th=[ 2966]
   bw (  KiB/s): min=275104, max=363920, per=100.00%, avg=336051.76, stdev=23092.28, samples=119
   iops        : min=68776, max=90980, avg=84012.99, stdev=5773.06, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=1.40%
  lat (msec)   : 2=95.77%, 4=2.83%, 10=0.01%
  cpu          : usr=18.67%, sys=61.22%, ctx=117059, majf=0, minf=138
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=5037548,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=328MiB/s (344MB/s), 328MiB/s-328MiB/s (344MB/s-344MB/s), io=19.2GiB (20.6GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=84.0k, BW=328MiB/s (344MB/s)(19.2GiB/60002msec)


===Fio: workload=read, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 69656703 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 71202752 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 72522955 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11896: Mon Jan 15 01:36:17 2024
  read: IOPS=65.9k, BW=515MiB/s (540MB/s)(30.2GiB/60001msec)
    slat (usec): min=4, max=1864, avg=12.74, stdev=49.20
    clat (usec): min=184, max=6359, avg=1926.21, stdev=467.78
     lat (usec): min=195, max=6376, avg=1939.27, stdev=467.70
    clat percentiles (usec):
     |  1.00th=[ 1188],  5.00th=[ 1287], 10.00th=[ 1401], 20.00th=[ 1532],
     | 30.00th=[ 1631], 40.00th=[ 1729], 50.00th=[ 1811], 60.00th=[ 1926],
     | 70.00th=[ 2114], 80.00th=[ 2376], 90.00th=[ 2638], 95.00th=[ 2769],
     | 99.00th=[ 3064], 99.50th=[ 3163], 99.90th=[ 3654], 99.95th=[ 4047],
     | 99.99th=[ 5145]
   bw (  KiB/s): min=405328, max=639392, per=100.00%, avg=527976.74, stdev=78848.08, samples=119
   iops        : min=50666, max=79924, avg=65997.14, stdev=9856.05, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=64.45%, 4=35.48%, 10=0.05%
  cpu          : usr=15.63%, sys=55.95%, ctx=90585, majf=0, minf=265
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3956680,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=515MiB/s (540MB/s), 515MiB/s-515MiB/s (540MB/s-540MB/s), io=30.2GiB (32.4GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) read:  read: IOPS=65.9k, BW=515MiB/s (540MB/s)(30.2GiB/60001msec)


===Fio: workload=read, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 73701606 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 74966113 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 76172902 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11943: Mon Jan 15 01:37:19 2024
  read: IOPS=50.1k, BW=782MiB/s (820MB/s)(45.8GiB/60003msec)
    slat (usec): min=4, max=7684, avg=17.47, stdev=30.84
    clat (usec): min=330, max=10859, avg=2537.64, stdev=291.47
     lat (usec): min=368, max=10872, avg=2555.47, stdev=292.84
    clat percentiles (usec):
     |  1.00th=[ 2024],  5.00th=[ 2147], 10.00th=[ 2212], 20.00th=[ 2311],
     | 30.00th=[ 2376], 40.00th=[ 2442], 50.00th=[ 2507], 60.00th=[ 2573],
     | 70.00th=[ 2638], 80.00th=[ 2704], 90.00th=[ 2835], 95.00th=[ 2966],
     | 99.00th=[ 3687], 99.50th=[ 3916], 99.90th=[ 4359], 99.95th=[ 4490],
     | 99.99th=[ 5932]
   bw (  KiB/s): min=712032, max=920864, per=100.00%, avg=801380.57, stdev=44220.88, samples=119
   iops        : min=44502, max=57554, avg=50086.32, stdev=2763.78, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.63%, 4=98.99%, 10=0.38%, 20=0.01%
  cpu          : usr=15.53%, sys=53.33%, ctx=584230, majf=0, minf=522
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=3003212,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=782MiB/s (820MB/s), 782MiB/s-782MiB/s (820MB/s-820MB/s), io=45.8GiB (49.2GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) read:  read: IOPS=50.1k, BW=782MiB/s (820MB/s)(45.8GiB/60003msec)


===Fio: workload=read, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3333 MiB read, 0.000 read amp, 77160186 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3333 MiB read, 0.000 read amp, 78101177 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 3333 MiB read, 0.000 read amp, 79031903 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=11989: Mon Jan 15 01:38:22 2024
  read: IOPS=31.8k, BW=995MiB/s (1043MB/s)(58.3GiB/60004msec)
    slat (usec): min=5, max=7834, avg=28.67, stdev=29.90
    clat (usec): min=233, max=13609, avg=3990.32, stdev=348.86
     lat (usec): min=268, max=13651, avg=4019.35, stdev=350.99
    clat percentiles (usec):
     |  1.00th=[ 3425],  5.00th=[ 3523], 10.00th=[ 3621], 20.00th=[ 3720],
     | 30.00th=[ 3818], 40.00th=[ 3884], 50.00th=[ 3982], 60.00th=[ 4047],
     | 70.00th=[ 4146], 80.00th=[ 4228], 90.00th=[ 4359], 95.00th=[ 4490],
     | 99.00th=[ 4948], 99.50th=[ 5342], 99.90th=[ 6390], 99.95th=[ 7177],
     | 99.99th=[11600]
   bw (  KiB/s): min=949120, max=1153408, per=100.00%, avg=1018998.32, stdev=54019.25, samples=119
   iops        : min=29660, max=36044, avg=31843.76, stdev=1688.08, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.02%, 4=52.84%, 10=47.11%, 20=0.02%
  cpu          : usr=11.04%, sys=36.46%, ctx=1081033, majf=0, minf=1034
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1909746,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=995MiB/s (1043MB/s), 995MiB/s-995MiB/s (1043MB/s-1043MB/s), io=58.3GiB (62.6GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) read:  read: IOPS=31.8k, BW=995MiB/s (1043MB/s)(58.3GiB/60004msec)


===Fio: workload=read, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5000 MiB read, 0.000 read amp, 79773072 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5000 MiB read, 0.000 read amp, 80461340 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 5000 MiB read, 0.000 read amp, 81138328 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12036: Mon Jan 15 01:39:24 2024
  read: IOPS=16.9k, BW=1057MiB/s (1108MB/s)(61.9GiB/60008msec)
    slat (usec): min=6, max=4428, avg=56.70, stdev=27.30
    clat (usec): min=399, max=22040, avg=7509.52, stdev=613.87
     lat (usec): min=420, max=22082, avg=7566.63, stdev=618.08
    clat percentiles (usec):
     |  1.00th=[ 6849],  5.00th=[ 6915], 10.00th=[ 6915], 20.00th=[ 6980],
     | 30.00th=[ 7111], 40.00th=[ 7439], 50.00th=[ 7504], 60.00th=[ 7570],
     | 70.00th=[ 7701], 80.00th=[ 7767], 90.00th=[ 8029], 95.00th=[ 8848],
     | 99.00th=[ 9372], 99.50th=[ 9765], 99.90th=[11994], 99.95th=[13566],
     | 99.99th=[15533]
   bw (  MiB/s): min=  818, max= 1142, per=100.00%, avg=1057.59, stdev=64.03, samples=119
   iops        : min=13092, max=18274, avg=16921.36, stdev=1024.53, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.02%, 4=0.05%, 10=99.50%, 20=0.41%, 50=0.01%
  cpu          : usr=7.28%, sys=21.37%, ctx=1010311, majf=0, minf=2058
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1014751,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=1057MiB/s (1108MB/s), 1057MiB/s-1057MiB/s (1108MB/s-1108MB/s), io=61.9GiB (66.5GB), run=60008-60008msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) read:  read: IOPS=16.9k, BW=1057MiB/s (1108MB/s)(61.9GiB/60008msec)


===Fio: workload=randwrite, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4999 MiB read, 0.000 read amp, 81604860 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12088: Mon Jan 15 01:40:26 2024
  write: IOPS=5969, BW=23.3MiB/s (24.5MB/s)(1399MiB/60001msec); 0 zone resets
    slat (usec): min=9, max=458, avg=13.01, stdev= 5.80
    clat (nsec): min=627, max=3198.0k, avg=150591.28, stdev=61490.53
     lat (usec): min=122, max=3218, avg=163.92, stdev=62.28
    clat percentiles (usec):
     |  1.00th=[  117],  5.00th=[  120], 10.00th=[  123], 20.00th=[  127],
     | 30.00th=[  131], 40.00th=[  135], 50.00th=[  139], 60.00th=[  143],
     | 70.00th=[  147], 80.00th=[  155], 90.00th=[  167], 95.00th=[  245],
     | 99.00th=[  400], 99.50th=[  469], 99.90th=[  594], 99.95th=[ 1123],
     | 99.99th=[ 1876]
   bw (  KiB/s): min=21248, max=26320, per=100.00%, avg=23889.75, stdev=1129.62, samples=119
   iops        : min= 5312, max= 6580, avg=5972.42, stdev=282.42, samples=119
  lat (nsec)   : 750=0.01%
  lat (usec)   : 2=0.01%, 250=95.13%, 500=4.52%, 750=0.29%, 1000=0.01%
  lat (msec)   : 2=0.05%, 4=0.01%
  cpu          : usr=4.76%, sys=11.95%, ctx=358171, majf=0, minf=12
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,358164,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=23.3MiB/s (24.5MB/s), 23.3MiB/s-23.3MiB/s (24.5MB/s-24.5MB/s), io=1399MiB (1467MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5969, BW=23.3MiB/s (24.5MB/s)(1399MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4996 MiB read, 0.000 read amp, 81604996 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12142: Mon Jan 15 01:41:29 2024
  write: IOPS=17.7k, BW=69.3MiB/s (72.7MB/s)(4158MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=453, avg= 9.72, stdev= 6.09
    clat (usec): min=30, max=5365, avg=212.99, stdev=103.59
     lat (usec): min=107, max=5375, avg=222.99, stdev=103.39
    clat percentiles (usec):
     |  1.00th=[  126],  5.00th=[  139], 10.00th=[  149], 20.00th=[  163],
     | 30.00th=[  172], 40.00th=[  182], 50.00th=[  192], 60.00th=[  202],
     | 70.00th=[  215], 80.00th=[  237], 90.00th=[  306], 95.00th=[  371],
     | 99.00th=[  510], 99.50th=[  594], 99.90th=[ 1631], 99.95th=[ 1942],
     | 99.99th=[ 2409]
   bw (  KiB/s): min=64144, max=76888, per=100.00%, avg=70979.09, stdev=2803.97, samples=119
   iops        : min=16036, max=19222, avg=17744.79, stdev=700.99, samples=119
  lat (usec)   : 50=0.01%, 100=0.01%, 250=83.19%, 500=15.69%, 750=0.86%
  lat (usec)   : 1000=0.04%
  lat (msec)   : 2=0.17%, 4=0.04%, 10=0.01%
  cpu          : usr=9.22%, sys=22.00%, ctx=435259, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1064470,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
  WRITE: bw=69.3MiB/s (72.7MB/s), 69.3MiB/s-69.3MiB/s (72.7MB/s-72.7MB/s), io=4158MiB (4360MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=17.7k, BW=69.3MiB/s (72.7MB/s)(4158MiB/60001msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4995 MiB read, 0.000 read amp, 81605072 total
[0m

===Fio: workload=randwrite, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4994 MiB read, 0.000 read amp, 81605148 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12195: Mon Jan 15 01:42:31 2024
  write: IOPS=27.7k, BW=108MiB/s (114MB/s)(6500MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=500, avg= 9.55, stdev= 5.91
    clat (usec): min=64, max=3547, avg=276.29, stdev=119.50
     lat (usec): min=110, max=3557, avg=286.13, stdev=119.36
    clat percentiles (usec):
     |  1.00th=[  133],  5.00th=[  161], 10.00th=[  180], 20.00th=[  202],
     | 30.00th=[  221], 40.00th=[  237], 50.00th=[  253], 60.00th=[  273],
     | 70.00th=[  293], 80.00th=[  330], 90.00th=[  400], 95.00th=[  453],
     | 99.00th=[  611], 99.50th=[  807], 99.90th=[ 1532], 99.95th=[ 1795],
     | 99.99th=[ 2409]
   bw (  KiB/s): min=86880, max=119288, per=100.00%, avg=110963.23, stdev=5058.78, samples=119
   iops        : min=21720, max=29822, avg=27740.82, stdev=1264.69, samples=119
  lat (usec)   : 100=0.01%, 250=48.14%, 500=49.56%, 750=1.74%, 1000=0.14%
  lat (msec)   : 2=0.40%, 4=0.03%
  cpu          : usr=12.76%, sys=30.94%, ctx=420091, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1664035,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
  WRITE: bw=108MiB/s (114MB/s), 108MiB/s-108MiB/s (114MB/s-114MB/s), io=6500MiB (6816MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=27.7k, BW=108MiB/s (114MB/s)(6500MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4991 MiB read, 0.000 read amp, 81605308 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12250: Mon Jan 15 01:43:33 2024
  write: IOPS=38.9k, BW=152MiB/s (159MB/s)(9111MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=479, avg= 7.85, stdev= 5.35
    clat (usec): min=103, max=50622, avg=401.68, stdev=227.99
     lat (usec): min=122, max=50627, avg=409.76, stdev=227.92
    clat percentiles (usec):
     |  1.00th=[  159],  5.00th=[  221], 10.00th=[  243], 20.00th=[  285],
     | 30.00th=[  318], 40.00th=[  351], 50.00th=[  379], 60.00th=[  412],
     | 70.00th=[  449], 80.00th=[  494], 90.00th=[  553], 95.00th=[  611],
     | 99.00th=[  971], 99.50th=[ 1532], 99.90th=[ 2147], 99.95th=[ 2507],
     | 99.99th=[ 3392]
   bw (  KiB/s): min=132464, max=168640, per=100.00%, avg=155659.97, stdev=6660.71, samples=119
   iops        : min=33116, max=42160, avg=38914.97, stdev=1665.20, samples=119
  lat (usec)   : 250=11.92%, 500=69.45%, 750=16.93%, 1000=0.74%
  lat (msec)   : 2=0.84%, 4=0.13%, 10=0.01%, 50=0.01%, 100=0.01%
  cpu          : usr=13.07%, sys=32.90%, ctx=307193, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2332482,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
  WRITE: bw=152MiB/s (159MB/s), 152MiB/s-152MiB/s (159MB/s-159MB/s), io=9111MiB (9554MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=38.9k, BW=152MiB/s (159MB/s)(9111MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4988 MiB read, 0.000 read amp, 81605476 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12303: Mon Jan 15 01:44:36 2024
  write: IOPS=47.6k, BW=186MiB/s (195MB/s)(10.9GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=636, avg= 8.39, stdev= 6.29
    clat (usec): min=207, max=121642, avg=662.47, stdev=1869.93
     lat (usec): min=213, max=121647, avg=671.11, stdev=1870.03
    clat percentiles (usec):
     |  1.00th=[  289],  5.00th=[  359], 10.00th=[  392], 20.00th=[  441],
     | 30.00th=[  469], 40.00th=[  498], 50.00th=[  529], 60.00th=[  553],
     | 70.00th=[  594], 80.00th=[  644], 90.00th=[  750], 95.00th=[ 1434],
     | 99.00th=[ 2073], 99.50th=[ 2409], 99.90th=[23200], 99.95th=[48497],
     | 99.99th=[84411]
   bw (  KiB/s): min=59112, max=240312, per=99.92%, avg=190049.55, stdev=44094.46, samples=119
   iops        : min=14778, max=60078, avg=47512.39, stdev=11023.63, samples=119
  lat (usec)   : 250=0.15%, 500=41.20%, 750=48.74%, 1000=3.90%
  lat (msec)   : 2=4.71%, 4=1.07%, 10=0.06%, 20=0.05%, 50=0.06%
  lat (msec)   : 100=0.04%, 250=0.01%
  cpu          : usr=15.69%, sys=40.58%, ctx=252533, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2853091,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=186MiB/s (195MB/s), 186MiB/s-186MiB/s (195MB/s-195MB/s), io=10.9GiB (11.7GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=47.6k, BW=186MiB/s (195MB/s)(10.9GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4984 MiB read, 0.000 read amp, 81605652 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12357: Mon Jan 15 01:45:38 2024
  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=698, avg= 8.12, stdev= 6.67
    clat (usec): min=391, max=244189, avg=1263.89, stdev=2920.71
     lat (usec): min=414, max=244199, avg=1272.24, stdev=2920.70
    clat percentiles (usec):
     |  1.00th=[   523],  5.00th=[   660], 10.00th=[   766], 20.00th=[   881],
     | 30.00th=[   947], 40.00th=[  1012], 50.00th=[  1090], 60.00th=[  1156],
     | 70.00th=[  1237], 80.00th=[  1352], 90.00th=[  1532], 95.00th=[  1729],
     | 99.00th=[  2704], 99.50th=[  3785], 99.90th=[ 45351], 99.95th=[ 60031],
     | 99.99th=[125305]
   bw (  KiB/s): min=92544, max=253440, per=100.00%, avg=201650.08, stdev=24533.04, samples=119
   iops        : min=23136, max=63360, avg=50412.54, stdev=6133.24, samples=119
  lat (usec)   : 500=0.57%, 750=8.41%, 1000=29.19%
  lat (msec)   : 2=58.59%, 4=2.77%, 10=0.14%, 20=0.11%, 50=0.14%
  lat (msec)   : 100=0.07%, 250=0.01%
  cpu          : usr=14.80%, sys=41.42%, ctx=166139, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,3014108,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=196MiB/s (206MB/s), 196MiB/s-196MiB/s (206MB/s-206MB/s), io=11.5GiB (12.3GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4980 MiB read, 0.000 read amp, 81605837 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12414: Mon Jan 15 01:46:40 2024
  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=109885, avg=17.56, stdev=289.52
    clat (usec): min=238, max=116348, avg=2530.18, stdev=3265.97
     lat (usec): min=271, max=116362, avg=2547.99, stdev=3278.48
    clat percentiles (usec):
     |  1.00th=[ 1188],  5.00th=[ 1614], 10.00th=[ 1762], 20.00th=[ 1926],
     | 30.00th=[ 2040], 40.00th=[ 2147], 50.00th=[ 2245], 60.00th=[ 2343],
     | 70.00th=[ 2474], 80.00th=[ 2638], 90.00th=[ 2966], 95.00th=[ 3392],
     | 99.00th=[ 5014], 99.50th=[16581], 99.90th=[60031], 99.95th=[76022],
     | 99.99th=[90702]
   bw (  KiB/s): min=125800, max=239064, per=100.00%, avg=201113.01, stdev=18913.26, samples=119
   iops        : min=31450, max=59766, avg=50278.27, stdev=4728.28, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.04%
  lat (msec)   : 2=26.00%, 4=71.93%, 10=1.35%, 20=0.23%, 50=0.29%
  lat (msec)   : 100=0.14%, 250=0.01%
  cpu          : usr=15.15%, sys=42.26%, ctx=159051, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3012032,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=196MiB/s (206MB/s), 196MiB/s-196MiB/s (206MB/s-206MB/s), io=11.5GiB (12.3GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4977 MiB read, 0.000 read amp, 81606022 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12468: Mon Jan 15 01:47:43 2024
  write: IOPS=52.0k, BW=203MiB/s (213MB/s)(11.9GiB/60004msec); 0 zone resets
    slat (usec): min=4, max=147433, avg=16.70, stdev=367.05
    clat (usec): min=1195, max=151854, avg=4908.16, stdev=5847.61
     lat (usec): min=1261, max=151861, avg=4925.13, stdev=5858.84
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[    4], 10.00th=[    4], 20.00th=[    4],
     | 30.00th=[    4], 40.00th=[    4], 50.00th=[    5], 60.00th=[    5],
     | 70.00th=[    5], 80.00th=[    5], 90.00th=[    6], 95.00th=[    7],
     | 99.00th=[   30], 99.50th=[   47], 99.90th=[   86], 99.95th=[  101],
     | 99.99th=[  133]
   bw (  KiB/s): min=129664, max=248568, per=100.00%, avg=207849.48, stdev=20841.69, samples=119
   iops        : min=32416, max=62142, avg=51962.37, stdev=5210.37, samples=119
  lat (msec)   : 2=0.01%, 4=47.08%, 10=50.78%, 20=0.76%, 50=0.93%
  lat (msec)   : 100=0.40%, 250=0.05%
  cpu          : usr=15.84%, sys=47.82%, ctx=116130, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3117561,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
  WRITE: bw=203MiB/s (213MB/s), 203MiB/s-203MiB/s (213MB/s-213MB/s), io=11.9GiB (12.8GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=52.0k, BW=203MiB/s (213MB/s)(11.9GiB/60004msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4973 MiB read, 0.000 read amp, 81606208 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12521: Mon Jan 15 01:48:45 2024
  write: IOPS=52.4k, BW=205MiB/s (215MB/s)(12.0GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=129663, avg=16.59, stdev=357.33
    clat (usec): min=477, max=139151, avg=9754.07, stdev=8034.05
     lat (usec): min=483, max=139163, avg=9770.93, stdev=8041.76
    clat percentiles (msec):
     |  1.00th=[    7],  5.00th=[    8], 10.00th=[    8], 20.00th=[    8],
     | 30.00th=[    8], 40.00th=[    8], 50.00th=[    9], 60.00th=[    9],
     | 70.00th=[    9], 80.00th=[   10], 90.00th=[   11], 95.00th=[   16],
     | 99.00th=[   53], 99.50th=[   72], 99.90th=[  102], 99.95th=[  110],
     | 99.99th=[  138]
   bw (  KiB/s): min=151168, max=256920, per=100.00%, avg=209645.71, stdev=21116.11, samples=119
   iops        : min=37792, max=64230, avg=52411.43, stdev=5279.02, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=86.56%, 20=10.20%, 50=2.13%
  lat (msec)   : 100=0.99%, 250=0.12%
  cpu          : usr=15.77%, sys=46.97%, ctx=128889, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3143203,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
  WRITE: bw=205MiB/s (215MB/s), 205MiB/s-205MiB/s (215MB/s-215MB/s), io=12.0GiB (12.9GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=52.4k, BW=205MiB/s (215MB/s)(12.0GiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4969 MiB read, 0.000 read amp, 81606394 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12574: Mon Jan 15 01:49:47 2024
  write: IOPS=45.1k, BW=176MiB/s (185MB/s)(10.3GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=489, avg= 8.36, stdev= 5.91
    clat (usec): min=199, max=1674.1k, avg=698.75, stdev=12189.90
     lat (usec): min=206, max=1674.1k, avg=707.37, stdev=12189.89
    clat percentiles (usec):
     |  1.00th=[   289],  5.00th=[   355], 10.00th=[   388], 20.00th=[   429],
     | 30.00th=[   457], 40.00th=[   482], 50.00th=[   506], 60.00th=[   537],
     | 70.00th=[   570], 80.00th=[   611], 90.00th=[   676], 95.00th=[   750],
     | 99.00th=[   979], 99.50th=[  1172], 99.90th=[ 29754], 99.95th=[ 51119],
     | 99.99th=[110625]
   bw (  KiB/s): min= 4800, max=256824, per=100.00%, avg=196779.60, stdev=51242.53, samples=109
   iops        : min= 1200, max=64206, avg=49194.90, stdev=12810.63, samples=109
  lat (usec)   : 250=0.25%, 500=47.26%, 750=47.54%, 1000=4.07%
  lat (msec)   : 2=0.61%, 4=0.06%, 10=0.04%, 20=0.05%, 50=0.07%
  lat (msec)   : 100=0.04%, 250=0.01%, 500=0.01%, 2000=0.01%
  cpu          : usr=13.86%, sys=36.92%, ctx=175029, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2708420,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=176MiB/s (185MB/s), 176MiB/s-176MiB/s (185MB/s-185MB/s), io=10.3GiB (11.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) write:  write: IOPS=45.1k, BW=176MiB/s (185MB/s)(10.3GiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4967 MiB read, 0.000 read amp, 81606530 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12628: Mon Jan 15 01:50:50 2024
  write: IOPS=44.7k, BW=174MiB/s (183MB/s)(10.2GiB/60002msec); 0 zone resets
    slat (usec): min=4, max=1280.1k, avg=19.68, stdev=1937.36
    clat (usec): min=177, max=1282.7k, avg=2843.67, stdev=21839.28
     lat (usec): min=184, max=1282.7k, avg=2863.69, stdev=21925.08
    clat percentiles (usec):
     |  1.00th=[   1172],  5.00th=[   1450], 10.00th=[   1565],
     | 20.00th=[   1713], 30.00th=[   1811], 40.00th=[   1909],
     | 50.00th=[   2024], 60.00th=[   2147], 70.00th=[   2311],
     | 80.00th=[   2540], 90.00th=[   3064], 95.00th=[   3982],
     | 99.00th=[   5800], 99.50th=[  17171], 99.90th=[  74974],
     | 99.95th=[ 112722], 99.99th=[1199571]
   bw (  KiB/s): min= 5496, max=277392, per=100.00%, avg=192743.27, stdev=50974.52, samples=110
   iops        : min= 1374, max=69348, avg=48185.84, stdev=12743.64, samples=110
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.16%
  lat (msec)   : 2=48.54%, 4=46.35%, 10=4.26%, 20=0.24%, 50=0.24%
  lat (msec)   : 100=0.12%, 250=0.04%, 1000=0.01%, 2000=0.03%
  cpu          : usr=12.82%, sys=49.22%, ctx=86454, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2680103,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=174MiB/s (183MB/s), 174MiB/s-174MiB/s (183MB/s-183MB/s), io=10.2GiB (11.0GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=44.7k, BW=174MiB/s (183MB/s)(10.2GiB/60002msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4966 MiB read, 0.000 read amp, 81606598 total
[0m

===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4965 MiB read, 0.000 read amp, 81606666 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12683: Mon Jan 15 01:51:52 2024
  write: IOPS=43.2k, BW=169MiB/s (177MB/s)(9.90GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=1461.6k, avg=20.73, stdev=1979.40
    clat (usec): min=153, max=1464.0k, avg=2936.85, stdev=22305.48
     lat (usec): min=180, max=1464.0k, avg=2957.84, stdev=22393.05
    clat percentiles (usec):
     |  1.00th=[   1303],  5.00th=[   1647], 10.00th=[   1778],
     | 20.00th=[   1909], 30.00th=[   2040], 40.00th=[   2147],
     | 50.00th=[   2245], 60.00th=[   2376], 70.00th=[   2507],
     | 80.00th=[   2671], 90.00th=[   2966], 95.00th=[   3425],
     | 99.00th=[   4883], 99.50th=[  15533], 99.90th=[  68682],
     | 99.95th=[  85459], 99.99th=[1233126]
   bw (  KiB/s): min= 2344, max=261016, per=100.00%, avg=185532.68, stdev=52425.77, samples=111
   iops        : min=  586, max=65254, avg=46383.15, stdev=13106.45, samples=111
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=27.38%, 4=70.54%, 10=1.44%, 20=0.19%, 50=0.25%
  lat (msec)   : 100=0.12%, 250=0.01%, 500=0.01%, 1000=0.01%, 2000=0.03%
  cpu          : usr=13.40%, sys=36.13%, ctx=143442, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2594929,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=169MiB/s (177MB/s), 169MiB/s-169MiB/s (177MB/s-177MB/s), io=9.90GiB (10.6GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=43.2k, BW=169MiB/s (177MB/s)(9.90GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4962 MiB read, 0.000 read amp, 81606840 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12736: Mon Jan 15 01:52:54 2024
  write: IOPS=23.9k, BW=187MiB/s (196MB/s)(10.9GiB/60003msec); 0 zone resets
    slat (usec): min=4, max=2326.8k, avg=39.55, stdev=4318.55
    clat (usec): min=269, max=2330.3k, avg=5310.32, stdev=48644.70
     lat (usec): min=279, max=2330.3k, avg=5350.13, stdev=48836.90
    clat percentiles (usec):
     |  1.00th=[   1532],  5.00th=[   2008], 10.00th=[   2245],
     | 20.00th=[   2540], 30.00th=[   2737], 40.00th=[   2900],
     | 50.00th=[   3032], 60.00th=[   3195], 70.00th=[   3392],
     | 80.00th=[   3654], 90.00th=[   4146], 95.00th=[   4883],
     | 99.00th=[  39060], 99.50th=[  66847], 99.90th=[ 160433],
     | 99.95th=[1702888], 99.99th=[2071987]
   bw (  KiB/s): min=27920, max=364352, per=100.00%, avg=231379.76, stdev=69740.08, samples=98
   iops        : min= 3490, max=45544, avg=28922.51, stdev=8717.54, samples=98
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=4.82%, 4=83.17%, 10=9.55%, 20=0.72%, 50=0.94%
  lat (msec)   : 100=0.57%, 250=0.13%, 500=0.02%, 2000=0.05%, >=2000=0.02%
  cpu          : usr=7.03%, sys=21.14%, ctx=98033, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1435081,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=187MiB/s (196MB/s), 187MiB/s-187MiB/s (196MB/s-196MB/s), io=10.9GiB (11.8GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=23.9k, BW=187MiB/s (196MB/s)(10.9GiB/60003msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randwrite, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4958 MiB read, 0.000 read amp, 81607012 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12789: Mon Jan 15 01:53:57 2024
  write: IOPS=15.0k, BW=235MiB/s (246MB/s)(13.8GiB/60007msec); 0 zone resets
    slat (usec): min=5, max=1689.0k, avg=62.37, stdev=3666.15
    clat (usec): min=340, max=1697.2k, avg=8453.21, stdev=41268.60
     lat (usec): min=364, max=1697.2k, avg=8516.04, stdev=41430.36
    clat percentiles (msec):
     |  1.00th=[    3],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    6], 40.00th=[    6], 50.00th=[    6], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    7], 90.00th=[    7], 95.00th=[   16],
     | 99.00th=[   59], 99.50th=[   78], 99.90th=[  167], 99.95th=[ 1418],
     | 99.99th=[ 1703]
   bw (  KiB/s): min=24576, max=401728, per=100.00%, avg=262669.80, stdev=61928.43, samples=109
   iops        : min= 1536, max=25108, avg=16416.86, stdev=3870.53, samples=109
  lat (usec)   : 500=0.01%, 750=0.03%, 1000=0.08%
  lat (msec)   : 2=0.43%, 4=1.62%, 10=91.49%, 20=2.31%, 50=2.56%
  lat (msec)   : 100=1.20%, 250=0.21%, 2000=0.07%
  cpu          : usr=6.80%, sys=24.57%, ctx=198889, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,901619,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=235MiB/s (246MB/s), 235MiB/s-235MiB/s (246MB/s-246MB/s), io=13.8GiB (14.8GB), run=60007-60007msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=15.0k, BW=235MiB/s (246MB/s)(13.8GiB/60007msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randwrite, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4955 MiB read, 0.000 read amp, 81607180 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12842: Mon Jan 15 01:54:59 2024
  write: IOPS=6663, BW=208MiB/s (218MB/s)(12.2GiB/60010msec); 0 zone resets
    slat (usec): min=5, max=2248.9k, avg=144.69, stdev=7141.86
    clat (usec): min=421, max=2261.6k, avg=19060.45, stdev=80223.14
     lat (usec): min=492, max=2261.7k, avg=19205.79, stdev=80535.89
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   11], 60.00th=[   11],
     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   31], 95.00th=[   56],
     | 99.00th=[  113], 99.50th=[  133], 99.90th=[ 1804], 99.95th=[ 2198],
     | 99.99th=[ 2265]
   bw (  KiB/s): min= 2496, max=394112, per=100.00%, avg=241565.87, stdev=74313.96, samples=105
   iops        : min=   78, max=12316, avg=7548.93, stdev=2322.31, samples=105
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.03%
  lat (msec)   : 2=0.11%, 4=0.28%, 10=48.24%, 20=37.90%, 50=7.38%
  lat (msec)   : 100=4.52%, 250=1.37%, 2000=0.10%, >=2000=0.06%
  cpu          : usr=4.94%, sys=10.51%, ctx=198416, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,399876,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=208MiB/s (218MB/s), 208MiB/s-208MiB/s (218MB/s-218MB/s), io=12.2GiB (13.1GB), run=60010-60010msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=6663, BW=208MiB/s (218MB/s)(12.2GiB/60010msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randwrite, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4952 MiB read, 0.000 read amp, 81607344 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12899: Mon Jan 15 01:56:02 2024
  write: IOPS=3292, BW=206MiB/s (216MB/s)(12.1GiB/60089msec); 0 zone resets
    slat (usec): min=6, max=2256.7k, avg=294.53, stdev=9947.05
    clat (usec): min=551, max=2316.1k, avg=38576.78, stdev=111174.65
     lat (usec): min=694, max=2316.5k, avg=38872.18, stdev=111609.77
    clat percentiles (msec):
     |  1.00th=[   16],  5.00th=[   17], 10.00th=[   18], 20.00th=[   19],
     | 30.00th=[   20], 40.00th=[   21], 50.00th=[   21], 60.00th=[   22],
     | 70.00th=[   25], 80.00th=[   39], 90.00th=[   70], 95.00th=[  101],
     | 99.00th=[  155], 99.50th=[  188], 99.90th=[ 2005], 99.95th=[ 2265],
     | 99.99th=[ 2265]
   bw (  KiB/s): min=  768, max=463744, per=100.00%, avg=236516.49, stdev=81731.08, samples=107
   iops        : min=   12, max= 7246, avg=3695.57, stdev=1277.05, samples=107
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.06%, 4=0.08%, 10=0.18%, 20=40.18%, 50=43.62%
  lat (msec)   : 100=10.89%, 250=4.55%, 500=0.12%, 2000=0.26%, >=2000=0.06%
  cpu          : usr=3.89%, sys=6.94%, ctx=169046, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,197840,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=206MiB/s (216MB/s), 206MiB/s-206MiB/s (216MB/s-216MB/s), io=12.1GiB (13.0GB), run=60089-60089msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=3292, BW=206MiB/s (216MB/s)(12.1GiB/60089msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4949 MiB read, 0.000 read amp, 81607506 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=12955: Mon Jan 15 01:57:04 2024
  write: IOPS=48.5k, BW=189MiB/s (199MB/s)(11.1GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=1326.0k, avg=18.53, stdev=1481.89
    clat (usec): min=192, max=1328.6k, avg=2619.49, stdev=16703.11
     lat (usec): min=204, max=1328.6k, avg=2638.28, stdev=16768.66
    clat percentiles (usec):
     |  1.00th=[   1303],  5.00th=[   1598], 10.00th=[   1729],
     | 20.00th=[   1844], 30.00th=[   1942], 40.00th=[   2040],
     | 50.00th=[   2114], 60.00th=[   2180], 70.00th=[   2278],
     | 80.00th=[   2409], 90.00th=[   2573], 95.00th=[   2769],
     | 99.00th=[   3851], 99.50th=[  19530], 99.90th=[  65274],
     | 99.95th=[  93848], 99.99th=[1002439]
   bw (  KiB/s): min=17472, max=277824, per=100.00%, avg=206263.64, stdev=40304.54, samples=112
   iops        : min= 4368, max=69456, avg=51565.89, stdev=10076.12, samples=112
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.12%
  lat (msec)   : 2=35.45%, 4=63.47%, 10=0.30%, 20=0.17%, 50=0.31%
  lat (msec)   : 100=0.13%, 250=0.02%, 1000=0.02%, 2000=0.01%
  cpu          : usr=13.98%, sys=40.08%, ctx=146523, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2909329,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=189MiB/s (199MB/s), 189MiB/s-189MiB/s (199MB/s-199MB/s), io=11.1GiB (11.9GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=48.5k, BW=189MiB/s (199MB/s)(11.1GiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4947 MiB read, 0.000 read amp, 81607642 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=13010: Mon Jan 15 01:58:06 2024
  write: IOPS=26.8k, BW=210MiB/s (220MB/s)(12.3GiB/60004msec); 0 zone resets
    slat (usec): min=4, max=1312.5k, avg=34.63, stdev=2337.61
    clat (usec): min=254, max=1316.1k, avg=4734.78, stdev=26280.00
     lat (usec): min=272, max=1316.2k, avg=4769.74, stdev=26382.78
    clat percentiles (usec):
     |  1.00th=[   1647],  5.00th=[   2073], 10.00th=[   2278],
     | 20.00th=[   2507], 30.00th=[   2638], 40.00th=[   2769],
     | 50.00th=[   2900], 60.00th=[   3032], 70.00th=[   3163],
     | 80.00th=[   3425], 90.00th=[   4178], 95.00th=[   5145],
     | 99.00th=[  49021], 99.50th=[  70779], 99.90th=[ 120062],
     | 99.95th=[ 164627], 99.99th=[1166017]
   bw (  KiB/s): min=27312, max=372128, per=100.00%, avg=228545.86, stdev=54463.56, samples=112
   iops        : min= 3414, max=46516, avg=28568.25, stdev=6807.97, samples=112
  lat (usec)   : 500=0.01%, 750=0.02%, 1000=0.02%
  lat (msec)   : 2=3.84%, 4=84.77%, 10=8.26%, 20=0.75%, 50=1.36%
  lat (msec)   : 100=0.75%, 250=0.17%, 2000=0.05%
  cpu          : usr=8.15%, sys=30.96%, ctx=94043, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1609584,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=210MiB/s (220MB/s), 210MiB/s-210MiB/s (220MB/s-220MB/s), io=12.3GiB (13.2GB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) write:  write: IOPS=26.8k, BW=210MiB/s (220MB/s)(12.3GiB/60004msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4944 MiB read, 0.000 read amp, 81607778 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=13063: Mon Jan 15 01:59:09 2024
  write: IOPS=13.4k, BW=209MiB/s (219MB/s)(12.2GiB/60023msec); 0 zone resets
    slat (usec): min=4, max=1330.0k, avg=71.90, stdev=2792.37
    clat (usec): min=287, max=1337.3k, avg=9496.77, stdev=32016.84
     lat (usec): min=314, max=1337.3k, avg=9569.07, stdev=32146.75
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    6], 50.00th=[    6], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    7], 90.00th=[   22], 95.00th=[   27],
     | 99.00th=[   58], 99.50th=[   74], 99.90th=[  464], 99.95th=[  869],
     | 99.99th=[ 1334]
   bw (  KiB/s): min= 4736, max=392384, per=100.00%, avg=226361.88, stdev=92701.21, samples=113
   iops        : min=  296, max=24524, avg=14147.61, stdev=5793.82, samples=113
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.05%, 4=0.13%, 10=86.96%, 20=2.30%, 50=9.26%
  lat (msec)   : 100=1.07%, 250=0.09%, 500=0.03%, 750=0.02%, 1000=0.06%
  lat (msec)   : 2000=0.02%
  cpu          : usr=6.43%, sys=16.86%, ctx=251595, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,802723,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=209MiB/s (219MB/s), 209MiB/s-209MiB/s (219MB/s-219MB/s), io=12.2GiB (13.2GB), run=60023-60023msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) write:  write: IOPS=13.4k, BW=209MiB/s (219MB/s)(12.2GiB/60023msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4943 MiB read, 0.000 read amp, 81607846 total
[0m

===Fio: workload=write, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=write, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4942 MiB read, 0.000 read amp, 81607914 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=13116: Mon Jan 15 02:00:12 2024
  write: IOPS=6659, BW=208MiB/s (218MB/s)(12.3GiB/60762msec); 0 zone resets
    slat (usec): min=5, max=1456.1k, avg=145.53, stdev=4841.80
    clat (usec): min=390, max=1469.0k, avg=19072.81, stdev=55923.93
     lat (usec): min=413, max=1469.0k, avg=19218.92, stdev=56146.73
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   10], 60.00th=[   11],
     | 70.00th=[   12], 80.00th=[   13], 90.00th=[   39], 95.00th=[   59],
     | 99.00th=[  123], 99.50th=[  144], 99.90th=[ 1284], 99.95th=[ 1435],
     | 99.99th=[ 1469]
   bw (  KiB/s): min= 3200, max=415360, per=100.00%, avg=225106.37, stdev=72098.95, samples=115
   iops        : min=  100, max=12980, avg=7034.57, stdev=2253.09, samples=115
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=0.09%, 4=0.19%, 10=51.32%, 20=32.73%, 50=8.91%
  lat (msec)   : 100=5.00%, 250=1.55%, 750=0.01%, 1000=0.01%, 2000=0.14%
  cpu          : usr=4.34%, sys=10.44%, ctx=215631, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,404616,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=208MiB/s (218MB/s), 208MiB/s-208MiB/s (218MB/s-218MB/s), io=12.3GiB (13.3GB), run=60762-60762msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) write:  write: IOPS=6659, BW=208MiB/s (218MB/s)(12.3GiB/60762msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=write, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 4940 MiB read, 0.000 read amp, 81608048 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=13170: Mon Jan 15 02:01:14 2024
  write: IOPS=3286, BW=205MiB/s (215MB/s)(12.1GiB/60222msec); 0 zone resets
    slat (usec): min=6, max=1620.1k, avg=292.56, stdev=7918.75
    clat (usec): min=554, max=1656.4k, avg=38647.68, stdev=88918.85
     lat (usec): min=615, max=1656.7k, avg=38941.07, stdev=89265.81
    clat percentiles (msec):
     |  1.00th=[    7],  5.00th=[   18], 10.00th=[   18], 20.00th=[   19],
     | 30.00th=[   20], 40.00th=[   21], 50.00th=[   22], 60.00th=[   24],
     | 70.00th=[   28], 80.00th=[   44], 90.00th=[   77], 95.00th=[  106],
     | 99.00th=[  157], 99.50th=[  201], 99.90th=[ 1620], 99.95th=[ 1653],
     | 99.99th=[ 1653]
   bw (  KiB/s): min=47104, max=414976, per=100.00%, avg=232274.14, stdev=61294.31, samples=109
   iops        : min=  736, max= 6484, avg=3629.28, stdev=957.72, samples=109
  lat (usec)   : 750=0.01%, 1000=0.03%
  lat (msec)   : 2=0.20%, 4=0.35%, 10=0.85%, 20=37.87%, 50=43.79%
  lat (msec)   : 100=11.41%, 250=5.17%, 2000=0.32%
  cpu          : usr=3.59%, sys=7.42%, ctx=166204, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,197928,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=205MiB/s (215MB/s), 205MiB/s-205MiB/s (215MB/s-215MB/s), io=12.1GiB (13.0GB), run=60222-60222msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) write:  write: IOPS=3286, BW=205MiB/s (215MB/s)(12.1GiB/60222msec); 0 zone resets
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.483266 s, 217 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 5bd3184b-444a-4804-8f9a-c084dfb79deb
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.014: File-server Version 3.0 personality successfully loaded
0.014: Populating and pre-allocating filesets
0.223: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.223: Removing bigfileset tree (if exists)
0.229: Pre-allocating directories in bigfileset tree
0.625: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4936 MiB read, 0.000 read amp, 81608256 total
[0m43.885: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
43.885: Population and pre-allocation of filesets completed
43.885: Starting 1 filereader instances
44.894: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608300 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608301 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608302 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608304 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608305 total
[0m164.907: Run took 120 seconds...
164.918: Per-Operation Breakdown
statfile1            448660ops     3738ops/s   0.0mb/s    0.012ms/op [0.003ms - 7.727ms]
deletefile1          448649ops     3738ops/s   0.0mb/s    0.272ms/op [0.029ms - 447.257ms]
closefile3           448665ops     3739ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.479ms]
readfile1            448666ops     3739ops/s 485.7mb/s    0.114ms/op [0.005ms - 449.731ms]
openfile2            448666ops     3739ops/s   0.0mb/s    0.054ms/op [0.006ms - 33.768ms]
closefile2           448666ops     3739ops/s   0.0mb/s    0.004ms/op [0.001ms - 1.894ms]
appendfilerand1      448667ops     3739ops/s  29.2mb/s    0.845ms/op [0.001ms - 441.266ms]
openfile1            448670ops     3739ops/s   0.0mb/s    0.060ms/op [0.007ms - 34.460ms]
closefile1           448671ops     3739ops/s   0.0mb/s    0.007ms/op [0.001ms - 7.766ms]
wrtfile1             448677ops     3739ops/s 467.2mb/s   11.161ms/op [0.013ms - 450.916ms]
createfile1          448710ops     3739ops/s   0.0mb/s    0.141ms/op [0.023ms - 444.649ms]
164.918: IO Summary: 4935367 ops 41124.267 ops/s 3739/7477 rd/wr 982.1mb/s 1.152ms/op
164.918: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:164.918: IO Summary: 4935367 ops 41124.267 ops/s 3739/7477 rd/wr 982.1mb/s 1.152ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.017: OLTP Version 3.0  personality successfully loaded
0.017: Populating and pre-allocating filesets
0.017: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.017: Removing logfile tree (if exists)
0.023: Pre-allocating directories in logfile tree
0.027: Pre-allocating files in logfile tree
0.154: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.154: Removing datafiles tree (if exists)
0.157: Pre-allocating directories in datafiles tree
0.160: Pre-allocating files in datafiles tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608307 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608309 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608310 total
[0m61.336: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
61.336: Population and pre-allocation of filesets completed
61.336: Starting 200 shadow instances
61.454: Starting 10 dbwr instances
61.459: Starting 1 lgwr instances
62.465: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4935 MiB read, 0.000 read amp, 81608313 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4934 MiB read, 0.000 read amp, 81608314 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4934 MiB read, 0.000 read amp, 81608315 total
[0m182.508: Run took 120 seconds...
182.526: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     2032000ops    16928ops/s   0.0mb/s   10.483ms/op [0.017ms - 1551.511ms]
shadow-post-lg       2032200ops    16929ops/s   0.0mb/s    0.057ms/op [0.002ms - 56.029ms]
shadowhog            2032200ops    16929ops/s   0.0mb/s    0.299ms/op [0.081ms - 87.769ms]
shadowread           2057800ops    17142ops/s  33.1mb/s    0.826ms/op [0.001ms - 1062.507ms]
dbwr-aiowait         20309ops      169ops/s   0.0mb/s    3.320ms/op [0.005ms - 45.822ms]
dbwr-block           20314ops      169ops/s   0.0mb/s    9.216ms/op [0.003ms - 433.821ms]
dbwr-hog             20319ops      169ops/s   0.0mb/s    0.015ms/op [0.004ms - 14.376ms]
dbwrite-a            2033180ops    16937ops/s  33.1mb/s    0.005ms/op [0.001ms - 66.221ms]
lg-block             635ops        5ops/s   0.0mb/s  188.074ms/op [34.830ms - 1953.962ms]
lg-aiowait           636ops        5ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.008ms]
lg-write             637ops        5ops/s   1.3mb/s    0.010ms/op [0.001ms - 0.127ms]
182.526: IO Summary: 4112562 ops 34259.594 ops/s 17142/16943 rd/wr  67.4mb/s 0.432ms/op
182.526: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:182.526: IO Summary: 4112562 ops 34259.594 ops/s 17142/16943 rd/wr  67.4mb/s 0.432ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.016: Varmail Version 3.0 personality successfully loaded
0.016: Populating and pre-allocating filesets
1.049: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
1.049: Removing bigfileset tree (if exists)
1.055: Pre-allocating directories in bigfileset tree
1.056: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4934 MiB read, 0.000 read amp, 81608318 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4934 MiB read, 0.000 read amp, 81608340 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4934 MiB read, 0.000 read amp, 81608342 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4934 MiB read, 0.000 read amp, 81608343 total
[0m77.841: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
77.841: Population and pre-allocation of filesets completed
77.842: Starting 1 filereader instances
78.847: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 159999/160000 hits, 4933 MiB read, 0.000 read amp, 81608345 total
[0m198.858: Run took 120 seconds...
198.859: Per-Operation Breakdown
closefile4           165466ops     1379ops/s   0.0mb/s    0.003ms/op [0.001ms - 0.427ms]
readfile4            165466ops     1379ops/s  36.6mb/s    0.964ms/op [0.007ms - 1455.449ms]
openfile4            165466ops     1379ops/s   0.0mb/s    0.021ms/op [0.005ms - 4.804ms]
closefile3           165466ops     1379ops/s   0.0mb/s    0.006ms/op [0.001ms - 4.677ms]
fsyncfile3           165466ops     1379ops/s   0.0mb/s    3.585ms/op [0.585ms - 1484.486ms]
appendfilerand3      165473ops     1379ops/s  10.8mb/s    0.097ms/op [0.007ms - 1016.397ms]
readfile3            165473ops     1379ops/s  36.7mb/s    0.973ms/op [0.008ms - 1457.867ms]
openfile3            165476ops     1379ops/s   0.0mb/s    0.021ms/op [0.005ms - 1.187ms]
closefile2           165476ops     1379ops/s   0.0mb/s    0.006ms/op [0.001ms - 1.184ms]
fsyncfile2           165476ops     1379ops/s   0.0mb/s    3.452ms/op [0.857ms - 1484.435ms]
appendfilerand2      165479ops     1379ops/s  10.8mb/s    0.332ms/op [0.015ms - 1457.804ms]
createfile2          165481ops     1379ops/s   0.0mb/s    1.006ms/op [0.035ms - 1458.047ms]
deletefile1          165482ops     1379ops/s   0.0mb/s    1.003ms/op [0.042ms - 1479.761ms]
198.859: IO Summary: 2151146 ops 17924.631 ops/s 2758/2758 rd/wr  94.9mb/s 0.882ms/op
198.859: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:198.859: IO Summary: 2151146 ops 17924.631 ops/s 2758/2758 rd/wr  94.9mb/s 0.882ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T00:44:49.lsvd-nvme.240.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T00:44:49.lsvd-nvme.240.triple-hdd.txt
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=53.1k, BW=208MiB/s (218MB/s)(12.2GiB/60001msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=69.4k, BW=271MiB/s (284MB/s)(15.9GiB/60001msec)
Fio (iodepth=1; bs=4ki) randread:  read: IOPS=6893, BW=26.9MiB/s (28.2MB/s)(1616MiB/60001msec)
Fio (iodepth=4; bs=4ki) randread:  read: IOPS=21.7k, BW=84.9MiB/s (89.0MB/s)(5091MiB/60001msec)
Fio (iodepth=8; bs=4ki) randread:  read: IOPS=32.0k, BW=125MiB/s (131MB/s)(7491MiB/60001msec)
Fio (iodepth=16; bs=4ki) randread:  read: IOPS=47.2k, BW=184MiB/s (193MB/s)(10.8GiB/60001msec)
Fio (iodepth=32; bs=4ki) randread:  read: IOPS=64.0k, BW=250MiB/s (262MB/s)(14.6GiB/60001msec)
Fio (iodepth=64; bs=4ki) randread:  read: IOPS=64.4k, BW=252MiB/s (264MB/s)(14.7GiB/60001msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=64.2k, BW=251MiB/s (263MB/s)(14.7GiB/60001msec)
Fio (iodepth=256; bs=4ki) randread:  read: IOPS=66.1k, BW=258MiB/s (271MB/s)(15.1GiB/60001msec)
Fio (iodepth=512; bs=4ki) randread:  read: IOPS=65.1k, BW=254MiB/s (266MB/s)(14.9GiB/60002msec)
Fio (iodepth=32; bs=4ki) read:  read: IOPS=76.7k, BW=299MiB/s (314MB/s)(17.5GiB/60001msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=80.5k, BW=314MiB/s (330MB/s)(18.4GiB/60001msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=63.8k, BW=249MiB/s (261MB/s)(14.6GiB/60001msec)
Fio (iodepth=128; bs=8ki) randread:  read: IOPS=53.6k, BW=419MiB/s (439MB/s)(24.6GiB/60002msec)
Fio (iodepth=128; bs=16ki) randread:  read: IOPS=45.1k, BW=705MiB/s (740MB/s)(41.3GiB/60003msec)
Fio (iodepth=128; bs=32ki) randread:  read: IOPS=29.8k, BW=931MiB/s (976MB/s)(54.5GiB/60004msec)
Fio (iodepth=128; bs=64ki) randread:  read: IOPS=17.4k, BW=1087MiB/s (1140MB/s)(63.7GiB/60008msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=84.0k, BW=328MiB/s (344MB/s)(19.2GiB/60002msec)
Fio (iodepth=128; bs=8ki) read:  read: IOPS=65.9k, BW=515MiB/s (540MB/s)(30.2GiB/60001msec)
Fio (iodepth=128; bs=16ki) read:  read: IOPS=50.1k, BW=782MiB/s (820MB/s)(45.8GiB/60003msec)
Fio (iodepth=128; bs=32ki) read:  read: IOPS=31.8k, BW=995MiB/s (1043MB/s)(58.3GiB/60004msec)
Fio (iodepth=128; bs=64ki) read:  read: IOPS=16.9k, BW=1057MiB/s (1108MB/s)(61.9GiB/60008msec)
Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5969, BW=23.3MiB/s (24.5MB/s)(1399MiB/60001msec); 0 zone resets
Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=17.7k, BW=69.3MiB/s (72.7MB/s)(4158MiB/60001msec); 0 zone resets
Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=27.7k, BW=108MiB/s (114MB/s)(6500MiB/60001msec); 0 zone resets
Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=38.9k, BW=152MiB/s (159MB/s)(9111MiB/60001msec); 0 zone resets
Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=47.6k, BW=186MiB/s (195MB/s)(10.9GiB/60001msec); 0 zone resets
Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60001msec); 0 zone resets
Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=52.0k, BW=203MiB/s (213MB/s)(11.9GiB/60004msec); 0 zone resets
Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=52.4k, BW=205MiB/s (215MB/s)(12.0GiB/60001msec); 0 zone resets
Fio (iodepth=32; bs=4ki) write:  write: IOPS=45.1k, BW=176MiB/s (185MB/s)(10.3GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=44.7k, BW=174MiB/s (183MB/s)(10.2GiB/60002msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=43.2k, BW=169MiB/s (177MB/s)(9.90GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=23.9k, BW=187MiB/s (196MB/s)(10.9GiB/60003msec); 0 zone resets
Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=15.0k, BW=235MiB/s (246MB/s)(13.8GiB/60007msec); 0 zone resets
Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=6663, BW=208MiB/s (218MB/s)(12.2GiB/60010msec); 0 zone resets
Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=3292, BW=206MiB/s (216MB/s)(12.1GiB/60089msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=48.5k, BW=189MiB/s (199MB/s)(11.1GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=8ki) write:  write: IOPS=26.8k, BW=210MiB/s (220MB/s)(12.3GiB/60004msec); 0 zone resets
Fio (iodepth=128; bs=16ki) write:  write: IOPS=13.4k, BW=209MiB/s (219MB/s)(12.2GiB/60023msec); 0 zone resets
Fio (iodepth=128; bs=32ki) write:  write: IOPS=6659, BW=208MiB/s (218MB/s)(12.3GiB/60762msec); 0 zone resets
Fio (iodepth=128; bs=64ki) write:  write: IOPS=3286, BW=205MiB/s (215MB/s)(12.1GiB/60222msec); 0 zone resets
Filebench /tmp/filebench/fileserver.f:164.918: IO Summary: 4935367 ops 41124.267 ops/s 3739/7477 rd/wr 982.1mb/s 1.152ms/op
Filebench /tmp/filebench/oltp.f:182.526: IO Summary: 4112562 ops 34259.594 ops/s 17142/16943 rd/wr  67.4mb/s 0.432ms/op
Filebench /tmp/filebench/varmail.f:198.859: IO Summary: 2151146 ops 17924.631 ops/s 2758/2758 rd/wr  94.9mb/s 0.882ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7faa8cfe9640) exiting
+ ulimit -c
unlimited
+ '[' -z rssd2 ']'
+ pool_name=rssd2
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-15T02:11:10
+ default_cache_size=21474836480
+ cache_size=21474836480
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=20
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T02:11:10.lsvd-nvme.20.rssd2.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC objects.cc
CC translate.cc
CC io.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC rados_backend.cc
CC lsvd_debug.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-15T02:11:10
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thick rssd2 lsvd-benchmark 80g
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py rssd2 lsvd-benchmark
Removing all objects from pool rssd2 with prefix lsvd-benchmark
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 22240/65186 objects
+ ./thick-image --size=80g rssd2/lsvd-benchmark
Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 45% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 80% complete...Thick provisioning: 81% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 87% complete...Thick provisioning: 88% complete...Thick provisioning: 89% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 92% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p rssd2 stat lsvd-benchmark
rssd2/lsvd-benchmark mtime 2024-01-15T02:15:05.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/ 21474836480
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/
+ local cache_size=21474836480
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=21474836480
+ LSVD_CACHE_SIZE=21474836480
+ rm -rf /mnt/nvme-remote//lsvd-write/af309aba-4712-4d06-8879-5cbe1576fb36.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-15 02:15:34.018003] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-15 02:15:34.018206] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid908882 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-15 02:15:34.121540] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-15 02:15:34.265814] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-15 02:15:34.265877] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-15 02:15:34.265930] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-15 02:15:34.265932] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-15 02:15:39.748514] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-15 02:15:40.479359] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img rssd2 lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=rssd2
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create rssd2 lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 20480 MiB in 16 shards, 1280 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//6a40ae64-1aa1-4bcf-a5ed-7d945ee86c0b.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-15 02:16:16.340048] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 1/2 hits, 0 MiB read, 1.778 read amp, 2 total
[0m+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T02:11:10.lsvd-nvme.20.rssd2.txt client-bench.bash read_entire_img=1
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T02:11:10.lsvd-nvme.20.rssd2.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=1
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=1'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T02:11:10.lsvd-nvme.20.rssd2.txt
+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 1 '!=' read_entire_img=1 ']'
+ eval read_entire_img=1
++ read_entire_img=1
===Starting client benchmark

+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=1


===Reading entire image to warm cache===

+ [[ 1 -eq 1 ]]
+ printf '\n\n===Reading entire image to warm cache===\n\n'
+ dd if=/dev/nvme1n1 of=/dev/null bs=1048576 count=81910
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 10516/31936 hits, 1328 MiB read, 1.007 read amp, 31936 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 27870/84826 hits, 3532 MiB read, 1.008 read amp, 84826 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 45355/138112 hits, 5752 MiB read, 1.008 read amp, 138112 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 190924 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 242788 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 293944 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 346456 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 399568 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 453214 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 506272 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 560122 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 613180 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 667108 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 720142 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52506/160000 hits, 6666 MiB read, 1.008 read amp, 772990 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 826534 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 879802 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 932638 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 986392 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1040176 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1093498 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52506/160000 hits, 6666 MiB read, 1.008 read amp, 1146652 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1200250 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1253326 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1307092 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1360996 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1413520 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1466830 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 1519960 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1573102 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1626202 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1679752 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52507/160000 hits, 6666 MiB read, 1.008 read amp, 1733062 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1786096 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1840084 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1892326 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1944142 total
[0m81910+0 records in
81910+0 records out
85888860160 bytes (86 GB, 80 GiB) copied, 746.98 s, 115 MB/s
+ set +x


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 36436/160000 hits, 2057 MiB read, 3.753 read amp, 2087231 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 40541/160000 hits, 587 MiB read, 12.701 read amp, 2598722 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42731/160000 hits, 588 MiB read, 12.451 read amp, 3101833 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=14993: Mon Jan 15 02:29:51 2024
  read: IOPS=24.6k, BW=96.2MiB/s (101MB/s)(5774MiB/60007msec)
    slat (usec): min=4, max=21298, avg=37.25, stdev=81.83
    clat (usec): min=93, max=74860, avg=5156.80, stdev=5692.55
     lat (usec): min=112, max=74865, avg=5194.39, stdev=5693.99
    clat percentiles (usec):
     |  1.00th=[  212],  5.00th=[  351], 10.00th=[  510], 20.00th=[ 1418],
     | 30.00th=[ 2073], 40.00th=[ 2606], 50.00th=[ 3261], 60.00th=[ 4113],
     | 70.00th=[ 5342], 80.00th=[ 7570], 90.00th=[12387], 95.00th=[17171],
     | 99.00th=[27657], 99.50th=[31851], 99.90th=[39584], 99.95th=[43254],
     | 99.99th=[53216]
   bw (  KiB/s): min=64520, max=128224, per=99.91%, avg=98439.33, stdev=14234.52, samples=119
   iops        : min=16130, max=32056, avg=24609.85, stdev=3558.61, samples=119
  lat (usec)   : 100=0.01%, 250=1.85%, 500=7.93%, 750=4.95%, 1000=2.66%
  lat (msec)   : 2=11.20%, 4=30.24%, 10=27.08%, 20=10.87%, 50=3.22%
  lat (msec)   : 100=0.02%
  cpu          : usr=9.38%, sys=25.61%, ctx=415272, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1478039,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=96.2MiB/s (101MB/s), 96.2MiB/s-96.2MiB/s (101MB/s-101MB/s), io=5774MiB (6054MB), run=60007-60007msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=24.6k, BW=96.2MiB/s (101MB/s)(5774MiB/60007msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 123561/160000 hits, 588 MiB read, 3.873 read amp, 3653609 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152762/160000 hits, 588 MiB read, 0.769 read amp, 4480349 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152439/160000 hits, 588 MiB read, 0.803 read amp, 5284313 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15040: Mon Jan 15 02:30:53 2024
  read: IOPS=38.0k, BW=148MiB/s (156MB/s)(8899MiB/60004msec)
    slat (usec): min=4, max=10273, avg=23.99, stdev=79.55
    clat (usec): min=95, max=18456, avg=3345.51, stdev=1689.79
     lat (usec): min=111, max=18462, avg=3369.80, stdev=1694.57
    clat percentiles (usec):
     |  1.00th=[  212],  5.00th=[  281], 10.00th=[  355], 20.00th=[  816],
     | 30.00th=[ 3359], 40.00th=[ 3818], 50.00th=[ 4015], 60.00th=[ 4178],
     | 70.00th=[ 4359], 80.00th=[ 4490], 90.00th=[ 4752], 95.00th=[ 4948],
     | 99.00th=[ 5604], 99.50th=[ 7832], 99.90th=[10159], 99.95th=[10683],
     | 99.99th=[14877]
   bw (  KiB/s): min=132608, max=173592, per=100.00%, avg=152006.79, stdev=7086.65, samples=119
   iops        : min=33152, max=43398, avg=38001.71, stdev=1771.65, samples=119
  lat (usec)   : 100=0.01%, 250=2.78%, 500=13.11%, 750=3.77%, 1000=0.90%
  lat (msec)   : 2=2.06%, 4=26.29%, 10=50.95%, 20=0.13%
  cpu          : usr=9.79%, sys=33.64%, ctx=190476, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=2278110,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=148MiB/s (156MB/s), 148MiB/s-148MiB/s (156MB/s-156MB/s), io=8899MiB (9331MB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=38.0k, BW=148MiB/s (156MB/s)(8899MiB/60004msec)


===Fio: workload=randread, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152002/160000 hits, 588 MiB read, 0.850 read amp, 5957068 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 142210/160000 hits, 588 MiB read, 1.890 read amp, 5971514 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 132263/160000 hits, 588 MiB read, 2.947 read amp, 5986021 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15086: Mon Jan 15 02:31:56 2024
  read: IOPS=681, BW=2727KiB/s (2793kB/s)(160MiB/60001msec)
    slat (usec): min=7, max=455, avg=20.54, stdev= 8.87
    clat (usec): min=102, max=11864, avg=1438.15, stdev=792.40
     lat (usec): min=111, max=11884, avg=1459.19, stdev=792.58
    clat percentiles (usec):
     |  1.00th=[  119],  5.00th=[  139], 10.00th=[  178], 20.00th=[  202],
     | 30.00th=[ 1516], 40.00th=[ 1647], 50.00th=[ 1745], 60.00th=[ 1811],
     | 70.00th=[ 1909], 80.00th=[ 1991], 90.00th=[ 2147], 95.00th=[ 2311],
     | 99.00th=[ 2737], 99.50th=[ 2835], 99.90th=[ 3752], 99.95th=[ 4146],
     | 99.99th=[10290]
   bw (  KiB/s): min= 2408, max= 3048, per=100.00%, avg=2728.87, stdev=114.89, samples=119
   iops        : min=  602, max=  762, avg=682.22, stdev=28.72, samples=119
  lat (usec)   : 250=24.51%, 500=1.29%, 750=0.48%, 1000=0.01%
  lat (msec)   : 2=54.34%, 4=19.30%, 10=0.05%, 20=0.01%
  cpu          : usr=1.25%, sys=2.85%, ctx=40914, majf=0, minf=11
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=40912,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=2727KiB/s (2793kB/s), 2727KiB/s-2727KiB/s (2793kB/s-2793kB/s), io=160MiB (168MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randread:  read: IOPS=681, BW=2727KiB/s (2793kB/s)(160MiB/60001msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 122549/160000 hits, 588 MiB read, 3.979 read amp, 6000334 total
[0m

===Fio: workload=randread, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 88544/160000 hits, 588 MiB read, 7.589 read amp, 6096641 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 73056/160000 hits, 588 MiB read, 9.232 read amp, 6162483 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42853/160000 hits, 588 MiB read, 12.438 read amp, 6228609 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15133: Mon Jan 15 02:32:58 2024
  read: IOPS=3686, BW=14.4MiB/s (15.1MB/s)(864MiB/60002msec)
    slat (usec): min=4, max=461, avg=12.56, stdev= 8.39
    clat (usec): min=85, max=18920, avg=1068.53, stdev=763.73
     lat (usec): min=93, max=18941, avg=1081.42, stdev=764.34
    clat percentiles (usec):
     |  1.00th=[  111],  5.00th=[  124], 10.00th=[  137], 20.00th=[  163],
     | 30.00th=[  192], 40.00th=[ 1139], 50.00th=[ 1418], 60.00th=[ 1532],
     | 70.00th=[ 1631], 80.00th=[ 1729], 90.00th=[ 1860], 95.00th=[ 2024],
     | 99.00th=[ 2442], 99.50th=[ 2573], 99.90th=[ 2933], 99.95th=[ 3425],
     | 99.99th=[ 8979]
   bw (  KiB/s): min=11752, max=90544, per=100.00%, avg=14775.73, stdev=12878.42, samples=119
   iops        : min= 2938, max=22636, avg=3693.93, stdev=3219.61, samples=119
  lat (usec)   : 100=0.10%, 250=37.01%, 500=2.22%, 750=0.39%, 1000=0.03%
  lat (msec)   : 2=54.91%, 4=5.31%, 10=0.02%, 20=0.01%
  cpu          : usr=3.27%, sys=7.54%, ctx=159588, majf=0, minf=14
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=221186,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
   READ: bw=14.4MiB/s (15.1MB/s), 14.4MiB/s-14.4MiB/s (15.1MB/s-15.1MB/s), io=864MiB (906MB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randread:  read: IOPS=3686, BW=14.4MiB/s (15.1MB/s)(864MiB/60002msec)


===Fio: workload=randread, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 113713/160000 hits, 588 MiB read, 4.913 read amp, 6533209 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42724/160000 hits, 588 MiB read, 12.458 read amp, 6667456 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42922/160000 hits, 588 MiB read, 12.435 read amp, 6803646 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15180: Mon Jan 15 02:34:01 2024
  read: IOPS=9390, BW=36.7MiB/s (38.5MB/s)(2201MiB/60002msec)
    slat (usec): min=4, max=553, avg=10.94, stdev= 7.01
    clat (usec): min=77, max=13627, avg=837.96, stdev=713.31
     lat (usec): min=102, max=13637, avg=849.18, stdev=713.52
    clat percentiles (usec):
     |  1.00th=[  117],  5.00th=[  145], 10.00th=[  169], 20.00th=[  204],
     | 30.00th=[  227], 40.00th=[  249], 50.00th=[  297], 60.00th=[ 1287],
     | 70.00th=[ 1434], 80.00th=[ 1582], 90.00th=[ 1745], 95.00th=[ 1909],
     | 99.00th=[ 2311], 99.50th=[ 2474], 99.90th=[ 2868], 99.95th=[ 3163],
     | 99.99th=[ 9372]
   bw (  KiB/s): min=24384, max=133392, per=100.00%, avg=37684.64, stdev=33068.22, samples=119
   iops        : min= 6096, max=33348, avg=9421.16, stdev=8267.05, samples=119
  lat (usec)   : 100=0.01%, 250=40.17%, 500=14.07%, 750=0.57%, 1000=0.14%
  lat (msec)   : 2=41.47%, 4=3.55%, 10=0.02%, 20=0.01%
  cpu          : usr=5.52%, sys=13.87%, ctx=322702, majf=0, minf=18
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=563476,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
   READ: bw=36.7MiB/s (38.5MB/s), 36.7MiB/s-36.7MiB/s (38.5MB/s-38.5MB/s), io=2201MiB (2308MB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randread:  read: IOPS=9390, BW=36.7MiB/s (38.5MB/s)(2201MiB/60002msec)


===Fio: workload=randread, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 53386/160000 hits, 588 MiB read, 11.314 read amp, 7028824 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 61893/160000 hits, 588 MiB read, 10.418 read amp, 7303651 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 45738/160000 hits, 587 MiB read, 12.145 read amp, 7562247 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15226: Mon Jan 15 02:35:03 2024
  read: IOPS=12.7k, BW=49.6MiB/s (52.0MB/s)(2975MiB/60002msec)
    slat (usec): min=4, max=457, avg=10.50, stdev= 6.91
    clat (usec): min=64, max=14144, avg=1247.22, stdev=796.63
     lat (usec): min=98, max=14163, avg=1257.98, stdev=796.52
    clat percentiles (usec):
     |  1.00th=[  118],  5.00th=[  147], 10.00th=[  178], 20.00th=[  239],
     | 30.00th=[  375], 40.00th=[ 1369], 50.00th=[ 1500], 60.00th=[ 1614],
     | 70.00th=[ 1729], 80.00th=[ 1860], 90.00th=[ 2089], 95.00th=[ 2343],
     | 99.00th=[ 2900], 99.50th=[ 3130], 99.90th=[ 3818], 99.95th=[ 4359],
     | 99.99th=[10552]
   bw (  KiB/s): min=43288, max=121480, per=100.00%, avg=50848.94, stdev=8807.81, samples=119
   iops        : min=10822, max=30370, avg=12712.27, stdev=2201.94, samples=119
  lat (usec)   : 100=0.01%, 250=21.39%, 500=11.59%, 750=0.98%, 1000=0.10%
  lat (msec)   : 2=52.65%, 4=13.19%, 10=0.06%, 20=0.01%
  cpu          : usr=6.86%, sys=17.74%, ctx=497530, majf=0, minf=26
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=761582,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=49.6MiB/s (52.0MB/s), 49.6MiB/s-49.6MiB/s (52.0MB/s-52.0MB/s), io=2975MiB (3119MB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randread:  read: IOPS=12.7k, BW=49.6MiB/s (52.0MB/s)(2975MiB/60002msec)


===Fio: workload=randread, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 60242/160000 hits, 588 MiB read, 10.592 read amp, 7869878 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42773/160000 hits, 588 MiB read, 12.455 read amp, 8247658 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42987/160000 hits, 587 MiB read, 12.444 read amp, 8617296 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15275: Mon Jan 15 02:36:05 2024
  read: IOPS=18.0k, BW=70.2MiB/s (73.6MB/s)(4212MiB/60005msec)
    slat (usec): min=4, max=471, avg= 9.43, stdev= 5.91
    clat (usec): min=63, max=15887, avg=1768.76, stdev=1118.34
     lat (usec): min=102, max=15907, avg=1778.43, stdev=1118.34
    clat percentiles (usec):
     |  1.00th=[  157],  5.00th=[  227], 10.00th=[  293], 20.00th=[  465],
     | 30.00th=[ 1467], 40.00th=[ 1696], 50.00th=[ 1844], 60.00th=[ 1991],
     | 70.00th=[ 2180], 80.00th=[ 2442], 90.00th=[ 2966], 95.00th=[ 3589],
     | 99.00th=[ 5276], 99.50th=[ 6063], 99.90th=[ 7963], 99.95th=[ 8979],
     | 99.99th=[11076]
   bw (  KiB/s): min=54840, max=83424, per=100.00%, avg=71949.85, stdev=5052.74, samples=119
   iops        : min=13710, max=20856, avg=17987.48, stdev=1263.20, samples=119
  lat (usec)   : 100=0.01%, 250=6.82%, 500=14.61%, 750=4.74%, 1000=1.05%
  lat (msec)   : 2=32.87%, 4=36.65%, 10=3.24%, 20=0.03%
  cpu          : usr=8.97%, sys=20.55%, ctx=534788, majf=0, minf=42
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=1078245,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=70.2MiB/s (73.6MB/s), 70.2MiB/s-70.2MiB/s (73.6MB/s-73.6MB/s), io=4212MiB (4416MB), run=60005-60005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randread:  read: IOPS=18.0k, BW=70.2MiB/s (73.6MB/s)(4212MiB/60005msec)


===Fio: workload=randread, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42660/160000 hits, 588 MiB read, 12.452 read amp, 8983334 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42157/160000 hits, 587 MiB read, 12.528 read amp, 9425015 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42119/160000 hits, 588 MiB read, 12.518 read amp, 9882224 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15322: Mon Jan 15 02:37:08 2024
  read: IOPS=21.3k, BW=83.2MiB/s (87.3MB/s)(4994MiB/60004msec)
    slat (usec): min=4, max=464, avg= 8.77, stdev= 5.93
    clat (usec): min=91, max=37704, avg=2992.68, stdev=2491.88
     lat (usec): min=104, max=37713, avg=3001.70, stdev=2491.94
    clat percentiles (usec):
     |  1.00th=[  208],  5.00th=[  343], 10.00th=[  494], 20.00th=[ 1004],
     | 30.00th=[ 1795], 40.00th=[ 2180], 50.00th=[ 2507], 60.00th=[ 2900],
     | 70.00th=[ 3359], 80.00th=[ 4113], 90.00th=[ 5800], 95.00th=[ 7898],
     | 99.00th=[12518], 99.50th=[14484], 99.90th=[19006], 99.95th=[20841],
     | 99.99th=[24773]
   bw (  KiB/s): min=62024, max=103272, per=99.98%, avg=85207.53, stdev=7776.24, samples=119
   iops        : min=15506, max=25818, avg=21301.92, stdev=1944.04, samples=119
  lat (usec)   : 100=0.01%, 250=2.06%, 500=8.17%, 750=5.93%, 1000=3.79%
  lat (msec)   : 2=15.37%, 4=43.62%, 10=18.56%, 20=2.42%, 50=0.07%
  cpu          : usr=9.11%, sys=22.29%, ctx=478817, majf=0, minf=74
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=1278393,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=83.2MiB/s (87.3MB/s), 83.2MiB/s-83.2MiB/s (87.3MB/s-87.3MB/s), io=4994MiB (5236MB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randread:  read: IOPS=21.3k, BW=83.2MiB/s (87.3MB/s)(4994MiB/60004msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42499/160000 hits, 588 MiB read, 12.477 read amp, 10307646 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42923/160000 hits, 587 MiB read, 12.445 read amp, 10790196 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42916/160000 hits, 588 MiB read, 12.434 read amp, 11310039 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15369: Mon Jan 15 02:38:10 2024
  read: IOPS=23.5k, BW=91.8MiB/s (96.3MB/s)(5510MiB/60013msec)
    slat (usec): min=4, max=87772, avg=38.88, stdev=127.94
    clat (usec): min=90, max=225790, avg=5404.66, stdev=6843.45
     lat (usec): min=111, max=225819, avg=5443.91, stdev=6845.66
    clat percentiles (usec):
     |  1.00th=[   196],  5.00th=[   314], 10.00th=[   441], 20.00th=[   971],
     | 30.00th=[  1860], 40.00th=[  2311], 50.00th=[  2868], 60.00th=[  3720],
     | 70.00th=[  5342], 80.00th=[  8586], 90.00th=[ 14091], 95.00th=[ 19006],
     | 99.00th=[ 30278], 99.50th=[ 34866], 99.90th=[ 43254], 99.95th=[ 46924],
     | 99.99th=[183501]
   bw (  KiB/s): min=62288, max=125904, per=100.00%, avg=94204.03, stdev=13701.61, samples=119
   iops        : min=15572, max=31476, avg=23551.04, stdev=3425.35, samples=119
  lat (usec)   : 100=0.01%, 250=2.53%, 500=9.30%, 750=5.34%, 1000=3.07%
  lat (msec)   : 2=12.73%, 4=29.36%, 10=20.71%, 20=12.62%, 50=4.30%
  lat (msec)   : 100=0.02%, 250=0.02%
  cpu          : usr=8.94%, sys=25.84%, ctx=438163, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1410454,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=91.8MiB/s (96.3MB/s), 91.8MiB/s-91.8MiB/s (96.3MB/s-96.3MB/s), io=5510MiB (5777MB), run=60013-60013msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=23.5k, BW=91.8MiB/s (96.3MB/s)(5510MiB/60013msec)


===Fio: workload=randread, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42787/160000 hits, 588 MiB read, 12.456 read amp, 11740752 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42784/160000 hits, 588 MiB read, 12.453 read amp, 12248608 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42601/160000 hits, 588 MiB read, 12.466 read amp, 12781391 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15415: Mon Jan 15 02:39:12 2024
  read: IOPS=24.8k, BW=96.7MiB/s (101MB/s)(5802MiB/60013msec)
    slat (usec): min=4, max=18298, avg=37.69, stdev=96.48
    clat (usec): min=116, max=83529, avg=10302.35, stdev=6094.36
     lat (usec): min=130, max=83586, avg=10340.36, stdev=6096.32
    clat percentiles (usec):
     |  1.00th=[ 2573],  5.00th=[ 4752], 10.00th=[ 5407], 20.00th=[ 6390],
     | 30.00th=[ 7242], 40.00th=[ 7898], 50.00th=[ 8586], 60.00th=[ 9241],
     | 70.00th=[10421], 80.00th=[12649], 90.00th=[17957], 95.00th=[23200],
     | 99.00th=[34341], 99.50th=[38011], 99.90th=[46924], 99.95th=[51643],
     | 99.99th=[62653]
   bw (  KiB/s): min=72064, max=128816, per=100.00%, avg=99069.98, stdev=11452.44, samples=119
   iops        : min=18016, max=32204, avg=24767.55, stdev=2863.10, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.03%
  lat (msec)   : 2=0.50%, 4=2.03%, 10=64.53%, 20=25.14%, 50=7.71%
  lat (msec)   : 100=0.06%
  cpu          : usr=8.55%, sys=23.93%, ctx=373247, majf=0, minf=267
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1485383,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=96.7MiB/s (101MB/s), 96.7MiB/s-96.7MiB/s (101MB/s-101MB/s), io=5802MiB (6084MB), run=60013-60013msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randread:  read: IOPS=24.8k, BW=96.7MiB/s (101MB/s)(5802MiB/60013msec)


===Fio: workload=randread, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42617/160000 hits, 587 MiB read, 12.488 read amp, 13255586 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42758/160000 hits, 588 MiB read, 12.454 read amp, 13780661 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42676/160000 hits, 588 MiB read, 12.458 read amp, 14310791 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=15461: Mon Jan 15 02:40:15 2024
  read: IOPS=24.8k, BW=96.9MiB/s (102MB/s)(5817MiB/60011msec)
    slat (usec): min=4, max=18675, avg=37.66, stdev=89.30
    clat (usec): min=6759, max=99225, avg=20590.39, stdev=7014.65
     lat (usec): min=6780, max=99323, avg=20628.37, stdev=7017.67
    clat percentiles (usec):
     |  1.00th=[12125],  5.00th=[13566], 10.00th=[14615], 20.00th=[15926],
     | 30.00th=[16909], 40.00th=[17695], 50.00th=[18744], 60.00th=[19792],
     | 70.00th=[21365], 80.00th=[23987], 90.00th=[29230], 95.00th=[34866],
     | 99.00th=[47973], 99.50th=[52691], 99.90th=[63177], 99.95th=[67634],
     | 99.99th=[78119]
   bw (  KiB/s): min=78584, max=124624, per=100.00%, avg=99389.65, stdev=10616.82, samples=119
   iops        : min=19646, max=31156, avg=24847.43, stdev=2654.21, samples=119
  lat (msec)   : 10=0.07%, 20=61.87%, 50=37.31%, 100=0.75%
  cpu          : usr=8.93%, sys=23.16%, ctx=410918, majf=0, minf=522
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1489101,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
   READ: bw=96.9MiB/s (102MB/s), 96.9MiB/s-96.9MiB/s (102MB/s-102MB/s), io=5817MiB (6099MB), run=60011-60011msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randread:  read: IOPS=24.8k, BW=96.9MiB/s (102MB/s)(5817MiB/60011msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42573/160000 hits, 587 MiB read, 12.489 read amp, 14805087 total
[0m

===Fio: workload=read, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152928/160000 hits, 588 MiB read, 0.751 read amp, 15304704 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152765/160000 hits, 588 MiB read, 0.769 read amp, 15816551 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152551/160000 hits, 588 MiB read, 0.791 read amp, 16327214 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16020: Mon Jan 15 02:41:17 2024
  read: IOPS=24.3k, BW=94.8MiB/s (99.5MB/s)(5691MiB/60001msec)
    slat (usec): min=4, max=505, avg= 7.72, stdev= 7.49
    clat (usec): min=103, max=13386, avg=1308.33, stdev=589.10
     lat (usec): min=112, max=13422, avg=1316.30, stdev=589.17
    clat percentiles (usec):
     |  1.00th=[  198],  5.00th=[  269], 10.00th=[  318], 20.00th=[  529],
     | 30.00th=[ 1254], 40.00th=[ 1385], 50.00th=[ 1467], 60.00th=[ 1549],
     | 70.00th=[ 1631], 80.00th=[ 1729], 90.00th=[ 1876], 95.00th=[ 2024],
     | 99.00th=[ 2409], 99.50th=[ 2606], 99.90th=[ 3425], 99.95th=[ 4424],
     | 99.99th=[ 6718]
   bw (  KiB/s): min=83216, max=108952, per=100.00%, avg=97175.73, stdev=4439.57, samples=119
   iops        : min=20804, max=27238, avg=24293.98, stdev=1109.87, samples=119
  lat (usec)   : 250=3.61%, 500=15.65%, 750=3.56%, 1000=1.13%
  lat (msec)   : 2=70.41%, 4=5.58%, 10=0.05%, 20=0.01%
  cpu          : usr=6.75%, sys=20.70%, ctx=124822, majf=0, minf=43
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=1456879,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=94.8MiB/s (99.5MB/s), 94.8MiB/s-94.8MiB/s (99.5MB/s-99.5MB/s), io=5691MiB (5967MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) read:  read: IOPS=24.3k, BW=94.8MiB/s (99.5MB/s)(5691MiB/60001msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 17804031 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152198/160000 hits, 588 MiB read, 0.829 read amp, 18623418 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 151826/160000 hits, 588 MiB read, 0.868 read amp, 19354623 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16067: Mon Jan 15 02:42:20 2024
  read: IOPS=49.0k, BW=192MiB/s (201MB/s)(11.2GiB/60005msec)
    slat (usec): min=3, max=10131, avg=18.19, stdev=71.78
    clat (usec): min=96, max=19123, avg=2590.03, stdev=1609.41
     lat (usec): min=117, max=19128, avg=2608.49, stdev=1617.47
    clat percentiles (usec):
     |  1.00th=[  241],  5.00th=[  392], 10.00th=[ 1090], 20.00th=[ 1401],
     | 30.00th=[ 1516], 40.00th=[ 1598], 50.00th=[ 1745], 60.00th=[ 2671],
     | 70.00th=[ 3949], 80.00th=[ 4293], 90.00th=[ 4555], 95.00th=[ 4817],
     | 99.00th=[ 7570], 99.50th=[ 8586], 99.90th=[10159], 99.95th=[12256],
     | 99.99th=[14746]
   bw (  KiB/s): min=127344, max=344208, per=100.00%, avg=196735.32, stdev=87074.50, samples=119
   iops        : min=31836, max=86052, avg=49183.92, stdev=21768.62, samples=119
  lat (usec)   : 100=0.01%, 250=1.24%, 500=5.23%, 750=1.40%, 1000=0.91%
  lat (msec)   : 2=48.03%, 4=14.16%, 10=28.91%, 20=0.12%
  cpu          : usr=12.23%, sys=39.75%, ctx=168040, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=2942734,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=192MiB/s (201MB/s), 192MiB/s-192MiB/s (201MB/s-201MB/s), io=11.2GiB (12.1GB), run=60005-60005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=49.0k, BW=192MiB/s (201MB/s)(11.2GiB/60005msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42559/160000 hits, 588 MiB read, 12.474 read amp, 19854544 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 43075/160000 hits, 588 MiB read, 12.425 read amp, 20395714 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42938/160000 hits, 588 MiB read, 12.433 read amp, 20918163 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16113: Mon Jan 15 02:43:22 2024
  read: IOPS=25.0k, BW=97.7MiB/s (102MB/s)(5862MiB/60007msec)
    slat (usec): min=4, max=11699, avg=36.60, stdev=78.53
    clat (usec): min=90, max=177388, avg=5078.61, stdev=5607.08
     lat (usec): min=104, max=177478, avg=5115.55, stdev=5608.03
    clat percentiles (usec):
     |  1.00th=[  221],  5.00th=[  343], 10.00th=[  490], 20.00th=[ 1156],
     | 30.00th=[ 2008], 40.00th=[ 2540], 50.00th=[ 3097], 60.00th=[ 3916],
     | 70.00th=[ 5276], 80.00th=[ 8029], 90.00th=[12649], 95.00th=[16581],
     | 99.00th=[25297], 99.50th=[29230], 99.90th=[38011], 99.95th=[41681],
     | 99.99th=[82314]
   bw (  KiB/s): min=78184, max=124736, per=99.97%, avg=100015.39, stdev=9666.77, samples=119
   iops        : min=19546, max=31184, avg=25003.87, stdev=2416.69, samples=119
  lat (usec)   : 100=0.01%, 250=1.73%, 500=8.55%, 750=5.00%, 1000=3.26%
  lat (msec)   : 2=11.31%, 4=30.96%, 10=23.98%, 20=12.60%, 50=2.58%
  lat (msec)   : 100=0.01%, 250=0.01%
  cpu          : usr=9.72%, sys=26.02%, ctx=394720, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1500797,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=97.7MiB/s (102MB/s), 97.7MiB/s-97.7MiB/s (102MB/s-102MB/s), io=5862MiB (6147MB), run=60007-60007msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=25.0k, BW=97.7MiB/s (102MB/s)(5862MiB/60007msec)


===Fio: workload=randread, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42416/160000 hits, 1111 MiB read, 6.615 read amp, 21444461 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42897/160000 hits, 1111 MiB read, 6.586 read amp, 22034400 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42614/160000 hits, 1110 MiB read, 6.606 read amp, 22620204 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16159: Mon Jan 15 02:44:24 2024
  read: IOPS=26.2k, BW=205MiB/s (215MB/s)(12.0GiB/60012msec)
    slat (usec): min=4, max=21444, avg=35.09, stdev=103.21
    clat (usec): min=108, max=55589, avg=4841.66, stdev=4244.16
     lat (usec): min=150, max=55595, avg=4877.09, stdev=4245.38
    clat percentiles (usec):
     |  1.00th=[  334],  5.00th=[  611], 10.00th=[  963], 20.00th=[ 1942],
     | 30.00th=[ 2671], 40.00th=[ 3326], 50.00th=[ 3982], 60.00th=[ 4686],
     | 70.00th=[ 5407], 80.00th=[ 6456], 90.00th=[ 8848], 95.00th=[12780],
     | 99.00th=[22676], 99.50th=[26608], 99.90th=[34866], 99.95th=[38011],
     | 99.99th=[44303]
   bw (  KiB/s): min=153296, max=247808, per=100.00%, avg=210289.34, stdev=19401.33, samples=119
   iops        : min=19162, max=30976, avg=26286.18, stdev=2425.16, samples=119
  lat (usec)   : 250=0.30%, 500=3.01%, 750=3.79%, 1000=3.35%
  lat (msec)   : 2=10.30%, 4=29.45%, 10=41.75%, 20=6.51%, 50=1.54%
  lat (msec)   : 100=0.01%
  cpu          : usr=8.76%, sys=27.00%, ctx=302656, majf=0, minf=267
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1574306,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=205MiB/s (215MB/s), 205MiB/s-205MiB/s (215MB/s-215MB/s), io=12.0GiB (12.9GB), run=60012-60012msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randread:  read: IOPS=26.2k, BW=205MiB/s (215MB/s)(12.0GiB/60012msec)


===Fio: workload=randread, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 41947/160000 hits, 2002 MiB read, 3.685 read amp, 23120912 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 41048/160000 hits, 1996 MiB read, 3.723 read amp, 23703599 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 40646/160000 hits, 1998 MiB read, 3.732 read amp, 24295870 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16205: Mon Jan 15 02:45:27 2024
  read: IOPS=23.2k, BW=362MiB/s (380MB/s)(21.2GiB/60006msec)
    slat (usec): min=4, max=23100, avg=39.95, stdev=95.15
    clat (usec): min=123, max=57628, avg=5475.28, stdev=4760.94
     lat (usec): min=153, max=57638, avg=5515.60, stdev=4762.22
    clat percentiles (usec):
     |  1.00th=[  424],  5.00th=[  676], 10.00th=[  963], 20.00th=[ 2180],
     | 30.00th=[ 2900], 40.00th=[ 3589], 50.00th=[ 4359], 60.00th=[ 5145],
     | 70.00th=[ 6194], 80.00th=[ 7635], 90.00th=[10945], 95.00th=[14746],
     | 99.00th=[23987], 99.50th=[28443], 99.90th=[38011], 99.95th=[41681],
     | 99.99th=[47973]
   bw (  KiB/s): min=267680, max=434208, per=100.00%, avg=371259.43, stdev=36308.73, samples=119
   iops        : min=16730, max=27138, avg=23203.73, stdev=2269.27, samples=119
  lat (usec)   : 250=0.09%, 500=1.82%, 750=4.50%, 1000=4.15%
  lat (msec)   : 2=7.70%, 4=27.34%, 10=42.45%, 20=10.00%, 50=1.95%
  lat (msec)   : 100=0.01%
  cpu          : usr=9.28%, sys=28.09%, ctx=451255, majf=0, minf=523
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1391944,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=362MiB/s (380MB/s), 362MiB/s-362MiB/s (380MB/s-380MB/s), io=21.2GiB (22.8GB), run=60006-60006msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randread:  read: IOPS=23.2k, BW=362MiB/s (380MB/s)(21.2GiB/60006msec)


===Fio: workload=randread, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 40408/160000 hits, 3326 MiB read, 2.247 read amp, 24787956 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 37725/160000 hits, 3325 MiB read, 2.298 read amp, 25271483 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 36286/160000 hits, 3334 MiB read, 2.319 read amp, 25802210 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16257: Mon Jan 15 02:46:29 2024
  read: IOPS=17.2k, BW=537MiB/s (563MB/s)(31.5GiB/60006msec)
    slat (usec): min=5, max=14357, avg=53.43, stdev=80.89
    clat (usec): min=160, max=98702, avg=7391.12, stdev=5920.70
     lat (usec): min=208, max=98824, avg=7445.10, stdev=5922.69
    clat percentiles (usec):
     |  1.00th=[  562],  5.00th=[ 1467], 10.00th=[ 2311], 20.00th=[ 3359],
     | 30.00th=[ 4424], 40.00th=[ 5473], 50.00th=[ 6325], 60.00th=[ 7177],
     | 70.00th=[ 8029], 80.00th=[ 9241], 90.00th=[13173], 95.00th=[18482],
     | 99.00th=[32375], 99.50th=[38536], 99.90th=[50070], 99.95th=[54264],
     | 99.99th=[62653]
   bw (  KiB/s): min=409472, max=647360, per=100.00%, avg=550586.79, stdev=60988.45, samples=119
   iops        : min=12796, max=20230, avg=17205.85, stdev=1905.92, samples=119
  lat (usec)   : 250=0.02%, 500=0.68%, 750=1.32%, 1000=1.25%
  lat (msec)   : 2=4.43%, 4=18.52%, 10=57.11%, 20=12.52%, 50=4.05%
  lat (msec)   : 100=0.10%
  cpu          : usr=10.28%, sys=28.78%, ctx=650273, majf=0, minf=1035
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1031144,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=537MiB/s (563MB/s), 537MiB/s-537MiB/s (563MB/s-563MB/s), io=31.5GiB (33.8GB), run=60006-60006msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randread:  read: IOPS=17.2k, BW=537MiB/s (563MB/s)(31.5GiB/60006msec)


===Fio: workload=randread, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 35981/160000 hits, 4685 MiB read, 1.654 read amp, 26262163 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 30558/160000 hits, 5000 MiB read, 1.618 read amp, 26736849 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 28197/160000 hits, 5001 MiB read, 1.647 read amp, 27213307 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16304: Mon Jan 15 02:47:31 2024
  read: IOPS=12.0k, BW=750MiB/s (787MB/s)(44.0GiB/60032msec)
    slat (usec): min=6, max=64997, avg=79.26, stdev=115.06
    clat (usec): min=243, max=235425, avg=10577.62, stdev=7611.99
     lat (usec): min=324, max=235463, avg=10657.36, stdev=7616.36
    clat percentiles (usec):
     |  1.00th=[  1467],  5.00th=[  2769], 10.00th=[  3490], 20.00th=[  4883],
     | 30.00th=[  6456], 40.00th=[  7898], 50.00th=[  9110], 60.00th=[ 10552],
     | 70.00th=[ 12387], 80.00th=[ 14091], 90.00th=[ 18482], 95.00th=[ 24511],
     | 99.00th=[ 38536], 99.50th=[ 45351], 99.90th=[ 59507], 99.95th=[ 64750],
     | 99.99th=[158335]
   bw (  KiB/s): min=477184, max=935168, per=100.00%, avg=769501.58, stdev=114343.63, samples=119
   iops        : min= 7456, max=14612, avg=12023.46, stdev=1786.62, samples=119
  lat (usec)   : 250=0.01%, 500=0.10%, 750=0.30%, 1000=0.26%
  lat (msec)   : 2=1.13%, 4=12.00%, 10=42.60%, 20=35.26%, 50=8.06%
  lat (msec)   : 100=0.28%, 250=0.02%
  cpu          : usr=7.28%, sys=22.01%, ctx=712956, majf=0, minf=2058
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=720779,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=750MiB/s (787MB/s), 750MiB/s-750MiB/s (787MB/s-787MB/s), io=44.0GiB (47.2GB), run=60032-60032msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randread:  read: IOPS=12.0k, BW=750MiB/s (787MB/s)(44.0GiB/60032msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 110383/160000 hits, 2117 MiB read, 1.465 read amp, 27678435 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152782/160000 hits, 588 MiB read, 0.767 read amp, 28508443 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152408/160000 hits, 588 MiB read, 0.807 read amp, 29290392 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16350: Mon Jan 15 02:48:34 2024
  read: IOPS=37.4k, BW=146MiB/s (153MB/s)(8774MiB/60004msec)
    slat (usec): min=4, max=16848, avg=24.38, stdev=84.35
    clat (usec): min=103, max=22006, avg=3393.29, stdev=1750.09
     lat (usec): min=114, max=22020, avg=3417.97, stdev=1756.41
    clat percentiles (usec):
     |  1.00th=[  227],  5.00th=[  310], 10.00th=[  412], 20.00th=[  955],
     | 30.00th=[ 3228], 40.00th=[ 3785], 50.00th=[ 4047], 60.00th=[ 4228],
     | 70.00th=[ 4359], 80.00th=[ 4555], 90.00th=[ 4752], 95.00th=[ 5014],
     | 99.00th=[ 8717], 99.50th=[ 9372], 99.90th=[10290], 99.95th=[10683],
     | 99.99th=[16712]
   bw (  KiB/s): min=118928, max=175104, per=100.00%, avg=150049.14, stdev=9727.27, samples=119
   iops        : min=29732, max=43776, avg=37512.38, stdev=2431.80, samples=119
  lat (usec)   : 250=1.82%, 500=11.62%, 750=5.01%, 1000=1.75%
  lat (msec)   : 2=2.63%, 4=24.88%, 10=52.14%, 20=0.14%, 50=0.01%
  cpu          : usr=9.44%, sys=32.65%, ctx=177703, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=2246031,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=146MiB/s (153MB/s), 146MiB/s-146MiB/s (153MB/s-153MB/s), io=8774MiB (9200MB), run=60004-60004msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=37.4k, BW=146MiB/s (153MB/s)(8774MiB/60004msec)


===Fio: workload=read, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152433/160000 hits, 607 MiB read, 0.778 read amp, 29966322 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 158918/160000 hits, 1111 MiB read, 0.061 read amp, 31235488 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 144634/160000 hits, 1111 MiB read, 0.864 read amp, 31657364 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16396: Mon Jan 15 02:49:36 2024
  read: IOPS=31.1k, BW=243MiB/s (255MB/s)(14.2GiB/60008msec)
    slat (usec): min=4, max=9641, avg=29.51, stdev=102.83
    clat (usec): min=96, max=21236, avg=4083.69, stdev=3096.44
     lat (usec): min=114, max=21242, avg=4113.52, stdev=3114.47
    clat percentiles (usec):
     |  1.00th=[  235],  5.00th=[  971], 10.00th=[ 1434], 20.00th=[ 1795],
     | 30.00th=[ 2089], 40.00th=[ 2376], 50.00th=[ 2573], 60.00th=[ 2769],
     | 70.00th=[ 5473], 80.00th=[ 8225], 90.00th=[ 8979], 95.00th=[ 9372],
     | 99.00th=[10421], 99.50th=[10945], 99.90th=[16909], 99.95th=[18220],
     | 99.99th=[19792]
   bw (  KiB/s): min=133808, max=574768, per=100.00%, avg=249832.74, stdev=150311.69, samples=119
   iops        : min=16726, max=71846, avg=31229.11, stdev=18789.00, samples=119
  lat (usec)   : 100=0.01%, 250=1.30%, 500=2.76%, 750=0.80%, 1000=0.16%
  lat (msec)   : 2=22.37%, 4=41.04%, 10=29.77%, 20=1.82%, 50=0.01%
  cpu          : usr=8.78%, sys=29.56%, ctx=162282, majf=0, minf=267
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1866407,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=243MiB/s (255MB/s), 243MiB/s-243MiB/s (255MB/s-255MB/s), io=14.2GiB (15.3GB), run=60008-60008msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) read:  read: IOPS=31.1k, BW=243MiB/s (255MB/s)(14.2GiB/60008msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 143899/160000 hits, 1111 MiB read, 0.906 read amp, 32060053 total
[0m

===Fio: workload=read, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 33171264 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 129784/160000 hits, 2000 MiB read, 0.944 read amp, 33468579 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 128215/160000 hits, 2000 MiB read, 0.993 read amp, 33704040 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16442: Mon Jan 15 02:50:39 2024
  read: IOPS=22.3k, BW=348MiB/s (365MB/s)(20.4GiB/60015msec)
    slat (usec): min=4, max=16263, avg=42.01, stdev=112.39
    clat (usec): min=104, max=43416, avg=5706.88, stdev=5813.31
     lat (usec): min=137, max=43470, avg=5749.25, stdev=5851.34
    clat percentiles (usec):
     |  1.00th=[  502],  5.00th=[ 2212], 10.00th=[ 2278], 20.00th=[ 2376],
     | 30.00th=[ 2442], 40.00th=[ 2507], 50.00th=[ 2606], 60.00th=[ 2704],
     | 70.00th=[ 2999], 80.00th=[11731], 90.00th=[16909], 95.00th=[17957],
     | 99.00th=[19268], 99.50th=[20055], 99.90th=[24511], 99.95th=[26608],
     | 99.99th=[34341]
   bw (  KiB/s): min=135360, max=842784, per=100.00%, avg=357951.73, stdev=300988.43, samples=119
   iops        : min= 8460, max=52674, avg=22371.98, stdev=18811.77, samples=119
  lat (usec)   : 250=0.43%, 500=0.56%, 750=0.13%, 1000=0.03%
  lat (msec)   : 2=0.83%, 4=71.37%, 10=5.21%, 20=20.97%, 50=0.47%
  cpu          : usr=8.14%, sys=26.16%, ctx=332716, majf=0, minf=523
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1335671,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=348MiB/s (365MB/s), 348MiB/s-348MiB/s (365MB/s-365MB/s), io=20.4GiB (21.9GB), run=60015-60015msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) read:  read: IOPS=22.3k, BW=348MiB/s (365MB/s)(20.4GiB/60015msec)


===Fio: workload=read, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 146392/160000 hits, 3333 MiB read, 0.255 read amp, 34323432 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 106450/160000 hits, 3333 MiB read, 1.004 read amp, 34524393 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 136292/160000 hits, 3333 MiB read, 0.445 read amp, 34800462 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16489: Mon Jan 15 02:51:41 2024
  read: IOPS=12.4k, BW=388MiB/s (406MB/s)(22.7GiB/60032msec)
    slat (usec): min=5, max=9746, avg=76.94, stdev=123.82
    clat (usec): min=137, max=66582, avg=10241.97, stdev=9846.56
     lat (usec): min=169, max=66632, avg=10319.35, stdev=9891.09
    clat percentiles (usec):
     |  1.00th=[ 1713],  5.00th=[ 3523], 10.00th=[ 3720], 20.00th=[ 3916],
     | 30.00th=[ 4047], 40.00th=[ 4228], 50.00th=[ 4490], 60.00th=[ 5473],
     | 70.00th=[10552], 80.00th=[19006], 90.00th=[27657], 95.00th=[32375],
     | 99.00th=[36963], 99.50th=[38536], 99.90th=[40109], 99.95th=[41157],
     | 99.99th=[49546]
   bw (  KiB/s): min=186368, max=1022464, per=100.00%, avg=398697.95, stdev=315865.13, samples=119
   iops        : min= 5824, max=31952, avg=12459.29, stdev=9870.76, samples=119
  lat (usec)   : 250=0.12%, 500=0.32%, 750=0.13%, 1000=0.05%
  lat (msec)   : 2=0.78%, 4=25.29%, 10=42.61%, 20=11.69%, 50=18.99%
  lat (msec)   : 100=0.01%
  cpu          : usr=6.59%, sys=18.49%, ctx=501909, majf=0, minf=1036
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=744402,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=388MiB/s (406MB/s), 388MiB/s-388MiB/s (406MB/s-406MB/s), io=22.7GiB (24.4GB), run=60032-60032msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) read:  read: IOPS=12.4k, BW=388MiB/s (406MB/s)(22.7GiB/60032msec)


===Fio: workload=read, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 93004/160000 hits, 4939 MiB read, 0.848 read amp, 35000462 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 83564/160000 hits, 5000 MiB read, 0.955 read amp, 35207312 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 121936/160000 hits, 5000 MiB read, 0.476 read amp, 35461882 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16535: Mon Jan 15 02:52:43 2024
  read: IOPS=5690, BW=356MiB/s (373MB/s)(20.9GiB/60043msec)
    slat (usec): min=6, max=6345, avg=170.43, stdev=162.51
    clat (usec): min=200, max=98315, avg=22317.85, stdev=14915.34
     lat (usec): min=211, max=99153, avg=22488.87, stdev=14945.05
    clat percentiles (usec):
     |  1.00th=[  412],  5.00th=[ 3032], 10.00th=[ 5407], 20.00th=[ 7242],
     | 30.00th=[10028], 40.00th=[15270], 50.00th=[20579], 60.00th=[25822],
     | 70.00th=[31327], 80.00th=[36439], 90.00th=[42730], 95.00th=[46924],
     | 99.00th=[60556], 99.50th=[68682], 99.90th=[77071], 99.95th=[82314],
     | 99.99th=[96994]
   bw (  KiB/s): min=258432, max=1153792, per=100.00%, avg=364316.80, stdev=152345.46, samples=120
   iops        : min= 4038, max=18028, avg=5692.45, stdev=2380.40, samples=120
  lat (usec)   : 250=0.02%, 500=1.75%, 750=1.07%, 1000=0.18%
  lat (msec)   : 2=0.24%, 4=3.84%, 10=22.76%, 20=18.95%, 50=48.31%
  lat (msec)   : 100=2.89%
  cpu          : usr=4.53%, sys=12.80%, ctx=322893, majf=0, minf=2059
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=341674,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=356MiB/s (373MB/s), 356MiB/s-356MiB/s (373MB/s-373MB/s), io=20.9GiB (22.4GB), run=60043-60043msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) read:  read: IOPS=5690, BW=356MiB/s (373MB/s)(20.9GiB/60043msec)


===Fio: workload=randwrite, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118766/160000 hits, 4999 MiB read, 0.516 read amp, 35529660 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16585: Mon Jan 15 02:53:46 2024
  write: IOPS=5814, BW=22.7MiB/s (23.8MB/s)(1363MiB/60001msec); 0 zone resets
    slat (usec): min=7, max=459, avg=14.40, stdev= 7.77
    clat (nsec): min=682, max=2414.1k, avg=154019.51, stdev=66438.46
     lat (usec): min=117, max=2429, avg=168.72, stdev=67.70
    clat percentiles (usec):
     |  1.00th=[  116],  5.00th=[  120], 10.00th=[  123], 20.00th=[  129],
     | 30.00th=[  135], 40.00th=[  137], 50.00th=[  141], 60.00th=[  145],
     | 70.00th=[  149], 80.00th=[  155], 90.00th=[  169], 95.00th=[  265],
     | 99.00th=[  429], 99.50th=[  482], 99.90th=[  660], 99.95th=[ 1319],
     | 99.99th=[ 1876]
   bw (  KiB/s): min=20976, max=26848, per=100.00%, avg=23269.92, stdev=1283.73, samples=119
   iops        : min= 5244, max= 6712, avg=5817.45, stdev=320.93, samples=119
  lat (nsec)   : 750=0.01%
  lat (usec)   : 2=0.01%, 50=0.01%, 100=0.01%, 250=94.68%, 500=4.92%
  lat (usec)   : 750=0.33%, 1000=0.01%
  lat (msec)   : 2=0.06%, 4=0.01%
  cpu          : usr=4.17%, sys=12.84%, ctx=348852, majf=0, minf=12
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,348851,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=22.7MiB/s (23.8MB/s), 22.7MiB/s-22.7MiB/s (23.8MB/s-23.8MB/s), io=1363MiB (1429MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5814, BW=22.7MiB/s (23.8MB/s)(1363MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118767/160000 hits, 4996 MiB read, 0.516 read amp, 35529796 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16640: Mon Jan 15 02:54:48 2024
  write: IOPS=18.1k, BW=70.8MiB/s (74.2MB/s)(4247MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=482, avg= 9.60, stdev= 6.09
    clat (usec): min=31, max=7026, avg=208.44, stdev=98.18
     lat (usec): min=108, max=7036, avg=218.28, stdev=97.99
    clat percentiles (usec):
     |  1.00th=[  125],  5.00th=[  137], 10.00th=[  147], 20.00th=[  161],
     | 30.00th=[  172], 40.00th=[  180], 50.00th=[  190], 60.00th=[  200],
     | 70.00th=[  210], 80.00th=[  231], 90.00th=[  289], 95.00th=[  343],
     | 99.00th=[  494], 99.50th=[  594], 99.90th=[ 1598], 99.95th=[ 1827],
     | 99.99th=[ 2343]
   bw (  KiB/s): min=66272, max=78008, per=100.00%, avg=72473.08, stdev=2617.60, samples=119
   iops        : min=16568, max=19502, avg=18118.29, stdev=654.41, samples=119
  lat (usec)   : 50=0.01%, 100=0.01%, 250=84.79%, 500=14.26%, 750=0.65%
  lat (usec)   : 1000=0.04%
  lat (msec)   : 2=0.22%, 4=0.03%, 10=0.01%
  cpu          : usr=9.20%, sys=22.29%, ctx=449189, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1087151,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
  WRITE: bw=70.8MiB/s (74.2MB/s), 70.8MiB/s-70.8MiB/s (74.2MB/s-74.2MB/s), io=4247MiB (4453MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=18.1k, BW=70.8MiB/s (74.2MB/s)(4247MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118772/160000 hits, 4994 MiB read, 0.516 read amp, 35529948 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16693: Mon Jan 15 02:55:50 2024
  write: IOPS=28.1k, BW=110MiB/s (115MB/s)(6584MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=540, avg= 9.52, stdev= 6.23
    clat (usec): min=60, max=4110, avg=272.62, stdev=112.61
     lat (usec): min=112, max=4119, avg=282.41, stdev=112.48
    clat percentiles (usec):
     |  1.00th=[  137],  5.00th=[  167], 10.00th=[  186], 20.00th=[  210],
     | 30.00th=[  227], 40.00th=[  243], 50.00th=[  258], 60.00th=[  273],
     | 70.00th=[  289], 80.00th=[  318], 90.00th=[  367], 95.00th=[  408],
     | 99.00th=[  570], 99.50th=[  758], 99.90th=[ 1696], 99.95th=[ 1942],
     | 99.99th=[ 2409]
   bw (  KiB/s): min=101272, max=123912, per=100.00%, avg=112528.94, stdev=4164.50, samples=119
   iops        : min=25318, max=30978, avg=28132.25, stdev=1041.15, samples=119
  lat (usec)   : 100=0.01%, 250=45.45%, 500=52.99%, 750=1.05%, 1000=0.07%
  lat (msec)   : 2=0.40%, 4=0.04%, 10=0.01%
  cpu          : usr=12.64%, sys=32.06%, ctx=451686, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1685519,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
  WRITE: bw=110MiB/s (115MB/s), 110MiB/s-110MiB/s (115MB/s-115MB/s), io=6584MiB (6904MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=28.1k, BW=110MiB/s (115MB/s)(6584MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118776/160000 hits, 4991 MiB read, 0.516 read amp, 35530108 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16750: Mon Jan 15 02:56:53 2024
  write: IOPS=40.8k, BW=159MiB/s (167MB/s)(9553MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=461, avg= 8.30, stdev= 5.54
    clat (usec): min=103, max=7188, avg=382.09, stdev=161.21
     lat (usec): min=117, max=7194, avg=390.62, stdev=161.13
    clat percentiles (usec):
     |  1.00th=[  186],  5.00th=[  227], 10.00th=[  249], 20.00th=[  285],
     | 30.00th=[  314], 40.00th=[  338], 50.00th=[  363], 60.00th=[  388],
     | 70.00th=[  416], 80.00th=[  449], 90.00th=[  502], 95.00th=[  562],
     | 99.00th=[  873], 99.50th=[ 1434], 99.90th=[ 2212], 99.95th=[ 2409],
     | 99.99th=[ 2704]
   bw (  KiB/s): min=145368, max=179200, per=100.00%, avg=163152.00, stdev=6984.22, samples=119
   iops        : min=36342, max=44800, avg=40788.03, stdev=1746.04, samples=119
  lat (usec)   : 250=10.36%, 500=79.22%, 750=9.02%, 1000=0.50%
  lat (msec)   : 2=0.74%, 4=0.17%, 10=0.01%
  cpu          : usr=14.57%, sys=35.81%, ctx=313591, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2445519,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
  WRITE: bw=159MiB/s (167MB/s), 159MiB/s-159MiB/s (167MB/s-167MB/s), io=9553MiB (10.0GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=40.8k, BW=159MiB/s (167MB/s)(9553MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118781/160000 hits, 4988 MiB read, 0.516 read amp, 35530278 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16804: Mon Jan 15 02:57:55 2024
  write: IOPS=53.3k, BW=208MiB/s (218MB/s)(12.2GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=564, avg= 8.56, stdev= 6.25
    clat (usec): min=204, max=9381, avg=589.22, stdev=225.40
     lat (usec): min=211, max=9391, avg=598.03, stdev=225.27
    clat percentiles (usec):
     |  1.00th=[  302],  5.00th=[  375], 10.00th=[  412], 20.00th=[  457],
     | 30.00th=[  490], 40.00th=[  523], 50.00th=[  553], 60.00th=[  586],
     | 70.00th=[  627], 80.00th=[  676], 90.00th=[  758], 95.00th=[  848],
     | 99.00th=[ 1614], 99.50th=[ 1811], 99.90th=[ 2638], 99.95th=[ 2868],
     | 99.99th=[ 4228]
   bw (  KiB/s): min=166384, max=243256, per=100.00%, avg=213523.09, stdev=15258.68, samples=119
   iops        : min=41596, max=60814, avg=53380.76, stdev=3814.68, samples=119
  lat (usec)   : 250=0.13%, 500=33.23%, 750=56.06%, 1000=7.65%
  lat (msec)   : 2=2.58%, 4=0.34%, 10=0.01%
  cpu          : usr=17.95%, sys=45.66%, ctx=211868, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,3200223,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=208MiB/s (218MB/s), 208MiB/s-208MiB/s (218MB/s-218MB/s), io=12.2GiB (13.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=53.3k, BW=208MiB/s (218MB/s)(12.2GiB/60001msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118784/160000 hits, 4986 MiB read, 0.517 read amp, 35530372 total
[0m

===Fio: workload=randwrite, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118785/160000 hits, 4984 MiB read, 0.517 read amp, 35530466 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16857: Mon Jan 15 02:58:57 2024
  write: IOPS=55.3k, BW=216MiB/s (226MB/s)(12.7GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=479, avg= 8.06, stdev= 6.29
    clat (usec): min=371, max=17565, avg=1147.62, stdev=367.95
     lat (usec): min=392, max=17571, avg=1155.91, stdev=367.81
    clat percentiles (usec):
     |  1.00th=[  537],  5.00th=[  701], 10.00th=[  799], 20.00th=[  889],
     | 30.00th=[  971], 40.00th=[ 1045], 50.00th=[ 1106], 60.00th=[ 1172],
     | 70.00th=[ 1237], 80.00th=[ 1336], 90.00th=[ 1500], 95.00th=[ 1680],
     | 99.00th=[ 2442], 99.50th=[ 2737], 99.90th=[ 3818], 99.95th=[ 4621],
     | 99.99th=[ 5932]
   bw (  KiB/s): min=165784, max=253328, per=99.96%, avg=221030.05, stdev=15320.57, samples=119
   iops        : min=41446, max=63332, avg=55257.58, stdev=3830.18, samples=119
  lat (usec)   : 500=0.52%, 750=6.47%, 1000=26.68%
  lat (msec)   : 2=63.57%, 4=2.68%, 10=0.07%, 20=0.01%
  cpu          : usr=16.70%, sys=44.77%, ctx=193161, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,3316986,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=216MiB/s (226MB/s), 216MiB/s-216MiB/s (226MB/s-226MB/s), io=12.7GiB (13.6GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=55.3k, BW=216MiB/s (226MB/s)(12.7GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118793/160000 hits, 4980 MiB read, 0.517 read amp, 35530654 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16911: Mon Jan 15 03:00:00 2024
  write: IOPS=56.2k, BW=220MiB/s (230MB/s)(12.9GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=15359, avg=15.46, stdev=45.65
    clat (usec): min=205, max=17682, avg=2258.11, stdev=495.90
     lat (usec): min=272, max=17689, avg=2273.82, stdev=497.03
    clat percentiles (usec):
     |  1.00th=[ 1450],  5.00th=[ 1663], 10.00th=[ 1778], 20.00th=[ 1926],
     | 30.00th=[ 2040], 40.00th=[ 2114], 50.00th=[ 2212], 60.00th=[ 2278],
     | 70.00th=[ 2376], 80.00th=[ 2474], 90.00th=[ 2737], 95.00th=[ 3130],
     | 99.00th=[ 4047], 99.50th=[ 4555], 99.90th=[ 5800], 99.95th=[ 6325],
     | 99.99th=[ 7701]
   bw (  KiB/s): min=205272, max=258592, per=99.98%, avg=224954.08, stdev=12553.79, samples=119
   iops        : min=51318, max=64648, avg=56238.50, stdev=3138.46, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.07%
  lat (msec)   : 2=26.11%, 4=72.76%, 10=1.05%, 20=0.01%
  cpu          : usr=17.27%, sys=44.53%, ctx=185520, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3374983,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=220MiB/s (230MB/s), 220MiB/s-220MiB/s (230MB/s-230MB/s), io=12.9GiB (13.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=56.2k, BW=220MiB/s (230MB/s)(12.9GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118796/160000 hits, 4976 MiB read, 0.517 read amp, 35530842 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=16967: Mon Jan 15 03:01:02 2024
  write: IOPS=56.0k, BW=219MiB/s (229MB/s)(12.8GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=17086, avg=15.53, stdev=49.02
    clat (usec): min=628, max=25421, avg=4554.95, stdev=737.70
     lat (usec): min=788, max=25427, avg=4570.73, stdev=738.97
    clat percentiles (usec):
     |  1.00th=[ 3490],  5.00th=[ 3752], 10.00th=[ 3916], 20.00th=[ 4146],
     | 30.00th=[ 4293], 40.00th=[ 4359], 50.00th=[ 4424], 60.00th=[ 4490],
     | 70.00th=[ 4621], 80.00th=[ 4817], 90.00th=[ 5276], 95.00th=[ 5735],
     | 99.00th=[ 7046], 99.50th=[ 7832], 99.90th=[ 9765], 99.95th=[10814],
     | 99.99th=[23725]
   bw (  KiB/s): min=182152, max=258704, per=100.00%, avg=224064.34, stdev=10890.80, samples=119
   iops        : min=45538, max=64676, avg=56016.10, stdev=2722.70, samples=119
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=12.57%, 10=87.34%, 20=0.06%, 50=0.02%
  cpu          : usr=17.14%, sys=44.78%, ctx=183205, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3359169,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
  WRITE: bw=219MiB/s (229MB/s), 219MiB/s-219MiB/s (229MB/s-229MB/s), io=12.8GiB (13.8GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=56.0k, BW=219MiB/s (229MB/s)(12.8GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118800/160000 hits, 4972 MiB read, 0.518 read amp, 35531030 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17054: Mon Jan 15 03:02:05 2024
  write: IOPS=56.4k, BW=220MiB/s (231MB/s)(12.9GiB/60002msec); 0 zone resets
    slat (usec): min=4, max=16049, avg=15.38, stdev=45.27
    clat (usec): min=434, max=29146, avg=9056.17, stdev=1021.20
     lat (usec): min=2663, max=29159, avg=9071.80, stdev=1022.28
    clat percentiles (usec):
     |  1.00th=[ 7308],  5.00th=[ 7570], 10.00th=[ 7767], 20.00th=[ 8455],
     | 30.00th=[ 8717], 40.00th=[ 8848], 50.00th=[ 8979], 60.00th=[ 9110],
     | 70.00th=[ 9372], 80.00th=[ 9765], 90.00th=[10159], 95.00th=[10683],
     | 99.00th=[12256], 99.50th=[13042], 99.90th=[15008], 99.95th=[16057],
     | 99.99th=[27657]
   bw (  KiB/s): min=189552, max=259016, per=99.96%, avg=225604.24, stdev=13368.99, samples=119
   iops        : min=47388, max=64754, avg=56401.11, stdev=3342.27, samples=119
  lat (usec)   : 500=0.01%
  lat (msec)   : 4=0.01%, 10=86.73%, 20=13.22%, 50=0.05%
  cpu          : usr=16.85%, sys=45.88%, ctx=183808, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3385472,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
  WRITE: bw=220MiB/s (231MB/s), 220MiB/s-220MiB/s (231MB/s-231MB/s), io=12.9GiB (13.9GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=56.4k, BW=220MiB/s (231MB/s)(12.9GiB/60002msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118802/160000 hits, 4968 MiB read, 0.518 read amp, 35531218 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17107: Mon Jan 15 03:03:07 2024
  write: IOPS=47.0k, BW=184MiB/s (192MB/s)(10.8GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=482, avg= 8.35, stdev= 5.85
    clat (usec): min=196, max=429251, avg=670.74, stdev=5464.21
     lat (usec): min=203, max=429287, avg=679.33, stdev=5464.19
    clat percentiles (usec):
     |  1.00th=[   302],  5.00th=[   367], 10.00th=[   396], 20.00th=[   441],
     | 30.00th=[   474], 40.00th=[   506], 50.00th=[   537], 60.00th=[   570],
     | 70.00th=[   611], 80.00th=[   668], 90.00th=[   742], 95.00th=[   807],
     | 99.00th=[  1012], 99.50th=[  1188], 99.90th=[  4228], 99.95th=[ 22676],
     | 99.99th=[316670]
   bw (  KiB/s): min=26648, max=264816, per=99.95%, avg=187883.03, stdev=60611.64, samples=119
   iops        : min= 6662, max=66204, avg=46970.79, stdev=15152.93, samples=119
  lat (usec)   : 250=0.20%, 500=38.49%, 750=52.41%, 1000=7.82%
  lat (msec)   : 2=0.92%, 4=0.06%, 10=0.04%, 20=0.01%, 50=0.01%
  lat (msec)   : 100=0.01%, 250=0.02%, 500=0.02%
  cpu          : usr=14.73%, sys=38.64%, ctx=197002, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2819792,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=184MiB/s (192MB/s), 184MiB/s-184MiB/s (192MB/s-192MB/s), io=10.8GiB (11.5GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) write:  write: IOPS=47.0k, BW=184MiB/s (192MB/s)(10.8GiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118807/160000 hits, 4966 MiB read, 0.518 read amp, 35531354 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17160: Mon Jan 15 03:04:09 2024
  write: IOPS=49.2k, BW=192MiB/s (202MB/s)(11.3GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=405104, avg=18.21, stdev=889.96
    clat (usec): min=345, max=407743, avg=2579.56, stdev=10060.90
     lat (usec): min=373, max=407763, avg=2598.02, stdev=10100.68
    clat percentiles (usec):
     |  1.00th=[  1401],  5.00th=[  1614], 10.00th=[  1729], 20.00th=[  1876],
     | 30.00th=[  1991], 40.00th=[  2089], 50.00th=[  2180], 60.00th=[  2278],
     | 70.00th=[  2376], 80.00th=[  2474], 90.00th=[  2671], 95.00th=[  2868],
     | 99.00th=[  3687], 99.50th=[  4883], 99.90th=[160433], 99.95th=[304088],
     | 99.99th=[362808]
   bw (  KiB/s): min=32784, max=276312, per=99.92%, avg=196804.97, stdev=61431.11, samples=119
   iops        : min= 8196, max=69078, avg=49201.29, stdev=15357.78, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.12%
  lat (msec)   : 2=30.45%, 4=68.65%, 10=0.53%, 20=0.09%, 50=0.01%
  lat (msec)   : 250=0.07%, 500=0.08%
  cpu          : usr=13.78%, sys=40.58%, ctx=162160, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2954412,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=192MiB/s (202MB/s), 192MiB/s-192MiB/s (202MB/s-202MB/s), io=11.3GiB (12.1GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=49.2k, BW=192MiB/s (202MB/s)(11.3GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118817/160000 hits, 4964 MiB read, 0.518 read amp, 35531490 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17214: Mon Jan 15 03:05:12 2024
  write: IOPS=46.3k, BW=181MiB/s (190MB/s)(10.6GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=374272, avg=19.02, stdev=756.79
    clat (usec): min=183, max=376918, avg=2744.57, stdev=8682.65
     lat (usec): min=194, max=376946, avg=2763.87, stdev=8716.87
    clat percentiles (usec):
     |  1.00th=[  1418],  5.00th=[  1696], 10.00th=[  1827], 20.00th=[  1975],
     | 30.00th=[  2114], 40.00th=[  2212], 50.00th=[  2311], 60.00th=[  2409],
     | 70.00th=[  2540], 80.00th=[  2704], 90.00th=[  3032], 95.00th=[  3425],
     | 99.00th=[  5014], 99.50th=[  6390], 99.90th=[164627], 99.95th=[267387],
     | 99.99th=[329253]
   bw (  KiB/s): min=55696, max=260576, per=100.00%, avg=185416.74, stdev=48902.26, samples=119
   iops        : min=13924, max=65144, avg=46354.17, stdev=12225.56, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.03%
  lat (msec)   : 2=21.42%, 4=76.07%, 10=2.14%, 20=0.09%, 50=0.06%
  lat (msec)   : 100=0.03%, 250=0.08%, 500=0.06%
  cpu          : usr=14.31%, sys=42.37%, ctx=146090, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2776767,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=181MiB/s (190MB/s), 181MiB/s-181MiB/s (190MB/s-190MB/s), io=10.6GiB (11.4GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=46.3k, BW=181MiB/s (190MB/s)(10.6GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118827/160000 hits, 4961 MiB read, 0.519 read amp, 35531669 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17270: Mon Jan 15 03:06:14 2024
  write: IOPS=32.1k, BW=251MiB/s (263MB/s)(14.7GiB/60003msec); 0 zone resets
    slat (usec): min=4, max=641983, avg=28.87, stdev=1535.31
    clat (usec): min=210, max=647043, avg=3960.64, stdev=17593.09
     lat (usec): min=229, max=647053, avg=3989.76, stdev=17664.27
    clat percentiles (usec):
     |  1.00th=[  1565],  5.00th=[  1991], 10.00th=[  2180], 20.00th=[  2474],
     | 30.00th=[  2638], 40.00th=[  2769], 50.00th=[  2900], 60.00th=[  3064],
     | 70.00th=[  3228], 80.00th=[  3425], 90.00th=[  3785], 95.00th=[  4178],
     | 99.00th=[  6456], 99.50th=[ 14746], 99.90th=[333448], 99.95th=[383779],
     | 99.99th=[557843]
   bw (  KiB/s): min=10672, max=394768, per=100.00%, avg=258014.92, stdev=106223.89, samples=118
   iops        : min= 1334, max=49346, avg=32251.90, stdev=13278.01, samples=118
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=5.24%, 4=88.04%, 10=6.12%, 20=0.16%, 50=0.07%
  lat (msec)   : 100=0.03%, 250=0.13%, 500=0.18%, 750=0.02%
  cpu          : usr=9.98%, sys=28.29%, ctx=126164, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1924149,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=251MiB/s (263MB/s), 251MiB/s-251MiB/s (263MB/s-263MB/s), io=14.7GiB (15.8GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=32.1k, BW=251MiB/s (263MB/s)(14.7GiB/60003msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randwrite, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118833/160000 hits, 4957 MiB read, 0.519 read amp, 35531847 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17327: Mon Jan 15 03:07:16 2024
  write: IOPS=20.6k, BW=322MiB/s (338MB/s)(18.9GiB/60006msec); 0 zone resets
    slat (usec): min=4, max=498009, avg=45.61, stdev=1276.58
    clat (usec): min=698, max=605438, avg=6164.86, stdev=15665.14
     lat (usec): min=719, max=605528, avg=6210.82, stdev=15735.94
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    6], 50.00th=[    6], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    6], 90.00th=[    7], 95.00th=[    7],
     | 99.00th=[   18], 99.50th=[   25], 99.90th=[  330], 99.95th=[  414],
     | 99.99th=[  550]
   bw (  KiB/s): min=19168, max=430528, per=99.95%, avg=329461.24, stdev=96588.22, samples=119
   iops        : min= 1198, max=26908, avg=20591.29, stdev=6036.75, samples=119
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.03%, 10=98.11%, 20=0.98%, 50=0.56%
  lat (msec)   : 100=0.10%, 250=0.10%, 500=0.09%, 750=0.03%
  cpu          : usr=9.64%, sys=26.20%, ctx=324377, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1236234,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=322MiB/s (338MB/s), 322MiB/s-322MiB/s (338MB/s-338MB/s), io=18.9GiB (20.3GB), run=60006-60006msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=20.6k, BW=322MiB/s (338MB/s)(18.9GiB/60006msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118835/160000 hits, 4956 MiB read, 0.519 read amp, 35531934 total
[0m

===Fio: workload=randwrite, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randwrite, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118837/160000 hits, 4954 MiB read, 0.519 read amp, 35532021 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17380: Mon Jan 15 03:08:19 2024
  write: IOPS=11.0k, BW=345MiB/s (361MB/s)(20.2GiB/60009msec); 0 zone resets
    slat (usec): min=5, max=654873, avg=86.62, stdev=1982.73
    clat (usec): min=531, max=666127, avg=11515.97, stdev=22589.17
     lat (usec): min=564, max=666270, avg=11603.08, stdev=22683.18
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   10], 60.00th=[   10],
     | 70.00th=[   11], 80.00th=[   11], 90.00th=[   12], 95.00th=[   14],
     | 99.00th=[   32], 99.50th=[   68], 99.90th=[  489], 99.95th=[  527],
     | 99.99th=[  667]
   bw (  KiB/s): min=30080, max=472832, per=100.00%, avg=355545.49, stdev=90583.32, samples=118
   iops        : min=  940, max=14776, avg=11110.81, stdev=2830.74, samples=118
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.07%, 4=0.15%, 10=62.78%, 20=34.38%, 50=1.95%
  lat (msec)   : 100=0.24%, 250=0.21%, 500=0.11%, 750=0.10%
  cpu          : usr=7.53%, sys=18.30%, ctx=330821, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,661834,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=345MiB/s (361MB/s), 345MiB/s-345MiB/s (361MB/s-361MB/s), io=20.2GiB (21.7GB), run=60009-60009msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=11.0k, BW=345MiB/s (361MB/s)(20.2GiB/60009msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randwrite, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118842/160000 hits, 4951 MiB read, 0.520 read amp, 35532187 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17433: Mon Jan 15 03:09:21 2024
  write: IOPS=5832, BW=365MiB/s (382MB/s)(21.4GiB/60020msec); 0 zone resets
    slat (usec): min=7, max=576411, avg=165.56, stdev=2344.03
    clat (usec): min=753, max=596520, avg=21776.38, stdev=29183.14
     lat (usec): min=862, max=596917, avg=21942.69, stdev=29307.14
    clat percentiles (msec):
     |  1.00th=[   16],  5.00th=[   17], 10.00th=[   18], 20.00th=[   18],
     | 30.00th=[   19], 40.00th=[   19], 50.00th=[   19], 60.00th=[   20],
     | 70.00th=[   20], 80.00th=[   21], 90.00th=[   23], 95.00th=[   28],
     | 99.00th=[   51], 99.50th=[  128], 99.90th=[  542], 99.95th=[  550],
     | 99.99th=[  600]
   bw (  KiB/s): min=34176, max=485120, per=99.95%, avg=373093.11, stdev=90819.52, samples=119
   iops        : min=  534, max= 7580, avg=5829.60, stdev=1419.06, samples=119
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.04%, 4=0.07%, 10=0.21%, 20=73.16%, 50=25.50%
  lat (msec)   : 100=0.32%, 250=0.32%, 500=0.17%, 750=0.20%
  cpu          : usr=6.90%, sys=12.57%, ctx=307884, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,350052,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=365MiB/s (382MB/s), 365MiB/s-365MiB/s (382MB/s-382MB/s), io=21.4GiB (22.9GB), run=60020-60020msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=5832, BW=365MiB/s (382MB/s)(21.4GiB/60020msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118857/160000 hits, 4948 MiB read, 0.520 read amp, 35532349 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17487: Mon Jan 15 03:10:23 2024
  write: IOPS=60.6k, BW=237MiB/s (248MB/s)(13.9GiB/60002msec); 0 zone resets
    slat (usec): min=4, max=315534, avg=14.37, stdev=395.27
    clat (usec): min=488, max=317913, avg=2095.15, stdev=4471.24
     lat (usec): min=493, max=317921, avg=2109.77, stdev=4488.85
    clat percentiles (usec):
     |  1.00th=[  1106],  5.00th=[  1434], 10.00th=[  1532], 20.00th=[  1680],
     | 30.00th=[  1778], 40.00th=[  1860], 50.00th=[  1926], 60.00th=[  2008],
     | 70.00th=[  2114], 80.00th=[  2245], 90.00th=[  2507], 95.00th=[  2769],
     | 99.00th=[  3916], 99.50th=[  4883], 99.90th=[  8225], 99.95th=[ 21627],
     | 99.99th=[256902]
   bw (  KiB/s): min=118656, max=280648, per=99.96%, avg=242415.26, stdev=35968.56, samples=119
   iops        : min=29664, max=70162, avg=60603.83, stdev=8992.10, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.38%
  lat (msec)   : 2=59.00%, 4=39.68%, 10=0.85%, 20=0.04%, 50=0.01%
  lat (msec)   : 250=0.03%, 500=0.01%
  cpu          : usr=16.77%, sys=50.27%, ctx=156738, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3637622,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=237MiB/s (248MB/s), 237MiB/s-237MiB/s (248MB/s-248MB/s), io=13.9GiB (14.9GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=60.6k, BW=237MiB/s (248MB/s)(13.9GiB/60002msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118859/160000 hits, 4945 MiB read, 0.520 read amp, 35532485 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17542: Mon Jan 15 03:11:26 2024
  write: IOPS=39.4k, BW=308MiB/s (323MB/s)(18.0GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=424320, avg=23.21, stdev=703.40
    clat (usec): min=214, max=427708, avg=3223.91, stdev=7932.59
     lat (usec): min=231, max=427738, avg=3247.38, stdev=7963.86
    clat percentiles (usec):
     |  1.00th=[  1647],  5.00th=[  2040], 10.00th=[  2278], 20.00th=[  2507],
     | 30.00th=[  2638], 40.00th=[  2737], 50.00th=[  2868], 60.00th=[  2966],
     | 70.00th=[  3097], 80.00th=[  3294], 90.00th=[  3687], 95.00th=[  4228],
     | 99.00th=[  6587], 99.50th=[  8979], 99.90th=[ 52691], 99.95th=[244319],
     | 99.99th=[358613]
   bw (  KiB/s): min=80128, max=388848, per=99.96%, avg=315052.34, stdev=65165.72, samples=119
   iops        : min=10016, max=48606, avg=39381.54, stdev=8145.71, samples=119
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.02%, 1000=0.03%
  lat (msec)   : 2=4.36%, 4=88.88%, 10=6.28%, 20=0.27%, 50=0.05%
  lat (msec)   : 100=0.01%, 250=0.04%, 500=0.05%
  cpu          : usr=11.13%, sys=36.93%, ctx=167720, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2363857,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=308MiB/s (323MB/s), 308MiB/s-308MiB/s (323MB/s-323MB/s), io=18.0GiB (19.4GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) write:  write: IOPS=39.4k, BW=308MiB/s (323MB/s)(18.0GiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118859/160000 hits, 4943 MiB read, 0.520 read amp, 35532621 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17595: Mon Jan 15 03:12:28 2024
  write: IOPS=21.2k, BW=331MiB/s (347MB/s)(19.4GiB/60115msec); 0 zone resets
    slat (usec): min=4, max=345754, avg=44.78, stdev=1042.39
    clat (usec): min=482, max=355006, avg=6003.77, stdev=11778.52
     lat (usec): min=499, max=355192, avg=6048.88, stdev=11825.18
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    6], 50.00th=[    6], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    6], 90.00th=[    7], 95.00th=[    8],
     | 99.00th=[   17], 99.50th=[   22], 99.90th=[  288], 99.95th=[  313],
     | 99.99th=[  342]
   bw (  KiB/s): min=112512, max=409184, per=100.00%, avg=339084.80, stdev=68116.93, samples=120
   iops        : min= 7032, max=25574, avg=21192.82, stdev=4257.32, samples=120
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.07%, 4=0.55%, 10=97.24%, 20=1.49%, 50=0.45%
  lat (msec)   : 100=0.03%, 250=0.01%, 500=0.15%
  cpu          : usr=9.23%, sys=26.20%, ctx=326499, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1271697,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=331MiB/s (347MB/s), 331MiB/s-331MiB/s (347MB/s-347MB/s), io=19.4GiB (20.8GB), run=60115-60115msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) write:  write: IOPS=21.2k, BW=331MiB/s (347MB/s)(19.4GiB/60115msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=write, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118859/160000 hits, 4941 MiB read, 0.520 read amp, 35532757 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17648: Mon Jan 15 03:13:31 2024
  write: IOPS=11.4k, BW=356MiB/s (374MB/s)(20.9GiB/60009msec); 0 zone resets
    slat (usec): min=5, max=402770, avg=83.67, stdev=1589.24
    clat (usec): min=442, max=414128, avg=11143.00, stdev=18796.02
     lat (usec): min=465, max=414275, avg=11227.16, stdev=18866.88
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[    9], 40.00th=[   10], 50.00th=[   10], 60.00th=[   10],
     | 70.00th=[   11], 80.00th=[   11], 90.00th=[   12], 95.00th=[   14],
     | 99.00th=[   27], 99.50th=[   53], 99.90th=[  355], 99.95th=[  401],
     | 99.99th=[  414]
   bw (  KiB/s): min=77056, max=468800, per=99.97%, avg=364645.11, stdev=87244.42, samples=119
   iops        : min= 2408, max=14650, avg=11395.16, stdev=2726.39, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.06%, 4=0.12%, 10=70.34%, 20=27.14%, 50=1.81%
  lat (msec)   : 100=0.22%, 250=0.03%, 500=0.27%
  cpu          : usr=7.21%, sys=18.85%, ctx=345479, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,684006,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=356MiB/s (374MB/s), 356MiB/s-356MiB/s (374MB/s-374MB/s), io=20.9GiB (22.4GB), run=60009-60009msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) write:  write: IOPS=11.4k, BW=356MiB/s (374MB/s)(20.9GiB/60009msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=write, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118848/160000 hits, 4939 MiB read, 0.521 read amp, 35532891 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=17702: Mon Jan 15 03:14:33 2024
  write: IOPS=5857, BW=366MiB/s (384MB/s)(21.5GiB/60020msec); 0 zone resets
    slat (usec): min=6, max=528755, avg=164.65, stdev=2295.17
    clat (usec): min=389, max=550125, avg=21682.08, stdev=26528.35
     lat (usec): min=422, max=550328, avg=21847.48, stdev=26635.53
    clat percentiles (msec):
     |  1.00th=[   12],  5.00th=[   17], 10.00th=[   17], 20.00th=[   18],
     | 30.00th=[   18], 40.00th=[   19], 50.00th=[   19], 60.00th=[   20],
     | 70.00th=[   20], 80.00th=[   21], 90.00th=[   24], 95.00th=[   30],
     | 99.00th=[   59], 99.50th=[  199], 99.90th=[  401], 99.95th=[  426],
     | 99.99th=[  550]
   bw (  KiB/s): min= 3072, max=499712, per=100.00%, avg=377269.78, stdev=81966.83, samples=119
   iops        : min=   48, max= 7808, avg=5894.84, stdev=1280.73, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=0.09%, 4=0.15%, 10=0.57%, 20=73.69%, 50=24.31%
  lat (msec)   : 100=0.56%, 250=0.16%, 500=0.43%, 750=0.04%
  cpu          : usr=6.32%, sys=12.61%, ctx=310865, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,351581,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=366MiB/s (384MB/s), 366MiB/s-366MiB/s (384MB/s-384MB/s), io=21.5GiB (23.0GB), run=60020-60020msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) write:  write: IOPS=5857, BW=366MiB/s (384MB/s)(21.5GiB/60020msec); 0 zone resets
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.327591 s, 320 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 1a896637-36e8-4174-bafc-8a60e9809953
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118830/160000 hits, 4936 MiB read, 0.521 read amp, 35533048 total
[0mFilebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.016: File-server Version 3.0 personality successfully loaded
0.017: Populating and pre-allocating filesets
0.312: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.312: Removing bigfileset tree (if exists)
0.318: Pre-allocating directories in bigfileset tree
0.720: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118830/160000 hits, 4935 MiB read, 0.521 read amp, 35533087 total
[0m36.484: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
36.484: Population and pre-allocation of filesets completed
36.485: Starting 1 filereader instances
37.494: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118830/160000 hits, 4935 MiB read, 0.521 read amp, 35533096 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118833/160000 hits, 4933 MiB read, 0.522 read amp, 35533166 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118833/160000 hits, 4933 MiB read, 0.522 read amp, 35533167 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118832/160000 hits, 4933 MiB read, 0.522 read amp, 35533170 total
[0m157.505: Run took 120 seconds...
157.511: Per-Operation Breakdown
statfile1            796155ops     6634ops/s   0.0mb/s    0.011ms/op [0.003ms - 8.189ms]
deletefile1          796152ops     6634ops/s   0.0mb/s    0.205ms/op [0.028ms - 377.087ms]
closefile3           796157ops     6634ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.021ms]
readfile1            796157ops     6634ops/s 869.3mb/s    0.096ms/op [0.004ms - 32.476ms]
openfile2            796158ops     6634ops/s   0.0mb/s    0.063ms/op [0.005ms - 36.043ms]
closefile2           796158ops     6634ops/s   0.0mb/s    0.004ms/op [0.001ms - 7.231ms]
appendfilerand1      796158ops     6634ops/s  51.8mb/s    0.429ms/op [0.001ms - 200.492ms]
openfile1            796161ops     6634ops/s   0.0mb/s    0.067ms/op [0.007ms - 35.108ms]
closefile1           796161ops     6634ops/s   0.0mb/s    0.006ms/op [0.001ms - 5.985ms]
wrtfile1             796165ops     6634ops/s 830.4mb/s    5.758ms/op [0.013ms - 207.919ms]
createfile1          796205ops     6634ops/s   0.0mb/s    0.142ms/op [0.024ms - 142.822ms]
157.511: IO Summary: 8757787 ops 72975.334 ops/s 6634/13268 rd/wr 1751.6mb/s 0.617ms/op
157.511: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:157.511: IO Summary: 8757787 ops 72975.334 ops/s 6634/13268 rd/wr 1751.6mb/s 0.617ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.014: OLTP Version 3.0  personality successfully loaded
0.014: Populating and pre-allocating filesets
0.014: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.014: Removing logfile tree (if exists)
0.020: Pre-allocating directories in logfile tree
0.023: Pre-allocating files in logfile tree
0.155: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.155: Removing datafiles tree (if exists)
0.158: Pre-allocating directories in datafiles tree
0.161: Pre-allocating files in datafiles tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118830/160000 hits, 4932 MiB read, 0.522 read amp, 35533172 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118829/160000 hits, 4932 MiB read, 0.522 read amp, 35533176 total
[0m36.599: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
36.599: Population and pre-allocation of filesets completed
36.600: Starting 200 shadow instances
36.724: Starting 10 dbwr instances
36.729: Starting 1 lgwr instances
37.733: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118829/160000 hits, 4932 MiB read, 0.522 read amp, 35533179 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118829/160000 hits, 4932 MiB read, 0.522 read amp, 35533181 total
[0m157.763: Run took 120 seconds...
157.781: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     2654354ops    22114ops/s   0.0mb/s    7.534ms/op [0.019ms - 407.976ms]
shadow-post-lg       2654355ops    22114ops/s   0.0mb/s    0.074ms/op [0.002ms - 56.057ms]
shadowhog            2654372ops    22115ops/s   0.0mb/s    0.322ms/op [0.092ms - 106.521ms]
shadowread           2679999ops    22328ops/s  43.2mb/s    1.007ms/op [0.001ms - 247.483ms]
dbwr-aiowait         26534ops      221ops/s   0.0mb/s    2.402ms/op [0.005ms - 49.111ms]
dbwr-block           26537ops      221ops/s   0.0mb/s   17.390ms/op [0.003ms - 258.200ms]
dbwr-hog             26544ops      221ops/s   0.0mb/s    0.010ms/op [0.004ms - 10.595ms]
dbwrite-a            2655680ops    22125ops/s  43.2mb/s    0.004ms/op [0.000ms - 79.067ms]
lg-block             829ops        7ops/s   0.0mb/s  144.630ms/op [33.522ms - 737.348ms]
lg-aiowait           830ops        7ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.026ms]
lg-write             831ops        7ops/s   1.7mb/s    0.009ms/op [0.001ms - 0.096ms]
157.781: IO Summary: 5363874 ops 44688.471 ops/s 22328/22132 rd/wr  88.1mb/s 0.517ms/op
157.781: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:157.781: IO Summary: 5363874 ops 44688.471 ops/s 22328/22132 rd/wr  88.1mb/s 0.517ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.015: Varmail Version 3.0 personality successfully loaded
0.015: Populating and pre-allocating filesets
1.015: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
1.015: Removing bigfileset tree (if exists)
1.021: Pre-allocating directories in bigfileset tree
1.025: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118826/160000 hits, 4932 MiB read, 0.522 read amp, 35533186 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118821/160000 hits, 4932 MiB read, 0.522 read amp, 35533207 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118820/160000 hits, 4932 MiB read, 0.522 read amp, 35533209 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118819/160000 hits, 4932 MiB read, 0.522 read amp, 35533210 total
[0m78.720: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
78.721: Population and pre-allocation of filesets completed
78.721: Starting 1 filereader instances
79.726: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 118818/160000 hits, 4932 MiB read, 0.522 read amp, 35533211 total
[0m199.736: Run took 120 seconds...
199.737: Per-Operation Breakdown
closefile4           219323ops     1828ops/s   0.0mb/s    0.003ms/op [0.001ms - 0.416ms]
readfile4            219323ops     1828ops/s  48.5mb/s    0.658ms/op [0.008ms - 195.746ms]
openfile4            219323ops     1828ops/s   0.0mb/s    0.021ms/op [0.005ms - 4.033ms]
closefile3           219323ops     1828ops/s   0.0mb/s    0.006ms/op [0.001ms - 2.951ms]
fsyncfile3           219323ops     1828ops/s   0.0mb/s    2.765ms/op [0.565ms - 427.420ms]
appendfilerand3      219333ops     1828ops/s  14.3mb/s    0.068ms/op [0.007ms - 13.269ms]
readfile3            219333ops     1828ops/s  48.6mb/s    0.718ms/op [0.007ms - 195.771ms]
openfile3            219333ops     1828ops/s   0.0mb/s    0.022ms/op [0.005ms - 41.495ms]
closefile2           219333ops     1828ops/s   0.0mb/s    0.006ms/op [0.001ms - 22.345ms]
fsyncfile2           219333ops     1828ops/s   0.0mb/s    2.895ms/op [0.822ms - 422.305ms]
appendfilerand2      219336ops     1828ops/s  14.3mb/s    0.235ms/op [0.015ms - 160.379ms]
createfile2          219338ops     1828ops/s   0.0mb/s    0.596ms/op [0.031ms - 228.528ms]
deletefile1          219339ops     1828ops/s   0.0mb/s    0.616ms/op [0.044ms - 420.655ms]
199.737: IO Summary: 2851293 ops 23758.935 ops/s 3655/3655 rd/wr 125.7mb/s 0.662ms/op
199.737: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:199.737: IO Summary: 2851293 ops 23758.935 ops/s 3655/3655 rd/wr 125.7mb/s 0.662ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T02:11:10.lsvd-nvme.20.rssd2.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T02:11:10.lsvd-nvme.20.rssd2.txt
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=24.6k, BW=96.2MiB/s (101MB/s)(5774MiB/60007msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=38.0k, BW=148MiB/s (156MB/s)(8899MiB/60004msec)
Fio (iodepth=1; bs=4ki) randread:  read: IOPS=681, BW=2727KiB/s (2793kB/s)(160MiB/60001msec)
Fio (iodepth=4; bs=4ki) randread:  read: IOPS=3686, BW=14.4MiB/s (15.1MB/s)(864MiB/60002msec)
Fio (iodepth=8; bs=4ki) randread:  read: IOPS=9390, BW=36.7MiB/s (38.5MB/s)(2201MiB/60002msec)
Fio (iodepth=16; bs=4ki) randread:  read: IOPS=12.7k, BW=49.6MiB/s (52.0MB/s)(2975MiB/60002msec)
Fio (iodepth=32; bs=4ki) randread:  read: IOPS=18.0k, BW=70.2MiB/s (73.6MB/s)(4212MiB/60005msec)
Fio (iodepth=64; bs=4ki) randread:  read: IOPS=21.3k, BW=83.2MiB/s (87.3MB/s)(4994MiB/60004msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=23.5k, BW=91.8MiB/s (96.3MB/s)(5510MiB/60013msec)
Fio (iodepth=256; bs=4ki) randread:  read: IOPS=24.8k, BW=96.7MiB/s (101MB/s)(5802MiB/60013msec)
Fio (iodepth=512; bs=4ki) randread:  read: IOPS=24.8k, BW=96.9MiB/s (102MB/s)(5817MiB/60011msec)
Fio (iodepth=32; bs=4ki) read:  read: IOPS=24.3k, BW=94.8MiB/s (99.5MB/s)(5691MiB/60001msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=49.0k, BW=192MiB/s (201MB/s)(11.2GiB/60005msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=25.0k, BW=97.7MiB/s (102MB/s)(5862MiB/60007msec)
Fio (iodepth=128; bs=8ki) randread:  read: IOPS=26.2k, BW=205MiB/s (215MB/s)(12.0GiB/60012msec)
Fio (iodepth=128; bs=16ki) randread:  read: IOPS=23.2k, BW=362MiB/s (380MB/s)(21.2GiB/60006msec)
Fio (iodepth=128; bs=32ki) randread:  read: IOPS=17.2k, BW=537MiB/s (563MB/s)(31.5GiB/60006msec)
Fio (iodepth=128; bs=64ki) randread:  read: IOPS=12.0k, BW=750MiB/s (787MB/s)(44.0GiB/60032msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=37.4k, BW=146MiB/s (153MB/s)(8774MiB/60004msec)
Fio (iodepth=128; bs=8ki) read:  read: IOPS=31.1k, BW=243MiB/s (255MB/s)(14.2GiB/60008msec)
Fio (iodepth=128; bs=16ki) read:  read: IOPS=22.3k, BW=348MiB/s (365MB/s)(20.4GiB/60015msec)
Fio (iodepth=128; bs=32ki) read:  read: IOPS=12.4k, BW=388MiB/s (406MB/s)(22.7GiB/60032msec)
Fio (iodepth=128; bs=64ki) read:  read: IOPS=5690, BW=356MiB/s (373MB/s)(20.9GiB/60043msec)
Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5814, BW=22.7MiB/s (23.8MB/s)(1363MiB/60001msec); 0 zone resets
Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=18.1k, BW=70.8MiB/s (74.2MB/s)(4247MiB/60001msec); 0 zone resets
Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=28.1k, BW=110MiB/s (115MB/s)(6584MiB/60001msec); 0 zone resets
Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=40.8k, BW=159MiB/s (167MB/s)(9553MiB/60001msec); 0 zone resets
Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=53.3k, BW=208MiB/s (218MB/s)(12.2GiB/60001msec); 0 zone resets
Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=55.3k, BW=216MiB/s (226MB/s)(12.7GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=56.2k, BW=220MiB/s (230MB/s)(12.9GiB/60001msec); 0 zone resets
Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=56.0k, BW=219MiB/s (229MB/s)(12.8GiB/60001msec); 0 zone resets
Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=56.4k, BW=220MiB/s (231MB/s)(12.9GiB/60002msec); 0 zone resets
Fio (iodepth=32; bs=4ki) write:  write: IOPS=47.0k, BW=184MiB/s (192MB/s)(10.8GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=49.2k, BW=192MiB/s (202MB/s)(11.3GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=46.3k, BW=181MiB/s (190MB/s)(10.6GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=32.1k, BW=251MiB/s (263MB/s)(14.7GiB/60003msec); 0 zone resets
Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=20.6k, BW=322MiB/s (338MB/s)(18.9GiB/60006msec); 0 zone resets
Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=11.0k, BW=345MiB/s (361MB/s)(20.2GiB/60009msec); 0 zone resets
Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=5832, BW=365MiB/s (382MB/s)(21.4GiB/60020msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=60.6k, BW=237MiB/s (248MB/s)(13.9GiB/60002msec); 0 zone resets
Fio (iodepth=128; bs=8ki) write:  write: IOPS=39.4k, BW=308MiB/s (323MB/s)(18.0GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=16ki) write:  write: IOPS=21.2k, BW=331MiB/s (347MB/s)(19.4GiB/60115msec); 0 zone resets
Fio (iodepth=128; bs=32ki) write:  write: IOPS=11.4k, BW=356MiB/s (374MB/s)(20.9GiB/60009msec); 0 zone resets
Fio (iodepth=128; bs=64ki) write:  write: IOPS=5857, BW=366MiB/s (384MB/s)(21.5GiB/60020msec); 0 zone resets
Filebench /tmp/filebench/fileserver.f:157.511: IO Summary: 8757787 ops 72975.334 ops/s 6634/13268 rd/wr 1751.6mb/s 0.617ms/op
Filebench /tmp/filebench/oltp.f:157.781: IO Summary: 5363874 ops 44688.471 ops/s 22328/22132 rd/wr  88.1mb/s 0.517ms/op
Filebench /tmp/filebench/varmail.f:199.737: IO Summary: 2851293 ops 23758.935 ops/s 3655/3655 rd/wr 125.7mb/s 0.662ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7f029d7ea640) exiting
+ ulimit -c
unlimited
+ '[' -z triple-hdd ']'
+ pool_name=triple-hdd
+ out_post=nvme
++ date +%FT%T
+ cur_time=2024-01-15T03:23:47
+ default_cache_size=21474836480
+ cache_size=21474836480
++ git rev-parse --show-toplevel
+ lsvd_dir=/home/isaackhor/code/lsvd-rbd
++ ip addr
++ perl -lane 'print $1 if /inet (10\.1\.[0-9.]+)\/24/'
++ head -n 1
+ gw_ip=10.1.0.5
+ client_ip=10.1.0.6
+ rcache=/mnt/nvme/
+ wlog=/mnt/nvme-remote/
+ cache_size_gb=20
+ outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T03:23:47.lsvd-nvme.20.triple-hdd.txt
+ echo 'Running gateway on 10.1.0.5, client on 10.1.0.6'
Running gateway on 10.1.0.5, client on 10.1.0.6
+ imgname=lsvd-benchmark
+ imgsize=80g
+ blocksize=4096
+ source /home/isaackhor/code/lsvd-rbd/experiments/common.bash
++ set -xeuo pipefail
++ '[' 0 -ne 0 ']'
+ echo '===Building LSVD...'
===Building LSVD...
+ cd /home/isaackhor/code/lsvd-rbd
+ make clean
+ make -j20 release
CC translate.cc
CC io.cc
CC objects.cc
CC img_reader.cc
CC config.cc
CC mkcache.cc
CC nvme.cc
CC write_cache.cc
CC file_backend.cc
CC shared_read_cache.cc
CC lsvd_debug.cc
CC rados_backend.cc
CC liblsvd.cc
CC image.cc
CC imgtool.cc
CC thick-image.cc
In file included from translate.cc:28:
./extent.h:160:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  160 |         s.d = 1; // new extents are dirty
      |             ^ ~
./extent.h:520:11: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::_extent' requested here
  520 |         T _e(base, limit - base, e);
      |           ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:174:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  174 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:553:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::relimit' requested here
  553 |                 it->relimit(base);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
In file included from translate.cc:28:
./extent.h:185:13: warning: implicit truncation from 'int' to a one-bit wide bit-field changes value from 1 to -1 [-Wsingle-bit-bitfield-constant-conversion]
  185 |         s.d = 1; // dirty
      |             ^ ~
./extent.h:601:21: note: in instantiation of member function 'extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>::rebase' requested here
  601 |                 it->rebase(limit);
      |                     ^
./extent.h:671:9: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::_update' requested here
  671 |         _update(base, limit, e, false, nullptr);
      |         ^
translate.cc:451:18: note: in instantiation of member function 'extmap::extmap<extmap::_extent<extmap::_lba2obj, long, extmap::obj_offset>, long, extmap::obj_offset>::update' requested here
  451 |             map->update(m.lba, m.lba + m.len,
      |                  ^
3 warnings generated.
LD liblsvd.so
LD imgtool
LD thick-image
+ mkdir -p /home/isaackhor/code/lsvd-rbd/test/baklibs/
+ cp /home/isaackhor/code/lsvd-rbd/liblsvd.so /home/isaackhor/code/lsvd-rbd/test/baklibs/liblsvd.so.2024-01-15T03:23:47
+ kill_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ true
+ scripts/rpc.py spdk_kill_instance SIGKILL
+ true
+ pkill -f nvmf_tgt
+ true
+ pkill -f reactor_0
+ true
+ sleep 2
+ create_lsvd_thick triple-hdd lsvd-benchmark 80g
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local size=80g
+ cd /home/isaackhor/code/lsvd-rbd
+ ./remove_objs.py triple-hdd lsvd-benchmark
Removing all objects from pool triple-hdd with prefix lsvd-benchmark
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Removed 20488/67116 objects
+ ./thick-image --size=80g triple-hdd/lsvd-benchmark
Thick provisioning: 1% complete...Thick provisioning: 2% complete...Thick provisioning: 3% complete...Thick provisioning: 4% complete...Thick provisioning: 5% complete...Thick provisioning: 6% complete...Thick provisioning: 7% complete...Thick provisioning: 8% complete...Thick provisioning: 9% complete...Thick provisioning: 10% complete...Thick provisioning: 11% complete...Thick provisioning: 12% complete...Thick provisioning: 13% complete...Thick provisioning: 14% complete...Thick provisioning: 15% complete...Thick provisioning: 16% complete...Thick provisioning: 17% complete...Thick provisioning: 18% complete...Thick provisioning: 19% complete...Thick provisioning: 20% complete...Thick provisioning: 21% complete...Thick provisioning: 22% complete...Thick provisioning: 23% complete...Thick provisioning: 24% complete...Thick provisioning: 25% complete...Thick provisioning: 26% complete...Thick provisioning: 27% complete...Thick provisioning: 28% complete...Thick provisioning: 29% complete...Thick provisioning: 30% complete...Thick provisioning: 31% complete...Thick provisioning: 32% complete...Thick provisioning: 33% complete...Thick provisioning: 34% complete...Thick provisioning: 35% complete...Thick provisioning: 36% complete...Thick provisioning: 37% complete...Thick provisioning: 38% complete...Thick provisioning: 39% complete...Thick provisioning: 40% complete...Thick provisioning: 41% complete...Thick provisioning: 42% complete...Thick provisioning: 43% complete...Thick provisioning: 44% complete...Thick provisioning: 45% complete...Thick provisioning: 46% complete...Thick provisioning: 47% complete...Thick provisioning: 48% complete...Thick provisioning: 49% complete...Thick provisioning: 50% complete...Thick provisioning: 51% complete...Thick provisioning: 52% complete...Thick provisioning: 53% complete...Thick provisioning: 54% complete...Thick provisioning: 55% complete...Thick provisioning: 56% complete...Thick provisioning: 57% complete...Thick provisioning: 58% complete...Thick provisioning: 59% complete...Thick provisioning: 60% complete...Thick provisioning: 61% complete...Thick provisioning: 62% complete...Thick provisioning: 63% complete...Thick provisioning: 64% complete...Thick provisioning: 65% complete...Thick provisioning: 66% complete...Thick provisioning: 67% complete...Thick provisioning: 68% complete...Thick provisioning: 69% complete...Thick provisioning: 70% complete...Thick provisioning: 71% complete...Thick provisioning: 72% complete...Thick provisioning: 73% complete...Thick provisioning: 74% complete...Thick provisioning: 75% complete...Thick provisioning: 76% complete...Thick provisioning: 77% complete...Thick provisioning: 78% complete...Thick provisioning: 79% complete...Thick provisioning: 80% complete...Thick provisioning: 81% complete...Thick provisioning: 82% complete...Thick provisioning: 83% complete...Thick provisioning: 84% complete...Thick provisioning: 85% complete...Thick provisioning: 86% complete...Thick provisioning: 87% complete...Thick provisioning: 88% complete...Thick provisioning: 89% complete...Thick provisioning: 90% complete...Thick provisioning: 91% complete...Thick provisioning: 92% complete...Thick provisioning: 93% complete...Thick provisioning: 94% complete...Thick provisioning: 95% complete...Thick provisioning: 96% complete...Thick provisioning: 97% complete...Thick provisioning: 98% complete...Thick provisioning: 99% complete...Thick provisioning: 100% complete...Thick provisioning: 100% complete...done
+ rados -p triple-hdd stat lsvd-benchmark
triple-hdd/lsvd-benchmark mtime 2024-01-15T03:37:13.000000+0000, size 4096
+ fstrim /mnt/nvme
+ launch_lsvd_gw_background /mnt/nvme/ /mnt/nvme-remote/ 21474836480
+ local rcache_root=/mnt/nvme/
+ local wlog_root=/mnt/nvme-remote/
+ local cache_size=21474836480
+ echo 4096
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ mkdir -p /mnt/nvme//lsvd-read/ /mnt/nvme-remote//lsvd-write/
+ export LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ LSVD_RCACHE_DIR=/mnt/nvme//lsvd-read/
+ export LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ LSVD_WCACHE_DIR=/mnt/nvme-remote//lsvd-write/
+ export LSVD_GC_THRESHOLD=40
+ LSVD_GC_THRESHOLD=40
+ export LSVD_CACHE_SIZE=21474836480
+ LSVD_CACHE_SIZE=21474836480
+ rm -rf /mnt/nvme-remote//lsvd-write/6a40ae64-1aa1-4bcf-a5ed-7d945ee86c0b.wcache
+ sleep 5
+ LD_PRELOAD=/home/isaackhor/code/lsvd-rbd/liblsvd.so
+ ./build/bin/nvmf_tgt -m '[0,1,2,3]'
[2024-01-15 03:37:43.954914] Starting SPDK v23.09 git sha1 fb13eebf5 / DPDK 23.07.0 initialization...
[2024-01-15 03:37:43.955111] [ DPDK EAL parameters: nvmf --no-shconf -l 0,1,2,3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid967933 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2024-01-15 03:37:44.058828] app.c: 786:spdk_app_start: *NOTICE*: Total cores available: 4
[2024-01-15 03:37:44.203756] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 1
[2024-01-15 03:37:44.203825] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 2
[2024-01-15 03:37:44.203891] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 3
[2024-01-15 03:37:44.203897] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
+ configure_nvmf_common 10.1.0.5
+ local gateway_ip=10.1.0.5
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_register_cluster rbd_cluster
rbd_cluster
+ scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192
[2024-01-15 03:37:49.596519] tcp.c: 656:nvmf_tcp_create: *NOTICE*: *** TCP Transport Init ***
+ scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
+ scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 10.1.0.5 -s 9922
[2024-01-15 03:37:50.331374] tcp.c: 948:nvmf_tcp_listen: *NOTICE*: *** NVMe/TCP Target Listening on 10.1.0.5 port 9922 ***
+ add_rbd_img triple-hdd lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ local pool=triple-hdd
+ local img=lsvd-benchmark
+ local bdev=bdev_lsvd-benchmark
+ scripts/rpc.py bdev_rbd_create triple-hdd lsvd-benchmark 4096 -c rbd_cluster -b bdev_lsvd-benchmark
[1m[36m[INFO shared_read_cache.cc:692 sharded_cache] Initialising read cache of size 20480 MiB in 16 shards, 1280 MiB each
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_0 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_1 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_2 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_3 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_4 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_5 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_6 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_7 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_8 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_9 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_10 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_11 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_12 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_13 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_14 for the read cache
[0m[35m[DBG shared_read_cache.cc:432 shared_read_cache] Opening /mnt/nvme//lsvd-read//read_cache_shard_15 for the read cache
[0m[1m[36m[INFO image.cc:74 image_open] Creating write cache file /mnt/nvme-remote//lsvd-write//8c027672-5580-444b-8c42-9e65959ca969.wcache
[0m[1m[36m[INFO liblsvd.cc:280 rbd_open] Opened image: lsvd-benchmark, size 85899345920
[0m[35m[DBG translate.cc:1365 gc_thread] starting gc thread
[0m[2024-01-15 03:37:57.445611] bdev_rbd.c:1329:bdev_rbd_create: *NOTICE*: Add bdev_lsvd-benchmark rbd disk to lun
bdev_lsvd-benchmark
+ scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 bdev_lsvd-benchmark
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 1/2 hits, 0 MiB read, 1.778 read amp, 2 total
[0m+ trap 'cleanup_nvmf_rbd bdev_lsvd-benchmark; cleanup_nvmf; exit' SIGINT SIGTERM EXIT
+ run_client_bench 10.1.0.6 /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T03:23:47.lsvd-nvme.20.triple-hdd.txt client-bench.bash read_entire_img=1
+ local client_ip=10.1.0.6
+ local outfile=/home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T03:23:47.lsvd-nvme.20.triple-hdd.txt
+ local benchscript=client-bench.bash
+ local additional_args=read_entire_img=1
+ cd /home/isaackhor/code/lsvd-rbd/experiments
+ ssh 10.1.0.6 'mkdir -p /tmp/filebench; rm -rf /tmp/filebench/*'
+ scp ./filebench-workloads/fileserver.f ./filebench-workloads/oltp.f ./filebench-workloads/varmail.f root@10.1.0.6:/tmp/filebench/
+ ssh 10.1.0.6 'bash -s gw_ip=10.1.0.5 read_entire_img=1'
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T03:23:47.lsvd-nvme.20.triple-hdd.txt
===Starting client benchmark

+ for pair in $*
+ '[' 10.1.0.5 '!=' gw_ip=10.1.0.5 ']'
+ eval gw_ip=10.1.0.5
++ gw_ip=10.1.0.5
+ for pair in $*
+ '[' 1 '!=' read_entire_img=1 ']'
+ eval read_entire_img=1
++ read_entire_img=1
+ printf '===Starting client benchmark\n\n'
+ trap 'umount /mnt/fsbench || true; nvme disconnect -n nqn.2016-06.io.spdk:cnode1 || true; exit' SIGINT SIGTERM SIGHUP EXIT
+ modprobe nvme-fabrics
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode1
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode2
NQN:nqn.2016-06.io.spdk:cnode2 disconnected 0 controller(s)
+ nvme disconnect -n nqn.2016-06.io.spdk:cnode3
NQN:nqn.2016-06.io.spdk:cnode3 disconnected 0 controller(s)
+ gw_ip=10.1.0.5
+ nvme connect -t tcp --traddr 10.1.0.5 -s 9922 -n nqn.2016-06.io.spdk:cnode1 -o normal
device: nvme1
+ sleep 5
+ nvme list
Node                  SN                   Model                                    Namespace Usage                      Format           FW Rev  
--------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1          PHM27522000B480BGN   INTEL SSDPE21D480GA                      1         480.10  GB / 480.10  GB    512   B +  0 B   E2010414
/dev/nvme1n1          SPDK00000000000001   SPDK_Controller1                         1          85.90  GB /  85.90  GB      4 KiB +  0 B   23.09   
++ nvme list
++ perl -lane 'print @F[0] if /SPDK/'
Using device /dev/nvme1n1
+ dev_name=/dev/nvme1n1
+ printf 'Using device /dev/nvme1n1\n'
+ num_fio_processes=1
+ fio_size=80GiB
+ read_entire_img=1


===Reading entire image to warm cache===

+ [[ 1 -eq 1 ]]
+ printf '\n\n===Reading entire image to warm cache===\n\n'
+ dd if=/dev/nvme1n1 of=/dev/null bs=1048576 count=81910
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 4536/13714 hits, 569 MiB read, 1.007 read amp, 13714 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 16536/50284 hits, 2093 MiB read, 1.008 read amp, 50284 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 26041/79252 hits, 3300 MiB read, 1.008 read amp, 79252 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 38145/116140 hits, 4837 MiB read, 1.008 read amp, 116140 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 50089/152542 hits, 6354 MiB read, 1.008 read amp, 152542 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 189454 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 224848 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 260158 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 296074 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 332410 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 368722 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 405568 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 442366 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 478270 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 514630 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 550948 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 587188 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 623314 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 659128 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 694102 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 730066 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 764692 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 801208 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 836926 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 872944 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 909670 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 945580 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 981826 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1017916 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1053976 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1078408 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1107928 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52506/160000 hits, 6666 MiB read, 1.008 read amp, 1144042 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1179832 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1216210 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1250884 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1287976 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1323286 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1359970 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1395394 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1431844 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 1468120 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1504042 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1539730 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52501/160000 hits, 6666 MiB read, 1.008 read amp, 1575124 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52503/160000 hits, 6666 MiB read, 1.008 read amp, 1610770 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1646116 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1682146 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1717462 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52504/160000 hits, 6666 MiB read, 1.008 read amp, 1753624 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52499/160000 hits, 6666 MiB read, 1.008 read amp, 1789522 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1825134 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1861732 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52502/160000 hits, 6666 MiB read, 1.008 read amp, 1897750 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52500/160000 hits, 6666 MiB read, 1.008 read amp, 1934290 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52505/160000 hits, 6666 MiB read, 1.008 read amp, 1965916 total
[0m81910+0 records in
81910+0 records out
85888860160 bytes (86 GB, 80 GiB) copied, 1116.49 s, 76.9 MB/s
+ set +x


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 41564/160000 hits, 3528 MiB read, 2.098 read amp, 2048542 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 31402/160000 hits, 588 MiB read, 13.649 read amp, 2152867 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 34125/160000 hits, 588 MiB read, 13.358 read amp, 2254882 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22229: Mon Jan 15 03:57:42 2024
  read: IOPS=4862, BW=19.0MiB/s (19.9MB/s)(1148MiB/60418msec)
    slat (usec): min=4, max=3099, avg=198.67, stdev=261.19
    clat (usec): min=60, max=1170.1k, avg=26118.62, stdev=82483.41
     lat (usec): min=125, max=1170.3k, avg=26318.01, stdev=82483.53
    clat percentiles (usec):
     |  1.00th=[   123],  5.00th=[   145], 10.00th=[   180], 20.00th=[  1450],
     | 30.00th=[  5604], 40.00th=[  7570], 50.00th=[  9372], 60.00th=[ 11076],
     | 70.00th=[ 14484], 80.00th=[ 20317], 90.00th=[ 36963], 95.00th=[ 74974],
     | 99.00th=[526386], 99.50th=[692061], 99.90th=[843056], 99.95th=[893387],
     | 99.99th=[960496]
   bw (  KiB/s): min=16056, max=23360, per=100.00%, avg=19576.60, stdev=1182.80, samples=120
   iops        : min= 4014, max= 5840, avg=4894.15, stdev=295.70, samples=120
  lat (usec)   : 100=0.01%, 250=15.51%, 500=3.69%, 750=0.49%, 1000=0.04%
  lat (msec)   : 2=0.56%, 4=2.57%, 10=30.42%, 20=26.16%, 50=13.32%
  lat (msec)   : 100=3.34%, 250=1.78%, 500=1.00%, 750=0.80%, 1000=0.30%
  lat (msec)   : 2000=0.01%
  cpu          : usr=4.35%, sys=10.27%, ctx=235772, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=293776,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=19.0MiB/s (19.9MB/s), 19.0MiB/s-19.0MiB/s (19.9MB/s-19.9MB/s), io=1148MiB (1203MB), run=60418-60418msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=4862, BW=19.0MiB/s (19.9MB/s)(1148MiB/60418msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152330/160000 hits, 588 MiB read, 0.815 read amp, 2555116 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152186/160000 hits, 588 MiB read, 0.830 read amp, 2908664 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152176/160000 hits, 588 MiB read, 0.831 read amp, 3318451 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22276: Mon Jan 15 03:58:44 2024
  read: IOPS=18.7k, BW=73.1MiB/s (76.6MB/s)(4385MiB/60014msec)
    slat (usec): min=4, max=130290, avg=51.26, stdev=565.12
    clat (usec): min=98, max=476350, avg=6789.71, stdev=12817.65
     lat (usec): min=120, max=476439, avg=6841.26, stdev=12902.87
    clat percentiles (usec):
     |  1.00th=[   194],  5.00th=[   269], 10.00th=[   334], 20.00th=[  2802],
     | 30.00th=[  5145], 40.00th=[  5735], 50.00th=[  6325], 60.00th=[  6980],
     | 70.00th=[  7767], 80.00th=[  8586], 90.00th=[ 10159], 95.00th=[ 12649],
     | 99.00th=[ 21103], 99.50th=[ 25297], 99.90th=[287310], 99.95th=[325059],
     | 99.99th=[421528]
   bw (  KiB/s): min= 1664, max=97160, per=99.99%, avg=74815.33, stdev=20091.21, samples=119
   iops        : min=  416, max=24290, avg=18703.83, stdev=5022.81, samples=119
  lat (usec)   : 100=0.01%, 250=3.75%, 500=11.32%, 750=1.80%, 1000=0.27%
  lat (msec)   : 2=1.49%, 4=2.63%, 10=67.96%, 20=9.51%, 50=1.01%
  lat (msec)   : 100=0.09%, 250=0.05%, 500=0.12%
  cpu          : usr=5.29%, sys=15.00%, ctx=103856, majf=0, minf=140
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1122588,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=73.1MiB/s (76.6MB/s), 73.1MiB/s-73.1MiB/s (76.6MB/s-76.6MB/s), io=4385MiB (4598MB), run=60014-60014msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=18.7k, BW=73.1MiB/s (76.6MB/s)(4385MiB/60014msec)


===Fio: workload=randread, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 151231/160000 hits, 588 MiB read, 0.932 read amp, 3471961 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 149676/160000 hits, 588 MiB read, 1.097 read amp, 3474478 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 148060/160000 hits, 588 MiB read, 1.269 read amp, 3477056 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22322: Mon Jan 15 03:59:47 2024
  read: IOPS=120, BW=480KiB/s (492kB/s)(28.1MiB/60010msec)
    slat (usec): min=6, max=448, avg=31.11, stdev=12.21
    clat (usec): min=99, max=128949, avg=8289.93, stdev=8871.73
     lat (usec): min=108, max=128968, avg=8321.80, stdev=8871.80
    clat percentiles (usec):
     |  1.00th=[   129],  5.00th=[   139], 10.00th=[   190], 20.00th=[   223],
     | 30.00th=[   285], 40.00th=[  5342], 50.00th=[  7439], 60.00th=[  9241],
     | 70.00th=[ 11076], 80.00th=[ 13435], 90.00th=[ 18220], 95.00th=[ 21627],
     | 99.00th=[ 39060], 99.50th=[ 52691], 99.90th=[ 74974], 99.95th=[ 99091],
     | 99.99th=[128451]
   bw (  KiB/s): min=  296, max=  616, per=99.75%, avg=479.80, stdev=61.91, samples=119
   iops        : min=   74, max=  154, avg=119.95, stdev=15.48, samples=119
  lat (usec)   : 100=0.01%, 250=29.12%, 500=2.40%, 750=0.68%, 1000=0.04%
  lat (msec)   : 2=0.54%, 4=1.39%, 10=29.78%, 20=29.11%, 50=6.40%
  lat (msec)   : 100=0.49%, 250=0.04%
  cpu          : usr=0.22%, sys=0.58%, ctx=7205, majf=0, minf=12
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=7204,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=480KiB/s (492kB/s), 480KiB/s-480KiB/s (492kB/s-492kB/s), io=28.1MiB (29.5MB), run=60010-60010msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randread:  read: IOPS=120, BW=480KiB/s (492kB/s)(28.1MiB/60010msec)


===Fio: workload=randread, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 144573/160000 hits, 588 MiB read, 1.639 read amp, 3490985 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 136884/160000 hits, 588 MiB read, 2.456 read amp, 3503389 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 129242/160000 hits, 588 MiB read, 3.269 read amp, 3515722 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22368: Mon Jan 15 04:00:49 2024
  read: IOPS=699, BW=2798KiB/s (2865kB/s)(164MiB/60012msec)
    slat (usec): min=4, max=458, avg=22.24, stdev=12.11
    clat (usec): min=93, max=132825, avg=5688.54, stdev=6810.66
     lat (usec): min=103, max=132834, avg=5711.48, stdev=6812.21
    clat percentiles (usec):
     |  1.00th=[   114],  5.00th=[   129], 10.00th=[   145], 20.00th=[   176],
     | 30.00th=[   210], 40.00th=[   251], 50.00th=[  4752], 60.00th=[  6652],
     | 70.00th=[  8586], 80.00th=[ 10290], 90.00th=[ 12387], 95.00th=[ 17957],
     | 99.00th=[ 26870], 99.50th=[ 32900], 99.90th=[ 47449], 99.95th=[ 83362],
     | 99.99th=[123208]
   bw (  KiB/s): min= 1872, max=58168, per=100.00%, avg=2805.24, stdev=5120.72, samples=119
   iops        : min=  468, max=14542, avg=701.31, stdev=1280.18, samples=119
  lat (usec)   : 100=0.04%, 250=39.91%, 500=3.10%, 750=0.68%, 1000=0.02%
  lat (msec)   : 2=0.68%, 4=2.12%, 10=31.81%, 20=18.33%, 50=3.22%
  lat (msec)   : 100=0.07%, 250=0.03%
  cpu          : usr=1.17%, sys=2.45%, ctx=37277, majf=0, minf=15
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=41980,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
   READ: bw=2798KiB/s (2865kB/s), 2798KiB/s-2798KiB/s (2865kB/s-2865kB/s), io=164MiB (172MB), run=60012-60012msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randread:  read: IOPS=699, BW=2798KiB/s (2865kB/s)(164MiB/60012msec)


===Fio: workload=randread, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 123614/160000 hits, 588 MiB read, 3.867 read amp, 3572936 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 107286/160000 hits, 588 MiB read, 5.602 read amp, 3599034 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 90886/160000 hits, 588 MiB read, 7.343 read amp, 3625180 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22414: Mon Jan 15 04:01:52 2024
  read: IOPS=1882, BW=7530KiB/s (7711kB/s)(441MiB/60010msec)
    slat (usec): min=4, max=462, avg=16.07, stdev= 9.03
    clat (usec): min=103, max=154933, avg=4227.89, stdev=6057.42
     lat (usec): min=117, max=154947, avg=4244.55, stdev=6059.80
    clat percentiles (usec):
     |  1.00th=[   122],  5.00th=[   139], 10.00th=[   161], 20.00th=[   198],
     | 30.00th=[   219], 40.00th=[   239], 50.00th=[   273], 60.00th=[  4228],
     | 70.00th=[  6652], 80.00th=[  8979], 90.00th=[ 11076], 95.00th=[ 13960],
     | 99.00th=[ 24511], 99.50th=[ 29754], 99.90th=[ 43254], 99.95th=[ 55837],
     | 99.99th=[113771]
   bw (  KiB/s): min= 3584, max=131384, per=100.00%, avg=7555.03, stdev=17441.89, samples=119
   iops        : min=  896, max=32846, avg=1888.76, stdev=4360.47, samples=119
  lat (usec)   : 250=44.59%, 500=11.54%, 750=0.51%, 1000=0.01%
  lat (msec)   : 2=0.49%, 4=2.06%, 10=25.65%, 20=13.22%, 50=1.88%
  lat (msec)   : 100=0.04%, 250=0.02%
  cpu          : usr=2.17%, sys=4.86%, ctx=77755, majf=0, minf=19
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=112976,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
   READ: bw=7530KiB/s (7711kB/s), 7530KiB/s-7530KiB/s (7711kB/s-7711kB/s), io=441MiB (463MB), run=60010-60010msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randread:  read: IOPS=1882, BW=7530KiB/s (7711kB/s)(441MiB/60010msec)


===Fio: workload=randread, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 132794/160000 hits, 588 MiB read, 2.892 read amp, 3765491 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 123802/160000 hits, 588 MiB read, 3.843 read amp, 3816485 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 89854/160000 hits, 588 MiB read, 7.447 read amp, 3866074 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22460: Mon Jan 15 04:02:54 2024
  read: IOPS=4141, BW=16.2MiB/s (17.0MB/s)(971MiB/60010msec)
    slat (usec): min=4, max=455, avg=11.45, stdev= 7.39
    clat (usec): min=93, max=176558, avg=3848.12, stdev=6175.83
     lat (usec): min=100, max=176565, avg=3859.92, stdev=6177.12
    clat percentiles (usec):
     |  1.00th=[   112],  5.00th=[   131], 10.00th=[   174], 20.00th=[   245],
     | 30.00th=[   293], 40.00th=[   330], 50.00th=[   375], 60.00th=[   478],
     | 70.00th=[  5669], 80.00th=[  8356], 90.00th=[ 10945], 95.00th=[ 13829],
     | 99.00th=[ 24249], 99.50th=[ 29492], 99.90th=[ 47973], 99.95th=[ 80217],
     | 99.99th=[137364]
   bw (  KiB/s): min= 7456, max=188504, per=100.00%, avg=16634.08, stdev=34624.54, samples=119
   iops        : min= 1864, max=47126, avg=4158.52, stdev=8656.14, samples=119
  lat (usec)   : 100=0.05%, 250=21.00%, 500=39.79%, 750=1.25%, 1000=0.11%
  lat (msec)   : 2=0.43%, 4=1.97%, 10=21.64%, 20=12.04%, 50=1.64%
  lat (msec)   : 100=0.06%, 250=0.03%
  cpu          : usr=3.11%, sys=7.35%, ctx=125847, majf=0, minf=27
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=248536,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=16.2MiB/s (17.0MB/s), 16.2MiB/s-16.2MiB/s (17.0MB/s-17.0MB/s), io=971MiB (1018MB), run=60010-60010msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randread:  read: IOPS=4141, BW=16.2MiB/s (17.0MB/s)(971MiB/60010msec)


===Fio: workload=randread, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 93017/160000 hits, 588 MiB read, 7.111 read amp, 3969134 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 111280/160000 hits, 588 MiB read, 5.175 read amp, 4242132 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52741/160000 hits, 588 MiB read, 11.392 read amp, 4322068 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22507: Mon Jan 15 04:03:56 2024
  read: IOPS=7665, BW=29.9MiB/s (31.4MB/s)(1797MiB/60027msec)
    slat (usec): min=4, max=454, avg=10.66, stdev= 6.97
    clat (usec): min=87, max=222457, avg=4160.58, stdev=7339.50
     lat (usec): min=94, max=222484, avg=4171.60, stdev=7340.70
    clat percentiles (usec):
     |  1.00th=[   115],  5.00th=[   149], 10.00th=[   231], 20.00th=[   412],
     | 30.00th=[   486], 40.00th=[   562], 50.00th=[   627], 60.00th=[   685],
     | 70.00th=[  4752], 80.00th=[  8356], 90.00th=[ 11731], 95.00th=[ 17171],
     | 99.00th=[ 31589], 99.50th=[ 39060], 99.90th=[ 67634], 99.95th=[ 91751],
     | 99.99th=[135267]
   bw (  KiB/s): min=12752, max=232104, per=100.00%, avg=30815.39, stdev=54949.94, samples=119
   iops        : min= 3188, max=58026, avg=7703.85, stdev=13737.48, samples=119
  lat (usec)   : 100=0.08%, 250=10.30%, 500=21.91%, 750=32.14%, 1000=1.97%
  lat (msec)   : 2=0.46%, 4=1.42%, 10=16.63%, 20=11.71%, 50=3.15%
  lat (msec)   : 100=0.18%, 250=0.04%
  cpu          : usr=4.65%, sys=11.18%, ctx=177710, majf=0, minf=43
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=460127,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=29.9MiB/s (31.4MB/s), 29.9MiB/s-29.9MiB/s (31.4MB/s-31.4MB/s), io=1797MiB (1885MB), run=60027-60027msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randread:  read: IOPS=7665, BW=29.9MiB/s (31.4MB/s)(1797MiB/60027msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 42885/160000 hits, 588 MiB read, 12.435 read amp, 4395461 total
[0m

===Fio: workload=randread, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 38531/160000 hits, 588 MiB read, 12.896 read amp, 4489836 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 37014/160000 hits, 589 MiB read, 13.050 read amp, 4592327 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 43277/160000 hits, 588 MiB read, 12.394 read amp, 4698741 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22554: Mon Jan 15 04:04:59 2024
  read: IOPS=4869, BW=19.0MiB/s (19.9MB/s)(1143MiB/60090msec)
    slat (usec): min=4, max=463, avg=10.75, stdev= 7.42
    clat (usec): min=74, max=468374, avg=13127.96, stdev=21072.90
     lat (usec): min=98, max=468380, avg=13139.02, stdev=21072.89
    clat percentiles (usec):
     |  1.00th=[   108],  5.00th=[   122], 10.00th=[   149], 20.00th=[   239],
     | 30.00th=[  4686], 40.00th=[  6718], 50.00th=[  8586], 60.00th=[ 10421],
     | 70.00th=[ 12518], 80.00th=[ 17433], 90.00th=[ 27919], 95.00th=[ 42730],
     | 99.00th=[102237], 99.50th=[137364], 99.90th=[248513], 99.95th=[283116],
     | 99.99th=[367002]
   bw (  KiB/s): min=16024, max=23576, per=100.00%, avg=19503.60, stdev=1310.70, samples=120
   iops        : min= 4006, max= 5894, avg=4875.90, stdev=327.68, samples=120
  lat (usec)   : 100=0.10%, 250=20.24%, 500=3.17%, 750=0.37%, 1000=0.02%
  lat (msec)   : 2=0.56%, 4=2.68%, 10=30.70%, 20=25.91%, 50=12.42%
  lat (msec)   : 100=2.79%, 250=0.95%, 500=0.10%
  cpu          : usr=3.62%, sys=8.70%, ctx=221178, majf=0, minf=75
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=292617,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=19.0MiB/s (19.9MB/s), 19.0MiB/s-19.0MiB/s (19.9MB/s-19.9MB/s), io=1143MiB (1199MB), run=60090-60090msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randread:  read: IOPS=4869, BW=19.0MiB/s (19.9MB/s)(1143MiB/60090msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 111117/160000 hits, 588 MiB read, 5.192 read amp, 5089314 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 44307/160000 hits, 588 MiB read, 12.285 read amp, 5197745 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 41340/160000 hits, 588 MiB read, 12.607 read amp, 5302545 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22606: Mon Jan 15 04:06:02 2024
  read: IOPS=9603, BW=37.5MiB/s (39.3MB/s)(2270MiB/60497msec)
    slat (usec): min=4, max=2890, avg=92.11, stdev=194.65
    clat (usec): min=63, max=1401.1k, avg=13231.10, stdev=67399.49
     lat (usec): min=102, max=1401.8k, avg=13323.69, stdev=67412.99
    clat percentiles (usec):
     |  1.00th=[    122],  5.00th=[    157], 10.00th=[    229],
     | 20.00th=[   1844], 30.00th=[   2180], 40.00th=[   2245],
     | 50.00th=[   2343], 60.00th=[   2474], 70.00th=[   6390],
     | 80.00th=[  10421], 90.00th=[  18482], 95.00th=[  32113],
     | 99.00th=[ 181404], 99.50th=[ 549454], 99.90th=[1035994],
     | 99.95th=[1132463], 99.99th=[1300235]
   bw (  KiB/s): min=16072, max=237192, per=100.00%, avg=38725.33, stdev=58968.43, samples=120
   iops        : min= 4018, max=59298, avg=9681.33, stdev=14742.11, samples=120
  lat (usec)   : 100=0.01%, 250=10.65%, 500=2.16%, 750=0.29%, 1000=0.02%
  lat (msec)   : 2=9.42%, 4=42.36%, 10=13.98%, 20=12.05%, 50=6.10%
  lat (msec)   : 100=1.38%, 250=0.76%, 500=0.25%, 750=0.24%, 1000=0.19%
  lat (msec)   : 2000=0.13%
  cpu          : usr=5.62%, sys=12.86%, ctx=233743, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=581007,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=37.5MiB/s (39.3MB/s), 37.5MiB/s-37.5MiB/s (39.3MB/s-39.3MB/s), io=2270MiB (2380MB), run=60497-60497msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=9603, BW=37.5MiB/s (39.3MB/s)(2270MiB/60497msec)


===Fio: workload=randread, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 40894/160000 hits, 588 MiB read, 12.656 read amp, 5398312 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 40135/160000 hits, 588 MiB read, 12.723 read amp, 5507796 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 48772/160000 hits, 588 MiB read, 11.807 read amp, 5621895 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22654: Mon Jan 15 04:07:04 2024
  read: IOPS=5271, BW=20.6MiB/s (21.6MB/s)(1245MiB/60463msec)
    slat (usec): min=4, max=5597, avg=184.60, stdev=261.71
    clat (usec): min=104, max=1470.4k, avg=48371.70, stdev=90115.19
     lat (usec): min=111, max=1470.4k, avg=48556.82, stdev=90116.21
    clat percentiles (msec):
     |  1.00th=[   17],  5.00th=[   22], 10.00th=[   23], 20.00th=[   26],
     | 30.00th=[   28], 40.00th=[   31], 50.00th=[   33], 60.00th=[   35],
     | 70.00th=[   39], 80.00th=[   44], 90.00th=[   59], 95.00th=[   92],
     | 99.00th=[  527], 99.50th=[  860], 99.90th=[ 1062], 99.95th=[ 1116],
     | 99.99th=[ 1183]
   bw (  KiB/s): min=17424, max=29024, per=100.00%, avg=21230.27, stdev=1574.65, samples=120
   iops        : min= 4356, max= 7256, avg=5307.55, stdev=393.67, samples=120
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.04%, 4=0.10%, 10=0.25%, 20=2.47%, 50=83.10%
  lat (msec)   : 100=9.53%, 250=2.71%, 500=0.77%, 750=0.30%, 1000=0.54%
  lat (msec)   : 2000=0.19%
  cpu          : usr=3.43%, sys=8.51%, ctx=248901, majf=0, minf=267
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=318709,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=20.6MiB/s (21.6MB/s), 20.6MiB/s-20.6MiB/s (21.6MB/s-21.6MB/s), io=1245MiB (1305MB), run=60463-60463msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randread:  read: IOPS=5271, BW=20.6MiB/s (21.6MB/s)(1245MiB/60463msec)


===Fio: workload=randread, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 139806/160000 hits, 588 MiB read, 2.145 read amp, 6029238 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 57307/160000 hits, 588 MiB read, 10.901 read amp, 6141920 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 41814/160000 hits, 588 MiB read, 12.558 read amp, 6248861 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22702: Mon Jan 15 04:08:07 2024
  read: IOPS=9871, BW=38.6MiB/s (40.4MB/s)(2334MiB/60533msec)
    slat (usec): min=4, max=7104, avg=96.95, stdev=207.00
    clat (usec): min=134, max=1832.6k, avg=51752.66, stdev=77129.63
     lat (usec): min=150, max=1832.7k, avg=51850.05, stdev=77181.45
    clat percentiles (msec):
     |  1.00th=[    9],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   11], 60.00th=[   72],
     | 70.00th=[   80], 80.00th=[   87], 90.00th=[   97], 95.00th=[  110],
     | 99.00th=[  292], 99.50th=[  575], 99.90th=[ 1070], 99.95th=[ 1150],
     | 99.99th=[ 1250]
   bw (  KiB/s): min=16224, max=220392, per=100.00%, avg=39804.20, stdev=57340.57, samples=120
   iops        : min= 4056, max=55098, avg=9951.05, stdev=14335.14, samples=120
  lat (usec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=41.16%, 20=11.54%, 50=0.96%
  lat (msec)   : 100=38.14%, 250=6.99%, 500=0.58%, 750=0.30%, 1000=0.18%
  lat (msec)   : 2000=0.13%
  cpu          : usr=5.21%, sys=12.48%, ctx=228201, majf=0, minf=523
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=597574,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
   READ: bw=38.6MiB/s (40.4MB/s), 38.6MiB/s-38.6MiB/s (40.4MB/s-40.4MB/s), io=2334MiB (2448MB), run=60533-60533msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randread:  read: IOPS=9871, BW=38.6MiB/s (40.4MB/s)(2334MiB/60533msec)


===Fio: workload=read, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 108334/160000 hits, 588 MiB read, 5.492 read amp, 6394521 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152861/160000 hits, 588 MiB read, 0.759 read amp, 6780198 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152809/160000 hits, 588 MiB read, 0.764 read amp, 7174870 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22751: Mon Jan 15 04:09:10 2024
  read: IOPS=17.7k, BW=69.3MiB/s (72.7MB/s)(4159MiB/60002msec)
    slat (usec): min=4, max=675, avg= 6.77, stdev= 5.96
    clat (usec): min=95, max=128213, avg=1794.87, stdev=3587.68
     lat (usec): min=110, max=128227, avg=1801.85, stdev=3587.97
    clat percentiles (usec):
     |  1.00th=[   186],  5.00th=[   255], 10.00th=[   302], 20.00th=[   465],
     | 30.00th=[  1221], 40.00th=[  1369], 50.00th=[  1483], 60.00th=[  1614],
     | 70.00th=[  1795], 80.00th=[  2057], 90.00th=[  2671], 95.00th=[  3326],
     | 99.00th=[ 13304], 99.50th=[ 19006], 99.90th=[ 52167], 99.95th=[ 93848],
     | 99.99th=[119014]
   bw (  KiB/s): min= 2048, max=93824, per=99.99%, avg=70973.18, stdev=19174.09, samples=119
   iops        : min=  512, max=23456, avg=17743.31, stdev=4793.53, samples=119
  lat (usec)   : 100=0.01%, 250=4.69%, 500=16.15%, 750=2.81%, 1000=0.80%
  lat (msec)   : 2=53.99%, 4=18.58%, 10=1.55%, 20=1.01%, 50=0.30%
  lat (msec)   : 100=0.07%, 250=0.03%
  cpu          : usr=5.06%, sys=13.64%, ctx=97133, majf=0, minf=44
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=1064719,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: bw=69.3MiB/s (72.7MB/s), 69.3MiB/s-69.3MiB/s (72.7MB/s-72.7MB/s), io=4159MiB (4361MB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) read:  read: IOPS=17.7k, BW=69.3MiB/s (72.7MB/s)(4159MiB/60002msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 588 MiB read, 0.000 read amp, 7786297 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152545/160000 hits, 588 MiB read, 0.792 read amp, 8722035 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152457/160000 hits, 588 MiB read, 0.801 read amp, 9148023 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22797: Mon Jan 15 04:10:12 2024
  read: IOPS=32.0k, BW=125MiB/s (131MB/s)(7500MiB/60007msec)
    slat (usec): min=4, max=150910, avg=28.78, stdev=279.04
    clat (usec): min=101, max=158704, avg=3969.38, stdev=4108.24
     lat (usec): min=124, max=158712, avg=3998.48, stdev=4128.33
    clat percentiles (usec):
     |  1.00th=[   227],  5.00th=[   351], 10.00th=[  1303], 20.00th=[  1893],
     | 30.00th=[  2040], 40.00th=[  2147], 50.00th=[  2245], 60.00th=[  2409],
     | 70.00th=[  5276], 80.00th=[  6652], 90.00th=[  8455], 95.00th=[ 10028],
     | 99.00th=[ 17695], 99.50th=[ 21627], 99.90th=[ 32113], 99.95th=[ 51643],
     | 99.99th=[147850]
   bw (  KiB/s): min=56120, max=265816, per=100.00%, avg=128454.72, stdev=75625.44, samples=119
   iops        : min=14030, max=66454, avg=32113.70, stdev=18906.39, samples=119
  lat (usec)   : 250=1.55%, 500=5.66%, 750=1.15%, 1000=0.23%
  lat (msec)   : 2=18.27%, 4=38.73%, 10=29.34%, 20=4.41%, 50=0.61%
  lat (msec)   : 100=0.04%, 250=0.02%
  cpu          : usr=10.04%, sys=27.20%, ctx=106310, majf=0, minf=140
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=1920095,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=125MiB/s (131MB/s), 125MiB/s-125MiB/s (131MB/s-131MB/s), io=7500MiB (7865MB), run=60007-60007msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=32.0k, BW=125MiB/s (131MB/s)(7500MiB/60007msec)


===Fio: workload=randread, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 138804/160000 hits, 588 MiB read, 2.252 read amp, 9488020 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 58855/160000 hits, 588 MiB read, 10.742 read amp, 9614880 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 48431/160000 hits, 588 MiB read, 11.843 read amp, 9733454 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22845: Mon Jan 15 04:11:15 2024
  read: IOPS=5556, BW=21.7MiB/s (22.8MB/s)(1311MiB/60399msec)
    slat (usec): min=4, max=3378, avg=147.32, stdev=221.19
    clat (usec): min=83, max=1632.3k, avg=22883.70, stdev=85820.19
     lat (usec): min=97, max=1632.5k, avg=23031.68, stdev=85820.12
    clat percentiles (usec):
     |  1.00th=[    118],  5.00th=[    135], 10.00th=[    157],
     | 20.00th=[    223], 30.00th=[   3785], 40.00th=[   6325],
     | 50.00th=[   8455], 60.00th=[  10552], 70.00th=[  13304],
     | 80.00th=[  19268], 90.00th=[  34866], 95.00th=[  62129],
     | 99.00th=[ 455082], 99.50th=[ 775947], 99.90th=[1098908],
     | 99.95th=[1266680], 99.99th=[1350566]
   bw (  KiB/s): min=16528, max=27120, per=100.00%, avg=22363.53, stdev=2177.83, samples=120
   iops        : min= 4132, max= 6780, avg=5590.88, stdev=544.46, samples=120
  lat (usec)   : 100=0.02%, 250=21.89%, 500=5.16%, 750=0.60%, 1000=0.05%
  lat (msec)   : 2=0.52%, 4=2.36%, 10=26.76%, 20=23.47%, 50=12.70%
  lat (msec)   : 100=3.49%, 250=1.68%, 500=0.34%, 750=0.39%, 1000=0.41%
  lat (msec)   : 2000=0.15%
  cpu          : usr=4.86%, sys=10.67%, ctx=270705, majf=0, minf=139
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=335580,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=21.7MiB/s (22.8MB/s), 21.7MiB/s-21.7MiB/s (22.8MB/s-22.8MB/s), io=1311MiB (1375MB), run=60399-60399msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randread:  read: IOPS=5556, BW=21.7MiB/s (22.8MB/s)(1311MiB/60399msec)


===Fio: workload=randread, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 40385/160000 hits, 597 MiB read, 12.511 read amp, 9826866 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 67512/160000 hits, 1081 MiB read, 5.343 read amp, 9974992 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 131691/160000 hits, 1111 MiB read, 1.592 read amp, 10209511 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 72636/160000 hits, 1111 MiB read, 4.910 read amp, 10321395 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22891: Mon Jan 15 04:12:17 2024
  read: IOPS=7322, BW=57.2MiB/s (60.0MB/s)(3455MiB/60403msec)
    slat (usec): min=4, max=3692, avg=117.81, stdev=202.62
    clat (usec): min=81, max=1534.4k, avg=17356.50, stdev=85915.76
     lat (usec): min=107, max=1535.4k, avg=17474.96, stdev=85921.21
    clat percentiles (usec):
     |  1.00th=[    125],  5.00th=[    145], 10.00th=[    174],
     | 20.00th=[    269], 30.00th=[   1188], 40.00th=[   2769],
     | 50.00th=[   3523], 60.00th=[   6915], 70.00th=[   9896],
     | 80.00th=[  13566], 90.00th=[  25035], 95.00th=[  43254],
     | 99.00th=[ 212861], 99.50th=[ 817890], 99.90th=[1266680],
     | 99.95th=[1333789], 99.99th=[1451230]
   bw (  KiB/s): min=33168, max=319136, per=100.00%, avg=58955.33, stdev=55510.41, samples=120
   iops        : min= 4146, max=39892, avg=7369.42, stdev=6938.80, samples=120
  lat (usec)   : 100=0.01%, 250=19.05%, 500=6.11%, 750=1.80%, 1000=1.63%
  lat (msec)   : 2=7.22%, 4=15.68%, 10=19.12%, 20=16.51%, 50=8.65%
  lat (msec)   : 100=2.29%, 250=1.05%, 500=0.23%, 750=0.12%, 1000=0.19%
  lat (msec)   : 2000=0.35%
  cpu          : usr=5.72%, sys=13.27%, ctx=275433, majf=0, minf=267
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=442292,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=57.2MiB/s (60.0MB/s), 57.2MiB/s-57.2MiB/s (60.0MB/s-60.0MB/s), io=3455MiB (3623MB), run=60403-60403msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randread:  read: IOPS=7322, BW=57.2MiB/s (60.0MB/s)(3455MiB/60403msec)


===Fio: workload=randread, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randread, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 58961/160000 hits, 1787 MiB read, 3.532 read amp, 10442914 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 53870/160000 hits, 2002 MiB read, 3.313 read amp, 10564011 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 52852/160000 hits, 2000 MiB read, 3.347 read amp, 10681417 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22937: Mon Jan 15 04:13:20 2024
  read: IOPS=4966, BW=77.6MiB/s (81.4MB/s)(4689MiB/60422msec)
    slat (usec): min=6, max=3857, avg=185.09, stdev=248.49
    clat (usec): min=74, max=1833.7k, avg=25576.21, stdev=114266.44
     lat (usec): min=130, max=1833.8k, avg=25762.21, stdev=114267.01
    clat percentiles (usec):
     |  1.00th=[    137],  5.00th=[    153], 10.00th=[    174],
     | 20.00th=[    229], 30.00th=[    603], 40.00th=[   5932],
     | 50.00th=[   8225], 60.00th=[  10290], 70.00th=[  13042],
     | 80.00th=[  19530], 90.00th=[  35914], 95.00th=[  63701],
     | 99.00th=[ 438305], 99.50th=[1098908], 99.90th=[1484784],
     | 99.95th=[1568670], 99.99th=[1702888]
   bw (  KiB/s): min=65184, max=120256, per=100.00%, avg=79996.53, stdev=8628.82, samples=120
   iops        : min= 4074, max= 7516, avg=4999.78, stdev=539.30, samples=120
  lat (usec)   : 100=0.01%, 250=22.19%, 500=7.18%, 750=0.86%, 1000=0.06%
  lat (msec)   : 2=0.45%, 4=2.01%, 10=25.72%, 20=22.10%, 50=12.78%
  lat (msec)   : 100=3.67%, 250=1.66%, 500=0.32%, 750=0.15%, 1000=0.23%
  lat (msec)   : 2000=0.59%
  cpu          : usr=5.60%, sys=11.59%, ctx=266988, majf=0, minf=523
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=300114,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=77.6MiB/s (81.4MB/s), 77.6MiB/s-77.6MiB/s (81.4MB/s-81.4MB/s), io=4689MiB (4917MB), run=60422-60422msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randread:  read: IOPS=4966, BW=77.6MiB/s (81.4MB/s)(4689MiB/60422msec)


===Fio: workload=randread, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randread, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 58212/160000 hits, 2814 MiB read, 2.261 read amp, 10794188 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 56194/160000 hits, 3330 MiB read, 1.948 read amp, 10916576 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 54659/160000 hits, 3336 MiB read, 1.973 read amp, 11030379 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=22983: Mon Jan 15 04:14:23 2024
  read: IOPS=3992, BW=125MiB/s (131MB/s)(7567MiB/60655msec)
    slat (usec): min=6, max=9545, avg=201.03, stdev=284.35
    clat (usec): min=102, max=2538.1k, avg=31850.62, stdev=165374.18
     lat (usec): min=142, max=2538.6k, avg=32052.58, stdev=165373.95
    clat percentiles (usec):
     |  1.00th=[    167],  5.00th=[    208], 10.00th=[    251],
     | 20.00th=[    375], 30.00th=[   4424], 40.00th=[   6783],
     | 50.00th=[   8848], 60.00th=[  10814], 70.00th=[  13566],
     | 80.00th=[  20317], 90.00th=[  37487], 95.00th=[  64750],
     | 99.00th=[ 918553], 99.50th=[1652556], 99.90th=[1971323],
     | 99.95th=[2071987], 99.99th=[2231370]
   bw (  KiB/s): min=99968, max=200512, per=100.00%, avg=129076.80, stdev=15370.29, samples=120
   iops        : min= 3124, max= 6266, avg=4033.65, stdev=480.32, samples=120
  lat (usec)   : 250=9.82%, 500=14.09%, 750=2.20%, 1000=0.22%
  lat (msec)   : 2=0.46%, 4=1.83%, 10=27.40%, 20=23.70%, 50=13.31%
  lat (msec)   : 100=4.03%, 250=1.68%, 500=0.22%, 750=0.04%, 1000=0.02%
  lat (msec)   : 2000=0.91%, >=2000=0.08%
  cpu          : usr=4.49%, sys=10.55%, ctx=215983, majf=0, minf=1036
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=242146,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=125MiB/s (131MB/s), 125MiB/s-125MiB/s (131MB/s-131MB/s), io=7567MiB (7935MB), run=60655-60655msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randread:  read: IOPS=3992, BW=125MiB/s (131MB/s)(7567MiB/60655msec)


===Fio: workload=randread, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 63010/160000 hits, 4237 MiB read, 1.431 read amp, 11146005 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 56872/160000 hits, 5000 MiB read, 1.289 read amp, 11268829 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 50608/160000 hits, 4998 MiB read, 1.368 read amp, 11379813 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23029: Mon Jan 15 04:15:26 2024
  read: IOPS=3000, BW=188MiB/s (197MB/s)(11.1GiB/60628msec)
    slat (usec): min=6, max=4681, avg=256.59, stdev=354.03
    clat (usec): min=182, max=2800.0k, avg=42396.28, stdev=217892.77
     lat (usec): min=200, max=2800.0k, avg=42653.62, stdev=217894.72
    clat percentiles (usec):
     |  1.00th=[    273],  5.00th=[    363], 10.00th=[    457],
     | 20.00th=[   4752], 30.00th=[   6783], 40.00th=[   8586],
     | 50.00th=[  10159], 60.00th=[  11863], 70.00th=[  15926],
     | 80.00th=[  23725], 90.00th=[  45351], 95.00th=[  82314],
     | 99.00th=[1635779], 99.50th=[2055209], 99.90th=[2399142],
     | 99.95th=[2533360], 99.99th=[2667578]
   bw (  KiB/s): min=155264, max=478208, per=100.00%, avg=193883.73, stdev=38003.33, samples=120
   iops        : min= 2426, max= 7472, avg=3029.45, stdev=593.80, samples=120
  lat (usec)   : 250=0.46%, 500=10.97%, 750=3.12%, 1000=0.47%
  lat (msec)   : 2=0.56%, 4=1.64%, 10=31.39%, 20=27.66%, 50=14.77%
  lat (msec)   : 100=4.95%, 250=2.41%, 500=0.37%, 750=0.06%, 1000=0.01%
  lat (msec)   : 2000=0.53%, >=2000=0.62%
  cpu          : usr=2.87%, sys=8.66%, ctx=172099, majf=0, minf=2059
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=181894,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=188MiB/s (197MB/s), 188MiB/s-188MiB/s (197MB/s-197MB/s), io=11.1GiB (11.9GB), run=60628-60628msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randread:  read: IOPS=3000, BW=188MiB/s (197MB/s)(11.1GiB/60628msec)


===Fio: workload=read, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 97157/160000 hits, 2873 MiB read, 1.367 read amp, 11500385 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 153019/160000 hits, 588 MiB read, 0.742 read amp, 11757730 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 152871/160000 hits, 588 MiB read, 0.757 read amp, 11968751 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23079: Mon Jan 15 04:16:29 2024
  read: IOPS=11.7k, BW=45.5MiB/s (47.7MB/s)(2731MiB/60014msec)
    slat (usec): min=4, max=100472, avg=82.58, stdev=1134.98
    clat (usec): min=111, max=388977, avg=10900.33, stdev=24386.35
     lat (usec): min=125, max=388998, avg=10983.31, stdev=24549.36
    clat percentiles (usec):
     |  1.00th=[   227],  5.00th=[   306], 10.00th=[   367], 20.00th=[   627],
     | 30.00th=[  3818], 40.00th=[  5407], 50.00th=[  6325], 60.00th=[  7308],
     | 70.00th=[  8356], 80.00th=[ 10159], 90.00th=[ 16581], 95.00th=[ 24249],
     | 99.00th=[152044], 99.50th=[168821], 99.90th=[217056], 99.95th=[248513],
     | 99.99th=[316670]
   bw (  KiB/s): min= 3320, max=91120, per=99.77%, avg=46497.34, stdev=31891.34, samples=119
   iops        : min=  830, max=22780, avg=11624.37, stdev=7972.86, samples=119
  lat (usec)   : 250=1.75%, 500=14.36%, 750=6.28%, 1000=1.82%
  lat (msec)   : 2=2.49%, 4=3.64%, 10=49.09%, 20=13.58%, 50=3.09%
  lat (msec)   : 100=1.62%, 250=2.25%, 500=0.05%
  cpu          : usr=4.31%, sys=11.82%, ctx=56709, majf=0, minf=140
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=699260,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=45.5MiB/s (47.7MB/s), 45.5MiB/s-45.5MiB/s (47.7MB/s-47.7MB/s), io=2731MiB (2864MB), run=60014-60014msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) read:  read: IOPS=11.7k, BW=45.5MiB/s (47.7MB/s)(2731MiB/60014msec)


===Fio: workload=read, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=read, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 1111 MiB read, 0.000 read amp, 12449445 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 146076/160000 hits, 1111 MiB read, 0.783 read amp, 12766865 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 145737/160000 hits, 1111 MiB read, 0.802 read amp, 13005720 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23128: Mon Jan 15 04:17:31 2024
  read: IOPS=14.9k, BW=116MiB/s (122MB/s)(6965MiB/60023msec)
    slat (usec): min=4, max=136048, avg=64.15, stdev=379.07
    clat (usec): min=95, max=152398, avg=8551.05, stdev=7695.29
     lat (usec): min=127, max=152406, avg=8615.61, stdev=7735.96
    clat percentiles (usec):
     |  1.00th=[   176],  5.00th=[   273], 10.00th=[   506], 20.00th=[  2933],
     | 30.00th=[  3163], 40.00th=[  3326], 50.00th=[  3621], 60.00th=[ 10552],
     | 70.00th=[ 12780], 80.00th=[ 15270], 90.00th=[ 18744], 95.00th=[ 21890],
     | 99.00th=[ 29492], 99.50th=[ 32113], 99.90th=[ 40109], 99.95th=[ 48497],
     | 99.99th=[149947]
   bw (  KiB/s): min=63232, max=332816, per=100.00%, avg=119211.03, stdev=82657.15, samples=119
   iops        : min= 7904, max=41602, avg=14901.38, stdev=10332.14, samples=119
  lat (usec)   : 100=0.01%, 250=4.11%, 500=5.84%, 750=1.77%, 1000=0.30%
  lat (msec)   : 2=1.52%, 4=38.79%, 10=5.40%, 20=34.68%, 50=7.55%
  lat (msec)   : 100=0.03%, 250=0.02%
  cpu          : usr=6.64%, sys=17.15%, ctx=124454, majf=0, minf=268
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=891468,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=116MiB/s (122MB/s), 116MiB/s-116MiB/s (122MB/s-122MB/s), io=6965MiB (7303MB), run=60023-60023msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) read:  read: IOPS=14.9k, BW=116MiB/s (122MB/s)(6965MiB/60023msec)


===Fio: workload=read, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 160000/160000 hits, 2000 MiB read, 0.000 read amp, 13349183 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 144783/160000 hits, 2000 MiB read, 0.476 read amp, 13819256 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 133171/160000 hits, 2000 MiB read, 0.838 read amp, 13970587 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23178: Mon Jan 15 04:18:33 2024
  read: IOPS=12.3k, BW=192MiB/s (202MB/s)(11.3GiB/60027msec)
    slat (usec): min=4, max=59426, avg=77.37, stdev=317.66
    clat (usec): min=105, max=342547, avg=10313.77, stdev=12114.76
     lat (usec): min=137, max=343584, avg=10391.67, stdev=12187.27
    clat percentiles (usec):
     |  1.00th=[   182],  5.00th=[   474], 10.00th=[  2638], 20.00th=[  2737],
     | 30.00th=[  2933], 40.00th=[  3163], 50.00th=[  4113], 60.00th=[  4490],
     | 70.00th=[ 12125], 80.00th=[ 22152], 90.00th=[ 29492], 95.00th=[ 34866],
     | 99.00th=[ 41681], 99.50th=[ 44827], 99.90th=[ 52167], 99.95th=[ 59507],
     | 99.99th=[250610]
   bw (  KiB/s): min=53888, max=630016, per=100.00%, avg=197938.29, stdev=198952.13, samples=119
   iops        : min= 3368, max=39376, avg=12371.14, stdev=12434.51, samples=119
  lat (usec)   : 250=2.42%, 500=2.74%, 750=0.56%, 1000=0.11%
  lat (msec)   : 2=0.72%, 4=42.55%, 10=19.91%, 20=7.48%, 50=23.33%
  lat (msec)   : 100=0.13%, 250=0.02%, 500=0.01%
  cpu          : usr=5.46%, sys=17.26%, ctx=262072, majf=0, minf=524
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=739173,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=192MiB/s (202MB/s), 192MiB/s-192MiB/s (202MB/s-202MB/s), io=11.3GiB (12.1GB), run=60027-60027msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) read:  read: IOPS=12.3k, BW=192MiB/s (202MB/s)(11.3GiB/60027msec)


===Fio: workload=read, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 142688/160000 hits, 2499 MiB read, 0.433 read amp, 14153080 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 147777/160000 hits, 3333 MiB read, 0.229 read amp, 14688964 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 112093/160000 hits, 3333 MiB read, 0.898 read amp, 14815516 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23227: Mon Jan 15 04:19:36 2024
  read: IOPS=9313, BW=291MiB/s (305MB/s)(17.1GiB/60029msec)
    slat (usec): min=6, max=123463, avg=102.67, stdev=285.81
    clat (usec): min=129, max=194401, avg=13637.02, stdev=16116.18
     lat (usec): min=162, max=194560, avg=13740.32, stdev=16201.28
    clat percentiles (usec):
     |  1.00th=[   285],  5.00th=[  4424], 10.00th=[  4686], 20.00th=[  4817],
     | 30.00th=[  4948], 40.00th=[  5080], 50.00th=[  5276], 60.00th=[  5538],
     | 70.00th=[  8094], 80.00th=[ 24773], 90.00th=[ 39060], 95.00th=[ 49021],
     | 99.00th=[ 66847], 99.50th=[ 69731], 99.90th=[ 82314], 99.95th=[103285],
     | 99.99th=[181404]
   bw (  KiB/s): min=84864, max=813696, per=100.00%, avg=299565.71, stdev=284414.30, samples=119
   iops        : min= 2652, max=25428, avg=9361.45, stdev=8887.98, samples=119
  lat (usec)   : 250=0.68%, 500=1.29%, 750=0.21%, 1000=0.04%
  lat (msec)   : 2=0.47%, 4=0.98%, 10=67.06%, 20=5.88%, 50=18.69%
  lat (msec)   : 100=4.65%, 250=0.06%
  cpu          : usr=5.85%, sys=18.23%, ctx=373881, majf=0, minf=1036
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=559067,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=291MiB/s (305MB/s), 291MiB/s-291MiB/s (305MB/s-305MB/s), io=17.1GiB (18.3GB), run=60029-60029msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) read:  read: IOPS=9313, BW=291MiB/s (305MB/s)(17.1GiB/60029msec)
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 110134/160000 hits, 3333 MiB read, 0.935 read amp, 14931707 total
[0m

===Fio: workload=read, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=read, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 158704/160000 hits, 5000 MiB read, 0.016 read amp, 15493455 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 98115/160000 hits, 5000 MiB read, 0.774 read amp, 15615801 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79376/160000 hits, 5000 MiB read, 1.008 read amp, 15738491 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23273: Mon Jan 15 04:20:38 2024
  read: IOPS=6753, BW=422MiB/s (443MB/s)(24.8GiB/60057msec)
    slat (usec): min=6, max=25710, avg=141.99, stdev=215.58
    clat (usec): min=216, max=204313, avg=18803.55, stdev=19922.27
     lat (usec): min=322, max=204775, avg=18946.33, stdev=20026.44
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    8], 10.00th=[    8], 20.00th=[    9],
     | 30.00th=[    9], 40.00th=[    9], 50.00th=[    9], 60.00th=[   10],
     | 70.00th=[   12], 80.00th=[   32], 90.00th=[   50], 95.00th=[   63],
     | 99.00th=[   90], 99.50th=[  101], 99.90th=[  130], 99.95th=[  133],
     | 99.99th=[  142]
   bw (  KiB/s): min=146432, max=983296, per=100.00%, avg=432518.40, stdev=349724.10, samples=120
   iops        : min= 2288, max=15364, avg=6758.12, stdev=5464.43, samples=120
  lat (usec)   : 250=0.01%, 500=0.04%, 750=0.02%, 1000=0.01%
  lat (msec)   : 2=0.06%, 4=0.80%, 10=66.87%, 20=6.27%, 50=16.33%
  lat (msec)   : 100=9.11%, 250=0.50%
  cpu          : usr=5.03%, sys=16.25%, ctx=399379, majf=0, minf=2059
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=405614,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=422MiB/s (443MB/s), 422MiB/s-422MiB/s (443MB/s-443MB/s), io=24.8GiB (26.6GB), run=60057-60057msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) read:  read: IOPS=6753, BW=422MiB/s (443MB/s)(24.8GiB/60057msec)


===Fio: workload=randwrite, time=60, iodepth=1, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=1
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79394/160000 hits, 4999 MiB read, 1.008 read amp, 15743003 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23323: Mon Jan 15 04:21:41 2024
  write: IOPS=5682, BW=22.2MiB/s (23.3MB/s)(1332MiB/60001msec); 0 zone resets
    slat (usec): min=9, max=466, avg=15.15, stdev= 8.29
    clat (usec): min=62, max=8751, avg=156.04, stdev=62.18
     lat (usec): min=123, max=8762, avg=171.71, stdev=63.52
    clat percentiles (usec):
     |  1.00th=[  120],  5.00th=[  124], 10.00th=[  127], 20.00th=[  131],
     | 30.00th=[  135], 40.00th=[  139], 50.00th=[  145], 60.00th=[  149],
     | 70.00th=[  155], 80.00th=[  161], 90.00th=[  174], 95.00th=[  247],
     | 99.00th=[  396], 99.50th=[  453], 99.90th=[  603], 99.95th=[  857],
     | 99.99th=[ 1827]
   bw (  KiB/s): min=19760, max=25912, per=100.00%, avg=22742.59, stdev=1295.41, samples=119
   iops        : min= 4940, max= 6478, avg=5685.66, stdev=323.82, samples=119
  lat (usec)   : 100=0.01%, 250=95.11%, 500=4.55%, 750=0.28%, 1000=0.02%
  lat (msec)   : 2=0.03%, 4=0.01%, 10=0.01%
  cpu          : usr=5.44%, sys=13.53%, ctx=340982, majf=0, minf=10
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,340979,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=22.2MiB/s (23.3MB/s), 22.2MiB/s-22.2MiB/s (23.3MB/s-23.3MB/s), io=1332MiB (1397MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5682, BW=22.2MiB/s (23.3MB/s)(1332MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=4, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=4
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79463/160000 hits, 4997 MiB read, 1.007 read amp, 15743139 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23378: Mon Jan 15 04:22:43 2024
  write: IOPS=17.0k, BW=66.4MiB/s (69.7MB/s)(3987MiB/60001msec); 0 zone resets
    slat (usec): min=5, max=484, avg=14.48, stdev= 8.95
    clat (usec): min=59, max=6304, avg=216.41, stdev=96.21
     lat (usec): min=115, max=6314, avg=231.35, stdev=96.37
    clat percentiles (usec):
     |  1.00th=[  124],  5.00th=[  139], 10.00th=[  151], 20.00th=[  165],
     | 30.00th=[  180], 40.00th=[  190], 50.00th=[  200], 60.00th=[  212],
     | 70.00th=[  227], 80.00th=[  247], 90.00th=[  297], 95.00th=[  343],
     | 99.00th=[  465], 99.50th=[  586], 99.90th=[ 1516], 99.95th=[ 1827],
     | 99.99th=[ 2343]
   bw (  KiB/s): min=59672, max=77792, per=100.00%, avg=68070.66, stdev=3634.77, samples=119
   iops        : min=14918, max=19448, avg=17017.65, stdev=908.66, samples=119
  lat (usec)   : 100=0.05%, 250=81.09%, 500=18.06%, 750=0.54%, 1000=0.04%
  lat (msec)   : 2=0.18%, 4=0.02%, 10=0.01%
  cpu          : usr=12.83%, sys=30.39%, ctx=474120, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1020628,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
  WRITE: bw=66.4MiB/s (69.7MB/s), 66.4MiB/s-66.4MiB/s (69.7MB/s-69.7MB/s), io=3987MiB (4180MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=17.0k, BW=66.4MiB/s (69.7MB/s)(3987MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=8, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=8
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79539/160000 hits, 4994 MiB read, 1.007 read amp, 15743291 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23431: Mon Jan 15 04:23:45 2024
  write: IOPS=27.2k, BW=106MiB/s (111MB/s)(6378MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=532, avg=11.96, stdev= 9.53
    clat (usec): min=94, max=7001, avg=278.92, stdev=125.32
     lat (usec): min=100, max=7019, avg=291.23, stdev=126.15
    clat percentiles (usec):
     |  1.00th=[  137],  5.00th=[  167], 10.00th=[  188], 20.00th=[  212],
     | 30.00th=[  231], 40.00th=[  247], 50.00th=[  262], 60.00th=[  277],
     | 70.00th=[  297], 80.00th=[  326], 90.00th=[  375], 95.00th=[  420],
     | 99.00th=[  594], 99.50th=[  848], 99.90th=[ 1876], 99.95th=[ 2147],
     | 99.99th=[ 2769]
   bw (  KiB/s): min=87000, max=126424, per=100.00%, avg=108848.54, stdev=7714.97, samples=119
   iops        : min=21750, max=31606, avg=27212.18, stdev=1928.76, samples=119
  lat (usec)   : 100=0.01%, 250=42.72%, 500=55.52%, 750=1.19%, 1000=0.12%
  lat (msec)   : 2=0.37%, 4=0.07%, 10=0.01%
  cpu          : usr=14.07%, sys=35.21%, ctx=531141, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,1632786,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=8

Run status group 0 (all jobs):
  WRITE: bw=106MiB/s (111MB/s), 106MiB/s-106MiB/s (111MB/s-111MB/s), io=6378MiB (6688MB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=27.2k, BW=106MiB/s (111MB/s)(6378MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=16, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79619/160000 hits, 4991 MiB read, 1.007 read amp, 15743451 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23488: Mon Jan 15 04:24:48 2024
  write: IOPS=42.2k, BW=165MiB/s (173MB/s)(9889MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=472, avg= 7.60, stdev= 5.41
    clat (usec): min=97, max=57986, avg=369.52, stdev=311.51
     lat (usec): min=108, max=57991, avg=377.36, stdev=311.63
    clat percentiles (usec):
     |  1.00th=[  163],  5.00th=[  210], 10.00th=[  235], 20.00th=[  273],
     | 30.00th=[  302], 40.00th=[  326], 50.00th=[  351], 60.00th=[  375],
     | 70.00th=[  400], 80.00th=[  433], 90.00th=[  482], 95.00th=[  537],
     | 99.00th=[  955], 99.50th=[ 1532], 99.90th=[ 2311], 99.95th=[ 2900],
     | 99.99th=[ 5604]
   bw (  KiB/s): min=114488, max=180664, per=100.00%, avg=168840.20, stdev=10499.02, samples=119
   iops        : min=28622, max=45166, avg=42210.05, stdev=2624.76, samples=119
  lat (usec)   : 100=0.01%, 250=13.94%, 500=78.45%, 750=6.15%, 1000=0.50%
  lat (msec)   : 2=0.80%, 4=0.13%, 10=0.02%, 20=0.01%, 50=0.01%
  lat (msec)   : 100=0.01%
  cpu          : usr=14.91%, sys=33.55%, ctx=372886, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2531568,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
  WRITE: bw=165MiB/s (173MB/s), 165MiB/s-165MiB/s (173MB/s-173MB/s), io=9889MiB (10.4GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=42.2k, BW=165MiB/s (173MB/s)(9889MiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79704/160000 hits, 4988 MiB read, 1.006 read amp, 15743621 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23543: Mon Jan 15 04:25:50 2024
  write: IOPS=48.8k, BW=191MiB/s (200MB/s)(11.2GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=603, avg= 9.10, stdev= 7.53
    clat (usec): min=196, max=175540, avg=643.92, stdev=2036.14
     lat (usec): min=202, max=175547, avg=653.30, stdev=2036.38
    clat percentiles (usec):
     |  1.00th=[  306],  5.00th=[  355], 10.00th=[  392], 20.00th=[  433],
     | 30.00th=[  465], 40.00th=[  494], 50.00th=[  529], 60.00th=[  562],
     | 70.00th=[  594], 80.00th=[  652], 90.00th=[  766], 95.00th=[  963],
     | 99.00th=[ 1795], 99.50th=[ 2073], 99.90th=[30016], 99.95th=[48497],
     | 99.99th=[83362]
   bw (  KiB/s): min=112056, max=241552, per=99.84%, avg=195027.36, stdev=28321.94, samples=119
   iops        : min=28014, max=60388, avg=48756.86, stdev=7080.49, samples=119
  lat (usec)   : 250=0.16%, 500=41.71%, 750=47.52%, 1000=6.53%
  lat (msec)   : 2=3.51%, 4=0.33%, 10=0.07%, 20=0.05%, 50=0.08%
  lat (msec)   : 100=0.04%, 250=0.01%
  cpu          : usr=17.24%, sys=42.42%, ctx=316881, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2930175,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=191MiB/s (200MB/s), 191MiB/s-191MiB/s (200MB/s-200MB/s), io=11.2GiB (12.0GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=48.8k, BW=191MiB/s (200MB/s)(11.2GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=64, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=64
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79797/160000 hits, 4984 MiB read, 1.006 read amp, 15743803 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23600: Mon Jan 15 04:26:52 2024
  write: IOPS=48.8k, BW=191MiB/s (200MB/s)(11.2GiB/60001msec); 0 zone resets
    slat (usec): min=4, max=687, avg= 8.51, stdev= 8.11
    clat (usec): min=385, max=137641, avg=1300.10, stdev=3055.28
     lat (usec): min=391, max=137650, avg=1308.87, stdev=3055.68
    clat percentiles (usec):
     |  1.00th=[   502],  5.00th=[   660], 10.00th=[   742], 20.00th=[   848],
     | 30.00th=[   930], 40.00th=[   996], 50.00th=[  1057], 60.00th=[  1123],
     | 70.00th=[  1205], 80.00th=[  1319], 90.00th=[  1696], 95.00th=[  1975],
     | 99.00th=[  3195], 99.50th=[  5604], 99.90th=[ 54789], 99.95th=[ 68682],
     | 99.99th=[112722]
   bw (  KiB/s): min=93672, max=256664, per=100.00%, avg=195704.87, stdev=31967.85, samples=119
   iops        : min=23418, max=64166, avg=48926.22, stdev=7991.95, samples=119
  lat (usec)   : 500=0.94%, 750=9.79%, 1000=30.09%
  lat (msec)   : 2=54.53%, 4=4.00%, 10=0.22%, 20=0.11%, 50=0.19%
  lat (msec)   : 100=0.11%, 250=0.02%
  cpu          : usr=15.41%, sys=38.90%, ctx=307831, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2929923,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=191MiB/s (200MB/s), 191MiB/s-191MiB/s (200MB/s-200MB/s), io=11.2GiB (12.0GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=48.8k, BW=191MiB/s (200MB/s)(11.2GiB/60001msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79888/160000 hits, 4980 MiB read, 1.005 read amp, 15743984 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23655: Mon Jan 15 04:27:55 2024
  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60002msec); 0 zone resets
    slat (usec): min=4, max=110380, avg=17.56, stdev=315.93
    clat (usec): min=336, max=114575, avg=2530.46, stdev=3540.07
     lat (usec): min=347, max=114584, avg=2548.29, stdev=3553.61
    clat percentiles (usec):
     |  1.00th=[ 1287],  5.00th=[ 1631], 10.00th=[ 1762], 20.00th=[ 1926],
     | 30.00th=[ 2057], 40.00th=[ 2147], 50.00th=[ 2212], 60.00th=[ 2278],
     | 70.00th=[ 2409], 80.00th=[ 2540], 90.00th=[ 2868], 95.00th=[ 3359],
     | 99.00th=[ 5145], 99.50th=[22676], 99.90th=[64750], 99.95th=[79168],
     | 99.99th=[94897]
   bw (  KiB/s): min=112136, max=247152, per=99.98%, avg=200736.40, stdev=19227.28, samples=119
   iops        : min=28034, max=61790, avg=50184.13, stdev=4806.87, samples=119
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.19%
  lat (msec)   : 2=25.17%, 4=72.79%, 10=1.11%, 20=0.20%, 50=0.37%
  lat (msec)   : 100=0.16%, 250=0.01%
  cpu          : usr=15.51%, sys=38.16%, ctx=186792, majf=0, minf=699
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,3011703,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=196MiB/s (206MB/s), 196MiB/s-196MiB/s (206MB/s-206MB/s), io=11.5GiB (12.3GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60002msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=256, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 79980/160000 hits, 4977 MiB read, 1.005 read amp, 15744169 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23709: Mon Jan 15 04:28:57 2024
  write: IOPS=47.1k, BW=184MiB/s (193MB/s)(10.8GiB/60002msec); 0 zone resets
    slat (usec): min=4, max=101430, avg=18.62, stdev=296.21
    clat (usec): min=971, max=105958, avg=5414.44, stdev=4815.04
     lat (usec): min=981, max=105968, avg=5433.36, stdev=4824.78
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[    4], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    5], 50.00th=[    5], 60.00th=[    5],
     | 70.00th=[    6], 80.00th=[    6], 90.00th=[    8], 95.00th=[    8],
     | 99.00th=[   18], 99.50th=[   39], 99.90th=[   80], 99.95th=[   88],
     | 99.99th=[  104]
   bw (  KiB/s): min=109216, max=232952, per=99.96%, avg=188315.97, stdev=26980.40, samples=119
   iops        : min=27304, max=58238, avg=47078.97, stdev=6745.14, samples=119
  lat (usec)   : 1000=0.01%
  lat (msec)   : 2=0.01%, 4=9.60%, 10=88.13%, 20=1.39%, 50=0.52%
  lat (msec)   : 100=0.34%, 250=0.02%
  cpu          : usr=15.91%, sys=44.36%, ctx=314836, majf=0, minf=950
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2825927,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
  WRITE: bw=184MiB/s (193MB/s), 184MiB/s-184MiB/s (193MB/s-193MB/s), io=10.8GiB (11.6GB), run=60002-60002msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=47.1k, BW=184MiB/s (193MB/s)(10.8GiB/60002msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80027/160000 hits, 4975 MiB read, 1.005 read amp, 15744262 total
[0m

===Fio: workload=randwrite, time=60, iodepth=512, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=512
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80074/160000 hits, 4973 MiB read, 1.004 read amp, 15744355 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23766: Mon Jan 15 04:29:59 2024
  write: IOPS=42.4k, BW=166MiB/s (174MB/s)(9936MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=144531, avg=20.74, stdev=218.42
    clat (usec): min=677, max=156631, avg=12053.00, stdev=5777.22
     lat (usec): min=687, max=156640, avg=12074.07, stdev=5784.15
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[    9], 40.00th=[   10], 50.00th=[   11], 60.00th=[   12],
     | 70.00th=[   14], 80.00th=[   15], 90.00th=[   16], 95.00th=[   17],
     | 99.00th=[   33], 99.50th=[   45], 99.90th=[   73], 99.95th=[   81],
     | 99.99th=[  157]
   bw (  KiB/s): min=97608, max=223704, per=99.86%, avg=169344.59, stdev=33046.54, samples=119
   iops        : min=24402, max=55926, avg=42336.14, stdev=8261.63, samples=119
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=47.45%, 20=49.48%, 50=2.68%
  lat (msec)   : 100=0.34%, 250=0.04%
  cpu          : usr=15.13%, sys=54.38%, ctx=559981, majf=0, minf=1173
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2543651,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=512

Run status group 0 (all jobs):
  WRITE: bw=166MiB/s (174MB/s), 166MiB/s-166MiB/s (174MB/s-174MB/s), io=9936MiB (10.4GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=42.4k, BW=166MiB/s (174MB/s)(9936MiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=32, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=32
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80167/160000 hits, 4969 MiB read, 1.004 read amp, 15744540 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23823: Mon Jan 15 04:31:02 2024
  write: IOPS=42.2k, BW=165MiB/s (173MB/s)(9886MiB/60001msec); 0 zone resets
    slat (usec): min=4, max=639, avg= 9.50, stdev= 7.77
    clat (usec): min=184, max=1404.8k, avg=747.15, stdev=11592.28
     lat (usec): min=191, max=1404.8k, avg=756.96, stdev=11592.32
    clat percentiles (usec):
     |  1.00th=[   310],  5.00th=[   363], 10.00th=[   396], 20.00th=[   441],
     | 30.00th=[   482], 40.00th=[   519], 50.00th=[   553], 60.00th=[   594],
     | 70.00th=[   635], 80.00th=[   701], 90.00th=[   799], 95.00th=[   906],
     | 99.00th=[  1188], 99.50th=[  1713], 99.90th=[ 20579], 99.95th=[ 42206],
     | 99.99th=[826278]
   bw (  KiB/s): min= 5576, max=249112, per=100.00%, avg=182130.11, stdev=53149.49, samples=110
   iops        : min= 1394, max=62278, avg=45532.55, stdev=13287.38, samples=110
  lat (usec)   : 250=0.12%, 500=35.24%, 750=50.96%, 1000=11.39%
  lat (msec)   : 2=1.99%, 4=0.13%, 10=0.03%, 20=0.03%, 50=0.06%
  lat (msec)   : 100=0.03%, 250=0.01%, 1000=0.01%, 2000=0.01%
  cpu          : usr=14.07%, sys=38.71%, ctx=273684, majf=0, minf=13
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2530757,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=165MiB/s (173MB/s), 165MiB/s-165MiB/s (173MB/s-173MB/s), io=9886MiB (10.4GB), run=60001-60001msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=32; bs=4ki) write:  write: IOPS=42.2k, BW=165MiB/s (173MB/s)(9886MiB/60001msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80236/160000 hits, 4967 MiB read, 1.004 read amp, 15744676 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23877: Mon Jan 15 04:32:04 2024
  write: IOPS=43.8k, BW=171MiB/s (179MB/s)(10.0GiB/60057msec); 0 zone resets
    slat (usec): min=4, max=1716.6k, avg=20.80, stdev=1801.76
    clat (usec): min=266, max=1718.9k, avg=2899.41, stdev=20299.57
     lat (usec): min=281, max=1718.9k, avg=2920.49, stdev=20379.27
    clat percentiles (usec):
     |  1.00th=[   1319],  5.00th=[   1565], 10.00th=[   1696],
     | 20.00th=[   1844], 30.00th=[   1958], 40.00th=[   2057],
     | 50.00th=[   2147], 60.00th=[   2245], 70.00th=[   2343],
     | 80.00th=[   2507], 90.00th=[   3359], 95.00th=[   3752],
     | 99.00th=[   6849], 99.50th=[  24249], 99.90th=[  80217],
     | 99.95th=[ 104334], 99.99th=[1069548]
   bw (  KiB/s): min=31888, max=272336, per=100.00%, avg=189601.01, stdev=47048.00, samples=111
   iops        : min= 7972, max=68084, avg=47400.23, stdev=11761.99, samples=111
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.22%
  lat (msec)   : 2=33.32%, 4=64.27%, 10=1.42%, 20=0.21%, 50=0.30%
  lat (msec)   : 100=0.20%, 250=0.02%, 750=0.01%, 1000=0.01%, 2000=0.01%
  cpu          : usr=11.92%, sys=37.43%, ctx=308881, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2630842,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=171MiB/s (179MB/s), 171MiB/s-171MiB/s (179MB/s-179MB/s), io=10.0GiB (10.8GB), run=60057-60057msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=43.8k, BW=171MiB/s (179MB/s)(10.0GiB/60057msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80304/160000 hits, 4965 MiB read, 1.003 read amp, 15744812 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23930: Mon Jan 15 04:33:07 2024
  write: IOPS=44.0k, BW=172MiB/s (180MB/s)(10.2GiB/60722msec); 0 zone resets
    slat (usec): min=4, max=1346.0k, avg=20.42, stdev=1701.03
    clat (usec): min=259, max=1348.6k, avg=2889.60, stdev=19163.73
     lat (usec): min=376, max=1348.6k, avg=2910.30, stdev=19238.91
    clat percentiles (usec):
     |  1.00th=[   1418],  5.00th=[   1696], 10.00th=[   1844],
     | 20.00th=[   2008], 30.00th=[   2114], 40.00th=[   2180],
     | 50.00th=[   2245], 60.00th=[   2343], 70.00th=[   2474],
     | 80.00th=[   2638], 90.00th=[   3130], 95.00th=[   3687],
     | 99.00th=[   7177], 99.50th=[  14222], 99.90th=[  68682],
     | 99.95th=[  95945], 99.99th=[1132463]
   bw (  KiB/s): min=18872, max=238096, per=100.00%, avg=187289.82, stdev=42066.55, samples=114
   iops        : min= 4718, max=59524, avg=46822.49, stdev=10516.63, samples=114
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.19%
  lat (msec)   : 2=19.57%, 4=77.35%, 10=2.29%, 20=0.23%, 50=0.20%
  lat (msec)   : 100=0.13%, 250=0.01%, 1000=0.01%, 2000=0.02%
  cpu          : usr=13.84%, sys=33.98%, ctx=249081, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2669010,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=172MiB/s (180MB/s), 172MiB/s-172MiB/s (180MB/s-180MB/s), io=10.2GiB (10.9GB), run=60722-60722msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=44.0k, BW=172MiB/s (180MB/s)(10.2GiB/60722msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80394/160000 hits, 4961 MiB read, 1.003 read amp, 15744987 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=23984: Mon Jan 15 04:34:10 2024
  write: IOPS=24.3k, BW=190MiB/s (199MB/s)(11.1GiB/60014msec); 0 zone resets
    slat (usec): min=4, max=2355.2k, avg=38.48, stdev=4176.13
    clat (usec): min=291, max=2358.7k, avg=5221.46, stdev=46997.70
     lat (usec): min=301, max=2358.7k, avg=5260.25, stdev=47181.92
    clat percentiles (usec):
     |  1.00th=[   1680],  5.00th=[   2089], 10.00th=[   2311],
     | 20.00th=[   2540], 30.00th=[   2671], 40.00th=[   2802],
     | 50.00th=[   2933], 60.00th=[   3064], 70.00th=[   3261],
     | 80.00th=[   3490], 90.00th=[   3949], 95.00th=[   4817],
     | 99.00th=[  45351], 99.50th=[  71828], 99.90th=[ 137364],
     | 99.95th=[1635779], 99.99th=[2122318]
   bw (  KiB/s): min=22576, max=368000, per=100.00%, avg=231719.04, stdev=64130.59, samples=100
   iops        : min= 2822, max=46000, avg=28964.88, stdev=8016.37, samples=100
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.03%
  lat (msec)   : 2=3.72%, 4=86.70%, 10=6.90%, 20=0.67%, 50=1.09%
  lat (msec)   : 100=0.61%, 250=0.21%, 2000=0.04%, >=2000=0.02%
  cpu          : usr=8.86%, sys=21.13%, ctx=129130, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1459779,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=190MiB/s (199MB/s), 190MiB/s-190MiB/s (199MB/s-199MB/s), io=11.1GiB (12.0GB), run=60014-60014msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=24.3k, BW=190MiB/s (199MB/s)(11.1GiB/60014msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=randwrite, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80481/160000 hits, 4958 MiB read, 1.002 read amp, 15745159 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24037: Mon Jan 15 04:35:12 2024
  write: IOPS=14.7k, BW=229MiB/s (240MB/s)(13.4GiB/60005msec); 0 zone resets
    slat (usec): min=4, max=1768.1k, avg=65.58, stdev=3890.57
    clat (usec): min=483, max=1774.6k, avg=8656.04, stdev=43767.56
     lat (usec): min=502, max=1774.6k, avg=8721.93, stdev=43938.99
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    6], 50.00th=[    6], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    6], 90.00th=[    7], 95.00th=[   19],
     | 99.00th=[   63], 99.50th=[   79], 99.90th=[  793], 99.95th=[ 1385],
     | 99.99th=[ 1770]
   bw (  KiB/s): min= 7488, max=392960, per=100.00%, avg=261647.55, stdev=63184.04, samples=107
   iops        : min=  468, max=24560, avg=16352.97, stdev=3949.00, samples=107
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.02%
  lat (msec)   : 2=0.11%, 4=0.31%, 10=93.16%, 20=1.90%, 50=2.82%
  lat (msec)   : 100=1.41%, 250=0.16%, 1000=0.03%, 2000=0.07%
  cpu          : usr=5.90%, sys=14.51%, ctx=242928, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,880401,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=229MiB/s (240MB/s), 229MiB/s-229MiB/s (240MB/s-240MB/s), io=13.4GiB (14.4GB), run=60005-60005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=14.7k, BW=229MiB/s (240MB/s)(13.4GiB/60005msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=randwrite, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80568/160000 hits, 4955 MiB read, 1.002 read amp, 15745333 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24096: Mon Jan 15 04:36:15 2024
  write: IOPS=6453, BW=202MiB/s (211MB/s)(11.8GiB/60046msec); 0 zone resets
    slat (usec): min=5, max=2348.8k, avg=150.49, stdev=8047.05
    clat (usec): min=609, max=2362.4k, avg=19679.91, stdev=90282.08
     lat (usec): min=751, max=2362.5k, avg=19830.98, stdev=90633.59
    clat percentiles (msec):
     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   10], 60.00th=[   11],
     | 70.00th=[   11], 80.00th=[   12], 90.00th=[   32], 95.00th=[   61],
     | 99.00th=[  115], 99.50th=[  144], 99.90th=[ 2123], 99.95th=[ 2299],
     | 99.99th=[ 2366]
   bw (  KiB/s): min=115968, max=432064, per=100.00%, avg=243056.31, stdev=70981.10, samples=102
   iops        : min= 3624, max=13502, avg=7595.51, stdev=2218.16, samples=102
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.04%, 4=0.07%, 10=55.01%, 20=31.67%, 50=6.87%
  lat (msec)   : 100=4.66%, 250=1.44%, 500=0.06%, 2000=0.03%, >=2000=0.13%
  cpu          : usr=4.55%, sys=9.91%, ctx=200966, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,387498,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=202MiB/s (211MB/s), 202MiB/s-202MiB/s (211MB/s-211MB/s), io=11.8GiB (12.7GB), run=60046-60046msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=6453, BW=202MiB/s (211MB/s)(11.8GiB/60046msec); 0 zone resets


===Fio: workload=randwrite, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=randwrite, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80651/160000 hits, 4951 MiB read, 1.001 read amp, 15745499 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24151: Mon Jan 15 04:37:17 2024
  write: IOPS=3304, BW=207MiB/s (217MB/s)(12.1GiB/60101msec); 0 zone resets
    slat (usec): min=7, max=1874.0k, avg=292.63, stdev=8740.47
    clat (usec): min=590, max=1970.6k, avg=38430.92, stdev=97211.01
     lat (usec): min=752, max=1970.6k, avg=38724.55, stdev=97597.44
    clat percentiles (msec):
     |  1.00th=[   17],  5.00th=[   18], 10.00th=[   18], 20.00th=[   19],
     | 30.00th=[   19], 40.00th=[   20], 50.00th=[   21], 60.00th=[   22],
     | 70.00th=[   28], 80.00th=[   44], 90.00th=[   73], 95.00th=[   97],
     | 99.00th=[  163], 99.50th=[  215], 99.90th=[ 1871], 99.95th=[ 1905],
     | 99.99th=[ 1905]
   bw (  KiB/s): min=18944, max=436096, per=100.00%, avg=235250.96, stdev=60783.43, samples=108
   iops        : min=  296, max= 6814, avg=3675.80, stdev=949.74, samples=108
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.04%, 4=0.11%, 10=0.28%, 20=45.99%, 50=36.81%
  lat (msec)   : 100=12.15%, 250=4.23%, 500=0.06%, 2000=0.32%
  cpu          : usr=4.43%, sys=7.58%, ctx=175297, majf=0, minf=11
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,198621,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=207MiB/s (217MB/s), 207MiB/s-207MiB/s (217MB/s-217MB/s), io=12.1GiB (13.0GB), run=60101-60101msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=3304, BW=207MiB/s (217MB/s)(12.1GiB/60101msec); 0 zone resets
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80693/160000 hits, 4950 MiB read, 1.001 read amp, 15745581 total
[0m

===Fio: workload=write, time=60, iodepth=128, bs=4ki===

j1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80733/160000 hits, 4948 MiB read, 1.001 read amp, 15745663 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24205: Mon Jan 15 04:38:20 2024
  write: IOPS=43.4k, BW=169MiB/s (178MB/s)(10.1GiB/60841msec); 0 zone resets
    slat (usec): min=4, max=981654, avg=20.86, stdev=1315.02
    clat (usec): min=274, max=985417, avg=2927.74, stdev=14820.83
     lat (usec): min=302, max=985422, avg=2948.90, stdev=14878.98
    clat percentiles (usec):
     |  1.00th=[  1336],  5.00th=[  1565], 10.00th=[  1713], 20.00th=[  1860],
     | 30.00th=[  1975], 40.00th=[  2073], 50.00th=[  2180], 60.00th=[  2278],
     | 70.00th=[  2442], 80.00th=[  2802], 90.00th=[  3621], 95.00th=[  3851],
     | 99.00th=[  7832], 99.50th=[ 28443], 99.90th=[ 81265], 99.95th=[103285],
     | 99.99th=[826278]
   bw (  KiB/s): min=17448, max=260456, per=100.00%, avg=182021.86, stdev=43604.17, samples=116
   iops        : min= 4362, max=65114, avg=45505.48, stdev=10901.02, samples=116
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.15%
  lat (msec)   : 2=31.83%, 4=65.29%, 10=1.96%, 20=0.15%, 50=0.36%
  lat (msec)   : 100=0.20%, 250=0.02%, 500=0.01%, 750=0.01%, 1000=0.02%
  cpu          : usr=12.62%, sys=42.72%, ctx=373796, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,2639446,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=169MiB/s (178MB/s), 169MiB/s-169MiB/s (178MB/s-178MB/s), io=10.1GiB (10.8GB), run=60841-60841msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=4ki) write:  write: IOPS=43.4k, BW=169MiB/s (178MB/s)(10.1GiB/60841msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=8ki===

j1: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80801/160000 hits, 4946 MiB read, 1.001 read amp, 15745799 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24261: Mon Jan 15 04:39:23 2024
  write: IOPS=26.8k, BW=210MiB/s (220MB/s)(12.3GiB/60003msec); 0 zone resets
    slat (usec): min=4, max=1365.1k, avg=35.37, stdev=2138.06
    clat (usec): min=277, max=1367.2k, avg=4733.18, stdev=23998.93
     lat (usec): min=329, max=1367.2k, avg=4768.81, stdev=24092.53
    clat percentiles (usec):
     |  1.00th=[   1663],  5.00th=[   2073], 10.00th=[   2278],
     | 20.00th=[   2474], 30.00th=[   2606], 40.00th=[   2737],
     | 50.00th=[   2835], 60.00th=[   2933], 70.00th=[   3064],
     | 80.00th=[   3261], 90.00th=[   3654], 95.00th=[   4490],
     | 99.00th=[  58459], 99.50th=[  83362], 99.90th=[ 129500],
     | 99.95th=[ 145753], 99.99th=[1233126]
   bw (  KiB/s): min= 6608, max=360480, per=100.00%, avg=221793.67, stdev=55832.46, samples=115
   iops        : min=  826, max=45060, avg=27724.21, stdev=6979.12, samples=115
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.03%
  lat (msec)   : 2=3.84%, 4=89.42%, 10=3.18%, 20=0.91%, 50=1.41%
  lat (msec)   : 100=0.93%, 250=0.24%, 2000=0.03%
  cpu          : usr=7.88%, sys=20.41%, ctx=147416, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,1610061,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=210MiB/s (220MB/s), 210MiB/s-210MiB/s (220MB/s-220MB/s), io=12.3GiB (13.2GB), run=60003-60003msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=8ki) write:  write: IOPS=26.8k, BW=210MiB/s (220MB/s)(12.3GiB/60003msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=16ki===

j1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80869/160000 hits, 4944 MiB read, 1.000 read amp, 15745935 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24314: Mon Jan 15 04:40:25 2024
  write: IOPS=16.0k, BW=250MiB/s (262MB/s)(14.6GiB/60005msec); 0 zone resets
    slat (usec): min=4, max=1381.3k, avg=60.41, stdev=3161.21
    clat (usec): min=452, max=1412.3k, avg=7945.55, stdev=35582.97
     lat (usec): min=513, max=1412.4k, avg=8006.26, stdev=35723.14
    clat percentiles (msec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    5],
     | 30.00th=[    5], 40.00th=[    5], 50.00th=[    6], 60.00th=[    6],
     | 70.00th=[    6], 80.00th=[    6], 90.00th=[    7], 95.00th=[   18],
     | 99.00th=[   58], 99.50th=[   72], 99.90th=[  146], 99.95th=[ 1150],
     | 99.99th=[ 1385]
   bw (  KiB/s): min=35936, max=424288, per=100.00%, avg=273959.21, stdev=68691.13, samples=111
   iops        : min= 2246, max=26518, avg=17122.47, stdev=4293.19, samples=111
  lat (usec)   : 500=0.01%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.03%, 4=0.14%, 10=93.47%, 20=2.09%, 50=2.87%
  lat (msec)   : 100=1.21%, 250=0.09%, 1000=0.03%, 2000=0.07%
  cpu          : usr=5.39%, sys=15.59%, ctx=269665, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,959135,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=250MiB/s (262MB/s), 250MiB/s-250MiB/s (262MB/s-262MB/s), io=14.6GiB (15.7GB), run=60005-60005msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=16ki) write:  write: IOPS=16.0k, BW=250MiB/s (262MB/s)(14.6GiB/60005msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=32ki===

j1: (g=0): rw=write, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 80937/160000 hits, 4942 MiB read, 1.000 read amp, 15746071 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24369: Mon Jan 15 04:41:28 2024
  write: IOPS=6540, BW=204MiB/s (214MB/s)(12.1GiB/60413msec); 0 zone resets
    slat (usec): min=5, max=1646.5k, avg=149.05, stdev=5533.96
    clat (usec): min=524, max=1660.9k, avg=19416.71, stdev=61942.32
     lat (usec): min=579, max=1660.9k, avg=19566.21, stdev=62180.11
    clat percentiles (msec):
     |  1.00th=[    9],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],
     | 30.00th=[   10], 40.00th=[   10], 50.00th=[   10], 60.00th=[   11],
     | 70.00th=[   11], 80.00th=[   12], 90.00th=[   35], 95.00th=[   67],
     | 99.00th=[  125], 99.50th=[  157], 99.90th=[ 1301], 99.95th=[ 1603],
     | 99.99th=[ 1653]
   bw (  KiB/s): min=74432, max=417280, per=100.00%, avg=229836.80, stdev=59141.55, samples=110
   iops        : min= 2326, max=13040, avg=7182.40, stdev=1848.17, samples=110
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.08%, 4=0.17%, 10=53.54%, 20=31.51%, 50=7.23%
  lat (msec)   : 100=5.27%, 250=1.99%, 500=0.03%, 2000=0.16%
  cpu          : usr=3.68%, sys=9.19%, ctx=205496, majf=0, minf=106
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,395160,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=204MiB/s (214MB/s), 204MiB/s-204MiB/s (214MB/s-214MB/s), io=12.1GiB (12.9GB), run=60413-60413msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=32ki) write:  write: IOPS=6540, BW=204MiB/s (214MB/s)(12.1GiB/60413msec); 0 zone resets


===Fio: workload=write, time=60, iodepth=128, bs=64ki===

j1: (g=0): rw=write, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=io_uring, iodepth=128
fio-3.28
Starting 1 process
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81004/160000 hits, 4940 MiB read, 0.999 read amp, 15746205 total
[0m
j1: (groupid=0, jobs=1): err= 0: pid=24424: Mon Jan 15 04:42:30 2024
  write: IOPS=3376, BW=211MiB/s (221MB/s)(12.4GiB/60017msec); 0 zone resets
    slat (usec): min=6, max=1748.7k, avg=289.84, stdev=6906.81
    clat (usec): min=566, max=1773.1k, avg=37616.01, stdev=76320.23
     lat (usec): min=610, max=1773.4k, avg=37906.64, stdev=76612.45
    clat percentiles (msec):
     |  1.00th=[   17],  5.00th=[   18], 10.00th=[   18], 20.00th=[   19],
     | 30.00th=[   20], 40.00th=[   20], 50.00th=[   22], 60.00th=[   23],
     | 70.00th=[   29], 80.00th=[   46], 90.00th=[   74], 95.00th=[  104],
     | 99.00th=[  161], 99.50th=[  207], 99.90th=[ 1368], 99.95th=[ 1770],
     | 99.99th=[ 1770]
   bw (  KiB/s): min=78336, max=384384, per=100.00%, avg=227668.57, stdev=55877.62, samples=112
   iops        : min= 1224, max= 6006, avg=3557.32, stdev=873.09, samples=112
  lat (usec)   : 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.05%, 4=0.09%, 10=0.19%, 20=40.73%, 50=41.07%
  lat (msec)   : 100=12.44%, 250=5.15%, 2000=0.25%
  cpu          : usr=3.37%, sys=6.99%, ctx=181101, majf=0, minf=12
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.1%
     issued rwts: total=0,202637,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=211MiB/s (221MB/s), 211MiB/s-211MiB/s (221MB/s-221MB/s), io=12.4GiB (13.3GB), run=60017-60017msec

Disk stats (read/write):
  nvme1n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%

RESULT: Fio (iodepth=128; bs=64ki) write:  write: IOPS=3376, BW=211MiB/s (221MB/s)(12.4GiB/60017msec); 0 zone resets
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.485151 s, 216 MB/s
umount: /mnt/fsbench: not mounted.

==== Creating filesystem ====
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 20971520 4k blocks and 5242880 inodes
Filesystem UUID: 94914900-c380-4e0f-8caa-10c8a1e5d124
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000

Allocating group tables:   0/640       done                            
Writing inode tables:   0/640       done                            
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information:   0/640       done



=========================================
=== Running filebench workloads       ===
=========================================




===Filebench: workload=/tmp/filebench/fileserver.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.013: File-server Version 3.0 personality successfully loaded
0.013: Populating and pre-allocating filesets
0.233: bigfileset populated: 200000 files, avg. dir. width = 20, avg. dir. depth = 4.1, 0 leafdirs, 25028.705MB total size
0.233: Removing bigfileset tree (if exists)
0.238: Pre-allocating directories in bigfileset tree
0.629: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81084/160000 hits, 4937 MiB read, 0.999 read amp, 15746373 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81103/160000 hits, 4935 MiB read, 0.999 read amp, 15746411 total
[0m46.721: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
46.721: Population and pre-allocation of filesets completed
46.722: Starting 1 filereader instances
47.731: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81128/160000 hits, 4934 MiB read, 0.999 read amp, 15746460 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81128/160000 hits, 4934 MiB read, 0.999 read amp, 15746461 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81128/160000 hits, 4934 MiB read, 0.999 read amp, 15746464 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81127/160000 hits, 4934 MiB read, 0.999 read amp, 15746466 total
[0m167.743: Run took 120 seconds...
167.749: Per-Operation Breakdown
statfile1            465261ops     3877ops/s   0.0mb/s    0.012ms/op [0.003ms - 34.217ms]
deletefile1          465262ops     3877ops/s   0.0mb/s    0.265ms/op [0.031ms - 803.797ms]
closefile3           465264ops     3877ops/s   0.0mb/s    0.004ms/op [0.001ms - 2.108ms]
readfile1            465264ops     3877ops/s 504.5mb/s    0.115ms/op [0.005ms - 190.184ms]
openfile2            465264ops     3877ops/s   0.0mb/s    0.060ms/op [0.006ms - 8.893ms]
closefile2           465264ops     3877ops/s   0.0mb/s    0.004ms/op [0.001ms - 5.741ms]
appendfilerand1      465264ops     3877ops/s  30.3mb/s    0.802ms/op [0.001ms - 365.729ms]
openfile1            465267ops     3877ops/s   0.0mb/s    0.065ms/op [0.007ms - 29.402ms]
closefile1           465267ops     3877ops/s   0.0mb/s    0.006ms/op [0.001ms - 8.072ms]
wrtfile1             465271ops     3877ops/s 484.4mb/s   10.649ms/op [0.013ms - 399.592ms]
createfile1          465311ops     3877ops/s   0.0mb/s    0.174ms/op [0.024ms - 130.359ms]
167.750: IO Summary: 5117959 ops 42645.779 ops/s 3877/7754 rd/wr 1019.1mb/s 1.105ms/op
167.750: Shutting down processes

RESULT: Filebench /tmp/filebench/fileserver.f:167.750: IO Summary: 5117959 ops 42645.779 ops/s 3877/7754 rd/wr 1019.1mb/s 1.105ms/op


===Filebench: workload=/tmp/filebench/oltp.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.016: OLTP Version 3.0  personality successfully loaded
0.016: Populating and pre-allocating filesets
0.017: logfile populated: 1 files, avg. dir. width = 1024, avg. dir. depth = 0.0, 0 leafdirs, 100.000MB total size
0.017: Removing logfile tree (if exists)
0.024: Pre-allocating directories in logfile tree
0.025: Pre-allocating files in logfile tree
0.147: datafiles populated: 250 files, avg. dir. width = 1024, avg. dir. depth = 0.8, 0 leafdirs, 25000.000MB total size
0.148: Removing datafiles tree (if exists)
0.150: Pre-allocating directories in datafiles tree
0.151: Pre-allocating files in datafiles tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81129/160000 hits, 4934 MiB read, 0.999 read amp, 15746468 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81129/160000 hits, 4934 MiB read, 0.999 read amp, 15746469 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81130/160000 hits, 4934 MiB read, 0.999 read amp, 15746470 total
[0m62.671: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
62.671: Population and pre-allocation of filesets completed
62.672: Starting 200 shadow instances
62.778: Starting 10 dbwr instances
62.783: Starting 1 lgwr instances
63.787: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81131/160000 hits, 4934 MiB read, 0.999 read amp, 15746473 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81132/160000 hits, 4934 MiB read, 0.999 read amp, 15746474 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81133/160000 hits, 4934 MiB read, 0.999 read amp, 15746475 total
[0m183.806: Run took 120 seconds...
183.826: Per-Operation Breakdown
random-rate          0ops        0ops/s   0.0mb/s    0.000ms/op [0.000ms - 0.000ms]
shadow-post-dbwr     2061000ops    17172ops/s   0.0mb/s   10.249ms/op [0.018ms - 1319.110ms]
shadow-post-lg       2061200ops    17174ops/s   0.0mb/s    0.065ms/op [0.002ms - 145.540ms]
shadowhog            2061200ops    17174ops/s   0.0mb/s    0.316ms/op [0.091ms - 124.189ms]
shadowread           2086800ops    17387ops/s  33.5mb/s    0.911ms/op [0.001ms - 335.640ms]
dbwr-aiowait         20601ops      172ops/s   0.0mb/s    3.258ms/op [0.004ms - 63.097ms]
dbwr-block           20605ops      172ops/s   0.0mb/s   14.207ms/op [0.003ms - 530.217ms]
dbwr-hog             20611ops      172ops/s   0.0mb/s    0.010ms/op [0.004ms - 5.849ms]
dbwrite-a            2062380ops    17184ops/s  33.5mb/s    0.004ms/op [0.001ms - 61.242ms]
lg-block             644ops        5ops/s   0.0mb/s  186.231ms/op [33.674ms - 1753.571ms]
lg-aiowait           645ops        5ops/s   0.0mb/s    0.001ms/op [0.001ms - 0.032ms]
lg-write             646ops        5ops/s   1.3mb/s    0.010ms/op [0.001ms - 0.106ms]
183.826: IO Summary: 4171072 ops 34753.863 ops/s 17387/17189 rd/wr  68.4mb/s 0.474ms/op
183.826: Shutting down processes

RESULT: Filebench /tmp/filebench/oltp.f:183.826: IO Summary: 4171072 ops 34753.863 ops/s 17387/17189 rd/wr  68.4mb/s 0.474ms/op


===Filebench: workload=/tmp/filebench/varmail.f===

Filebench Version 1.5-alpha3
0.000: Allocated 177MB of shared memory
0.014: Varmail Version 3.0 personality successfully loaded
0.014: Populating and pre-allocating filesets
1.031: bigfileset populated: 900000 files, avg. dir. width = 1000000, avg. dir. depth = 1.0, 0 leafdirs, 28154.289MB total size
1.031: Removing bigfileset tree (if exists)
1.038: Pre-allocating directories in bigfileset tree
1.058: Pre-allocating files in bigfileset tree
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81138/160000 hits, 4934 MiB read, 0.999 read amp, 15746488 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81139/160000 hits, 4933 MiB read, 0.999 read amp, 15746497 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81139/160000 hits, 4933 MiB read, 0.999 read amp, 15746499 total
[0m[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81137/160000 hits, 4933 MiB read, 0.999 read amp, 15746504 total
[0m81.640: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)
81.640: Population and pre-allocation of filesets completed
81.640: Starting 1 filereader instances
82.644: Running...
[35m[DBG shared_read_cache.cc:805 report_cache_stats] 81137/160000 hits, 4933 MiB read, 0.999 read amp, 15746505 total
[0m202.656: Run took 120 seconds...
202.657: Per-Operation Breakdown
closefile4           173748ops     1448ops/s   0.0mb/s    0.004ms/op [0.001ms - 0.646ms]
readfile4            173748ops     1448ops/s  38.6mb/s    0.931ms/op [0.008ms - 1277.749ms]
openfile4            173748ops     1448ops/s   0.0mb/s    0.022ms/op [0.005ms - 0.804ms]
closefile3           173748ops     1448ops/s   0.0mb/s    0.006ms/op [0.001ms - 0.399ms]
fsyncfile3           173748ops     1448ops/s   0.0mb/s    3.245ms/op [0.631ms - 948.784ms]
appendfilerand3      173754ops     1448ops/s  11.3mb/s    0.074ms/op [0.001ms - 82.449ms]
readfile3            173756ops     1448ops/s  38.6mb/s    1.028ms/op [0.008ms - 1277.786ms]
openfile3            173756ops     1448ops/s   0.0mb/s    0.022ms/op [0.005ms - 1.939ms]
closefile2           173756ops     1448ops/s   0.0mb/s    0.006ms/op [0.001ms - 1.754ms]
fsyncfile2           173756ops     1448ops/s   0.0mb/s    3.391ms/op [0.809ms - 1276.733ms]
appendfilerand2      173764ops     1448ops/s  11.3mb/s    0.304ms/op [0.001ms - 629.219ms]
createfile2          173764ops     1448ops/s   0.0mb/s    0.991ms/op [0.037ms - 951.523ms]
deletefile1          173764ops     1448ops/s   0.0mb/s    0.893ms/op [0.043ms - 953.952ms]
202.657: IO Summary: 2258810 ops 18821.685 ops/s 2896/2896 rd/wr  99.8mb/s 0.840ms/op
202.657: Shutting down processes

RESULT: Filebench /tmp/filebench/varmail.f:202.657: IO Summary: 2258810 ops 18821.685 ops/s 2896/2896 rd/wr  99.8mb/s 0.840ms/op
NQN:nqn.2016-06.io.spdk:cnode1 disconnected 1 controller(s)
+ perl -lane 'print if s/^RESULT: //' /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T03:23:47.lsvd-nvme.20.triple-hdd.txt
+ tee -a /home/isaackhor/code/lsvd-rbd/experiments/results/2024-01-15T03:23:47.lsvd-nvme.20.triple-hdd.txt
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=4862, BW=19.0MiB/s (19.9MB/s)(1148MiB/60418msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=18.7k, BW=73.1MiB/s (76.6MB/s)(4385MiB/60014msec)
Fio (iodepth=1; bs=4ki) randread:  read: IOPS=120, BW=480KiB/s (492kB/s)(28.1MiB/60010msec)
Fio (iodepth=4; bs=4ki) randread:  read: IOPS=699, BW=2798KiB/s (2865kB/s)(164MiB/60012msec)
Fio (iodepth=8; bs=4ki) randread:  read: IOPS=1882, BW=7530KiB/s (7711kB/s)(441MiB/60010msec)
Fio (iodepth=16; bs=4ki) randread:  read: IOPS=4141, BW=16.2MiB/s (17.0MB/s)(971MiB/60010msec)
Fio (iodepth=32; bs=4ki) randread:  read: IOPS=7665, BW=29.9MiB/s (31.4MB/s)(1797MiB/60027msec)
Fio (iodepth=64; bs=4ki) randread:  read: IOPS=4869, BW=19.0MiB/s (19.9MB/s)(1143MiB/60090msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=9603, BW=37.5MiB/s (39.3MB/s)(2270MiB/60497msec)
Fio (iodepth=256; bs=4ki) randread:  read: IOPS=5271, BW=20.6MiB/s (21.6MB/s)(1245MiB/60463msec)
Fio (iodepth=512; bs=4ki) randread:  read: IOPS=9871, BW=38.6MiB/s (40.4MB/s)(2334MiB/60533msec)
Fio (iodepth=32; bs=4ki) read:  read: IOPS=17.7k, BW=69.3MiB/s (72.7MB/s)(4159MiB/60002msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=32.0k, BW=125MiB/s (131MB/s)(7500MiB/60007msec)
Fio (iodepth=128; bs=4ki) randread:  read: IOPS=5556, BW=21.7MiB/s (22.8MB/s)(1311MiB/60399msec)
Fio (iodepth=128; bs=8ki) randread:  read: IOPS=7322, BW=57.2MiB/s (60.0MB/s)(3455MiB/60403msec)
Fio (iodepth=128; bs=16ki) randread:  read: IOPS=4966, BW=77.6MiB/s (81.4MB/s)(4689MiB/60422msec)
Fio (iodepth=128; bs=32ki) randread:  read: IOPS=3992, BW=125MiB/s (131MB/s)(7567MiB/60655msec)
Fio (iodepth=128; bs=64ki) randread:  read: IOPS=3000, BW=188MiB/s (197MB/s)(11.1GiB/60628msec)
Fio (iodepth=128; bs=4ki) read:  read: IOPS=11.7k, BW=45.5MiB/s (47.7MB/s)(2731MiB/60014msec)
Fio (iodepth=128; bs=8ki) read:  read: IOPS=14.9k, BW=116MiB/s (122MB/s)(6965MiB/60023msec)
Fio (iodepth=128; bs=16ki) read:  read: IOPS=12.3k, BW=192MiB/s (202MB/s)(11.3GiB/60027msec)
Fio (iodepth=128; bs=32ki) read:  read: IOPS=9313, BW=291MiB/s (305MB/s)(17.1GiB/60029msec)
Fio (iodepth=128; bs=64ki) read:  read: IOPS=6753, BW=422MiB/s (443MB/s)(24.8GiB/60057msec)
Fio (iodepth=1; bs=4ki) randwrite:  write: IOPS=5682, BW=22.2MiB/s (23.3MB/s)(1332MiB/60001msec); 0 zone resets
Fio (iodepth=4; bs=4ki) randwrite:  write: IOPS=17.0k, BW=66.4MiB/s (69.7MB/s)(3987MiB/60001msec); 0 zone resets
Fio (iodepth=8; bs=4ki) randwrite:  write: IOPS=27.2k, BW=106MiB/s (111MB/s)(6378MiB/60001msec); 0 zone resets
Fio (iodepth=16; bs=4ki) randwrite:  write: IOPS=42.2k, BW=165MiB/s (173MB/s)(9889MiB/60001msec); 0 zone resets
Fio (iodepth=32; bs=4ki) randwrite:  write: IOPS=48.8k, BW=191MiB/s (200MB/s)(11.2GiB/60001msec); 0 zone resets
Fio (iodepth=64; bs=4ki) randwrite:  write: IOPS=48.8k, BW=191MiB/s (200MB/s)(11.2GiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=50.2k, BW=196MiB/s (206MB/s)(11.5GiB/60002msec); 0 zone resets
Fio (iodepth=256; bs=4ki) randwrite:  write: IOPS=47.1k, BW=184MiB/s (193MB/s)(10.8GiB/60002msec); 0 zone resets
Fio (iodepth=512; bs=4ki) randwrite:  write: IOPS=42.4k, BW=166MiB/s (174MB/s)(9936MiB/60001msec); 0 zone resets
Fio (iodepth=32; bs=4ki) write:  write: IOPS=42.2k, BW=165MiB/s (173MB/s)(9886MiB/60001msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=43.8k, BW=171MiB/s (179MB/s)(10.0GiB/60057msec); 0 zone resets
Fio (iodepth=128; bs=4ki) randwrite:  write: IOPS=44.0k, BW=172MiB/s (180MB/s)(10.2GiB/60722msec); 0 zone resets
Fio (iodepth=128; bs=8ki) randwrite:  write: IOPS=24.3k, BW=190MiB/s (199MB/s)(11.1GiB/60014msec); 0 zone resets
Fio (iodepth=128; bs=16ki) randwrite:  write: IOPS=14.7k, BW=229MiB/s (240MB/s)(13.4GiB/60005msec); 0 zone resets
Fio (iodepth=128; bs=32ki) randwrite:  write: IOPS=6453, BW=202MiB/s (211MB/s)(11.8GiB/60046msec); 0 zone resets
Fio (iodepth=128; bs=64ki) randwrite:  write: IOPS=3304, BW=207MiB/s (217MB/s)(12.1GiB/60101msec); 0 zone resets
Fio (iodepth=128; bs=4ki) write:  write: IOPS=43.4k, BW=169MiB/s (178MB/s)(10.1GiB/60841msec); 0 zone resets
Fio (iodepth=128; bs=8ki) write:  write: IOPS=26.8k, BW=210MiB/s (220MB/s)(12.3GiB/60003msec); 0 zone resets
Fio (iodepth=128; bs=16ki) write:  write: IOPS=16.0k, BW=250MiB/s (262MB/s)(14.6GiB/60005msec); 0 zone resets
Fio (iodepth=128; bs=32ki) write:  write: IOPS=6540, BW=204MiB/s (214MB/s)(12.1GiB/60413msec); 0 zone resets
Fio (iodepth=128; bs=64ki) write:  write: IOPS=3376, BW=211MiB/s (221MB/s)(12.4GiB/60017msec); 0 zone resets
Filebench /tmp/filebench/fileserver.f:167.750: IO Summary: 5117959 ops 42645.779 ops/s 3877/7754 rd/wr 1019.1mb/s 1.105ms/op
Filebench /tmp/filebench/oltp.f:183.826: IO Summary: 4171072 ops 34753.863 ops/s 17387/17189 rd/wr  68.4mb/s 0.474ms/op
Filebench /tmp/filebench/varmail.f:202.657: IO Summary: 2258810 ops 18821.685 ops/s 2896/2896 rd/wr  99.8mb/s 0.840ms/op
+ cleanup_nvmf_rbd bdev_lsvd-benchmark
+ local bdev_name=bdev_lsvd-benchmark
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py bdev_rbd_delete bdev_lsvd-benchmark
[1m[36m[INFO liblsvd.cc:287 rbd_close] Closing image lsvd-benchmark
[0m[35m[DBG shared_read_cache.cc:808 report_cache_stats] cache stats reporter exiting
[0m+ scripts/rpc.py bdev_rbd_unregister_cluster rbd_cluster
+ cleanup_nvmf
+ cd /home/isaackhor/code/lsvd-rbd/spdk
+ scripts/rpc.py spdk_kill_instance SIGTERM
+ exit
flush thread (7ff814ff1640) exiting
